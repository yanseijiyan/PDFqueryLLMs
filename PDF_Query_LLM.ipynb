{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 01: Install all necessary packages"
      ],
      "metadata": {
        "id": "kW6kQPQh6f1q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HMWrnuC6ZIc",
        "outputId": "885a6d9b-8734-46f3-e664-61d94cfd475e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.6)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.25)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.18 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.19)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.22 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.22)\n",
            "Requirement already satisfied: langsmith<0.1,>=0.0.83 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.87)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.22->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.22->langchain) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.22->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.22->langchain) (1.2.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.12.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.26.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.2)\n",
            "Requirement already satisfied: pyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.7.4)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "# Install necessary Python packages using pip\n",
        "\n",
        "# Install langchain package for working with language chains\n",
        "!pip install langchain\n",
        "\n",
        "# Install openai package for accessing OpenAI's API\n",
        "!pip install openai\n",
        "\n",
        "# Install pyPDF2 package for working with PDF files\n",
        "!pip install pyPDF2\n",
        "\n",
        "# Install faiss-cpu package for using Faiss library for similarity search\n",
        "!pip install faiss-cpu\n",
        "\n",
        "# Install tiktoken package for working with TikTok API\n",
        "!pip install tiktoken\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 02: Import all required libraries"
      ],
      "metadata": {
        "id": "FEzJvuOo8fPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing PdfReader class from PyPDF2 module to read PDF files.\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# Importing OpenAIEmbeddings class from langchain.embeddings.openai module\n",
        "# to access embeddings provided by OpenAI for natural language processing tasks.\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "# Importing CharacterTextSplitter class from langchain.text_splitter module\n",
        "# to split text into individual characters.\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "# Importing FAISS class from langchain.vectorstores.faiss module\n",
        "# to utilize FAISS, a library for efficient similarity search and clustering of dense vectors.\n",
        "from langchain.vectorstores.faiss import FAISS\n",
        "\n",
        "# Importing multiple vector store options including ElasticVectorSearch, Pinecone, Weaviate, and FAISS\n",
        "# from langchain.vectorstores module for storing and querying dense vectors efficiently.\n",
        "from langchain.vectorstores import ElasticVectorSearch, Pinecone, Weaviate, FAISS\n",
        "\n",
        "# Importing load_qa_chain function from langchain.chains.question_answering module\n",
        "# to load a pre-trained question answering model.\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "# Importing OpenAI module from langchain.llms to access OpenAI's large language models (LLMs).\n",
        "from langchain.llms import OpenAI\n",
        "\n"
      ],
      "metadata": {
        "id": "c1GgkBGV696W"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 03: Setup owr environment"
      ],
      "metadata": {
        "id": "jjPWHXLY9QuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting environment variable for OpenAI API key\n",
        "import os\n",
        "# Replace 'Your Openai Api key' with your actual API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = 'Your Openai Api key'"
      ],
      "metadata": {
        "id": "6BXiaKeR9PtP"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 04: Extracting text from the PDF document using PDF Reader"
      ],
      "metadata": {
        "id": "7n9WEhzJ-Ecv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a PdfReader object and assigning it to the variable 'reader'.\n",
        "# The PdfReader object is initialized with the file path to the PDF document to be read.\n",
        "# In this case, the file path points to a PDF document stored at the specified location.\n",
        "reader = PdfReader(\"/content/drive/MyDrive/PDF_Documents/nasa_systems_engineering_handbook_0.pdf\")"
      ],
      "metadata": {
        "id": "rUg1rBco9tWJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Steo 05: Read data from PDF file and put into a varible raw_text"
      ],
      "metadata": {
        "id": "S0PxJaUF_c3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty string to store the combined text from all pages.\n",
        "raw_text = \"\"\n",
        "\n",
        "# Loop through each page in the reader object and keep track of the page index using 'enumerate'.\n",
        "for i, page in enumerate(reader.pages):\n",
        "    # Extract the text content from the current page.\n",
        "    text = page.extract_text()\n",
        "\n",
        "    # Check if there is any text content on the page.\n",
        "    if text:\n",
        "        # If text exists on the page, concatenate it to the 'raw_text' string.\n",
        "        raw_text += text"
      ],
      "metadata": {
        "id": "fnfMux_w96AA"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print raw_text content\n",
        "raw_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "RxAebQVCAVjO",
        "outputId": "9cc55c92-8835-4ca6-efe3-0e1d9dd2ccae"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'National Aeronautics and  \\n Space Administration\\nNASA  \\nSYSTEMS ENGINEERING  \\nHANDBOOK\\ndesign\\ntest\\nintegrate\\nfly\\nwww.nasa.govNASA SP-2016-6105 Rev2 supersedes SP-2007-6105 Rev 1 dated December, 2007.\\nCover photos:  Top left:  In this photo, engineers led by researcher Greg Gatlin have sprayed fluorescent oil on a 5.8 percent scale \\nmodel of a futuristic hybrid wing body during tests in the 14- by 22-Foot Subsonic Wind Tunnel at NASA’s Langley Research Center \\nin Hampton, VA. The oil helps researchers “see” the flow patterns when air passes over and around the model. (NASA Langley/\\nPreston Martin) Top right:  Water impact test of a test version of the Orion spacecraft took place on August 24, 2016, at NASA \\nLangley Research Center (NASA) Bottom left:  two test mirror segments are placed onto the support structure that will hold them. \\n(NASA/Chris Gunn) Bottom right: This self-portrait of NASA’s Curiosity Mars rover shows the vehicle at the “Mojave” site, where its \\ndrill collected the mission’s second taste of Mount Sharp. (NASA/JPL-Caltech/MSSS)\\nComments, questions, and suggestions  regarding this document can be sent to:\\nSteven R. Hirshorn\\nChief Engineer, Aeronautics Research Mission Directorate (ARMD)\\nOffice of the Chief Engineer\\nNASA Headquarters, Room 6D37\\n300 E St SW\\nWashington, DC 20546-0001\\n202-358-0775\\nsteven.r.hirshorn@nasa.goviii\\nNASA SYSTEMS ENGINEERING HANDBOOKTable of Contents\\nPreface                                        viii\\nAcknowledgments                              ix\\n1.0 Introduction  1\\n1.1 Purpose    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 1\\n1.2 Scope and Depth    .  .  .  .  .  .  .  .  .  .  . 1\\n2.0 Fundamentals of Systems Engineering  3\\n2.1 The Common Technical Processes  \\nand the SE Engine   .  .  .  .  .  .  .  .  .  .  . 5\\n2.2 An Overview of the SE Engine by  \\nProject Phase    .  .  .  .  .  .  .  .  .  .  .  .  . 8\\n2.3 Example of Using the SE Engine   .  .  .  .  10\\n2.4 Distinctions between Product  \\nVerification and Product Validation   .  .  .  11\\n2.5 Cost Effectiveness Considerations   .  .  .  11\\n2.6 Human Systems Integration (HSI)  \\nin the SE Process    .  .  .  .  .  .  .  .  .  .  . 12\\n2.7 Competency Model for  \\nSystems Engineers   .  .  .  .  .  .  .  .  .  .  .  13\\n3.0 NASA Program/Project Life Cycle  17\\n3.1 Program Formulation   .  .  .  .  .  .  .  .  .  . 20\\n3.2 Program Implementation   .  .  .  .  .  .  .  . 20\\n3.3 Project Pre-Phase A: Concept Studies   . 21\\n3.4 Project Phase A: Concept and  \\nTechnology Development   .  .  .  .  .  .  .  . 23\\n3.5 Project Phase B: Preliminary Design  \\nand Technology Completion   .  .  .  .  .  .  25\\n3.6 Project Phase C: Final Design and \\nFabrication   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  27\\n3.7 Project Phase D: System Assembly, Integration and Test, Launch\\n  .  .  .  .  .  . 29\\n3.8 Project Phase E: Operations  \\nand Sustainment   .  .  .  .  .  .  .  .  .  .  .  .  31\\n3.9 Project Phase F: Closeout    .  .  .  .  .  .  . 313.10 Funding: The Budget Cycle   .  .  .  .  .  .  . 33\\n3.11 Tailoring and Customization of  \\nNPR 7123  .1 Req uirements   .  .  .  .  .  .  . 34\\n3.11.1  Introduction  34\\n3.11.2  Criteria for Tailoring  34\\n3.11.3   Tailoring SE NPR Requirements  \\nUsing the Compliance Matrix  35\\n3.11.4   Ways to Tailor a SE Requirement  36\\n3.11.5   Examples of Tailoring and \\nCustomization  37\\n3.11.6   Approvals for Tailoring  40\\n4.0 System Design Processes  43\\n4.1  Stakeholder Expectations Definition    .  .45\\n4.1.1  Process Description  45\\n4.1.2   Stakeholder Expectations Definition \\nGuidance  53\\n4.2 Technical Requirements Definition   .  .  . 54\\n4.2.1  Process Description  54\\n4.2.2   Technical Requirements Definition \\nGuidance  62\\n4.3 Logical Decomposition   .  .  .  .  .  .  .  .  . 62\\n4.3.1  Process Description  62\\n4.3.2   Logical Decomposition Guidance  65\\n4.4 Design Solution Definition    .  .  .  .  .  .  .65\\n4.4.1  Process Description  66\\n4.4.2   Design Solution Definition Guidance  76\\n5.0 Product Realization  77\\n5.1 Product Implementation    .  .  .  .  .  .  .  . 78\\n5.1.1  Process Description  79\\n5.1.2   Product Implementation Guidance  83\\n5.2 Product Integration   .  .  .  .  .  .  .  .  .  .  . 83\\n5.2.1  Process Description  85\\n5.2.2  Product Integration Guidance  88\\n5.3 Product Verification   .  .  .  .  .  .  .  .  .  .  . 88iv\\nNASA SYSTEMS ENGINEERING HANDBOOKTable of Contents\\n5.3.1  Process Description  89\\n5.3.2  Product Verification Guidance  99\\n5.4 Product Validation    .  .  .  .  .  .  .  .  .  .  .99\\n5.4.1  Process Description  99\\n5.4.2  Product Validation Guidance  106\\n5.5 Product Transition    .  .  .  .  .  .  .  .  .  . 106\\n5.5.1  Process Description  106\\n5.5.2  Product Transition Guidance  11 2\\n6.0 Crosscutting Technical Management  11 3\\n6.1 Technical Planning   .  .  .  .  .  .  .  .  .  . 113 \\n6.1.1  Process Description  11 4\\n6.1.2  Technical Planning Guidance  13 0\\n6.2 Requirements Management    .  .  .  .  . 130\\n6.2.1  Process Description  13 1\\n6.2.2   Requirements Management \\nGuidance  1 35\\n6.3 Interface Management   .  .  .  .  .  .  .  . 135\\n6.3.1  Process Description  13 6\\n6.3.2  Interface Management Guidance  13 8\\n6.4 Technical Risk Management   .  .  .  .  . 138\\n6.4.1   Risk Management Process \\nDescription  1 41\\n6.4.2   Risk Management Process \\nGuidance  1 43\\n6.5 Configuration Management   .  .  .  .  .  . 143\\n6.5.1  Process Description  14 4\\n6.5.2  CM Guidance  15 0\\n6.6 Technical Data Management   .  .  .  .  . 151\\n6.6.1  Process Description  15 1\\n6.6.2   Technical Data Management \\nGuidance  155\\n6.7 Technical Assessment   .  .  .  .  .  .  .  . 155\\n6.7.1  Process Description  15 7\\n6.7.2  Technical Assessment Guidance  16 0\\n6.8 Decision Analysis   .  .  .  .  .  .  .  .  .  .  . 160\\n6.8.1  Process Description  164\\n6.8.2  Decision Analysis Guidance  17 0Appendix A  Acronyms    .  .  .  .  .  .  .  .  .  .  .  .  .  . 173\\nAppendix B  Glossary   .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 176\\nAppendix C   How to Write a Good Requirement— \\nChecklist   .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 197\\nAppendix D  Requirements Verification Matrix   .  .  . 201\\nAppendix E   Creating the Validation Plan with a \\nValidation Requirements Matrix   .  .  .  .  203\\nAppendix F  Functional, Timing, and State Analysis  205\\nAppendix G  Technology Assessment/Insertion   .  . 206\\nAppendix H  Integration Plan Outline   .  .  .  .  .  .  .  . 214\\nAppendix I  Verification and Validation Plan Outline  216\\nAppendix J  SEMP Content Outline   .  .  .  .  .  .  .  . 223\\nAppendix K  Technical Plans   .  .  .  .  .  .  .  .  .  .  .  .  235\\nAppendix L   Interface Requirements Document  \\nOutline   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  236\\nAppendix M  CM Plan Outline    .  .  .  .  .  .  .  .  .  .  . 239\\nAppendix N   Guidance on Technical Peer  \\nReviews/Inspections   .  .  .  .  .  .  .  .  . 240\\nAppendix O  Reserved   .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 241\\nAppendix P  SOW Review Checklist   .  .  .  .  .  .  .  . 242\\nAppendix Q  Reserved   .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 243\\nAppendix R  HSI Plan Content Outline   .  .  .  .  .  .  . 244\\nAppendix S   Concept of Operations  \\nAnnotated Outline    .  .  .  .  .  .  .  .  .  . 251\\nAppendix T  Systems Engineering in Phase E   .  .  . 254\\nReferences Cited                             260\\nBibliography                                  270v\\nNASA SYSTEMS ENGINEERING HANDBOOKTable of Figures\\nFigure 2.0-1  SE in Context of Overall Project \\nManagement   .  .  .  .  .  .  .  .  .  .  .  .  . 5\\nFigure 2.1-1  The Systems Engineering Engine  \\n(NPR 7123  . 1)   .  .  .  .  .  .  .  .  .  .  .  .  . 6\\nFigure 2.2-1  Miniature Version of the Poster-Size NASA Project Life Cycle Process \\nFlow for Flight and Ground Systems \\nAccompanying this Handbook\\n   .  .  .  . 8\\nFigure 2.5-1  Life-Cycle Cost Impacts from Early Phase Decision-Making\\n  .  .  .  .  .  .  .  .  13\\nFigure 3.0-1  NASA Space Flight Project Life Cycle \\nfrom NPR 7120  . 5E   .  .  .  .  .  .  .  .  .  . 18\\nFigure 3.11-1  Notional Space Flight Products  \\nTailoring Process   .  .  .  .  .  .  .  .  .  .  . 36\\nFigure 4.0 -1 Interrelationships among the System Design Processes\\n  .  .  .  .  .  .  .  .  .  .  . 44\\nFigure 4.1 -1 Stakeholder Expectations Definition Process\\n  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 46\\nFigure 4.1 -2 Information Flow for Stakeholder Expectations\\n  .  .  .  .  .  .  .  .  .  .  .  .  . 48\\nFigure 4.1 -3 Example of a Lunar Sortie DRM Early in the Life Cycle\\n  .  .  .  .  .  .  .  .  .  .  .  .  .  51\\nFigure 4.2 -1 Technical Requirements Definition Process\\n  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 55\\nFigure 4.2-2  Flow, Type and Ownership of Requirements\\n  .  .  .  .  .  .  .  .  .  .  .  .  .  57\\nFigure 4.2 -3 The Flowdown of Requirements   .  .  .  . 58\\nFigure 4.3 -1 Logical Decomposition Process   .  .  .  . 63\\nFigure 4.4 -1 Design Solution Definition Process   .  .66\\nFigure 4.4 -2 The Doctrine of Successive  \\nRefinement   .  .  .  .  .  .  .  .  .  .  .  .  .  .  67\\nFigure 4.4 -3 A Quantitative Objective Function, Dependent on Life Cycle Cost and  \\nAll Aspects of Effectiveness\\n  .  .  .  .  .  .  71\\nFigure 5.0-1  Product Realization   .  .  .  .  .  .  .  .  .  .  78\\nFigure 5.1-1  Product Implementation Process   .  .  .  79Figure 5.2-1  Product Integration Process    .  .  .  .  .86\\nFigure 5.3-1  Product Verification Process    .  .  .  .  . 91\\nFigure 5.3-2  Example of End-to-End Data Flow  \\nfor a Scientific Satellite Mission   .  .  .  . 96\\nFigure 5.4-1  Product Validation Process   .  .  .  .  . 100\\nFigure 5.5-1  Product Transition Process   .  .  .  .  . 107\\nFigure 6.1 -1 Technical Planning Process   .  .  .  .  . 115\\nFigure 6.2 -1 Requirements Management Process  131\\nFigure 6.3 -1 Interface Management Process   .  .  . 136\\nFigure 6.4-1  Risk Scenario Development   .  .  .  .  . 139\\nFigure 6.4-2  Risk as an Aggregate Set of  \\nRisk Triplets   .  .  .  .  .  .  .  .  .  .  .  .  . 139\\nFigure 6.4-3  Risk Management Process   .  .  .  .  . 141\\nFigure 6.4-4  Risk Management as the Interaction  \\nof Risk-Informed Decision Making  \\nand Continuous Risk Management   . 142\\nFigure 6.5 -1 Configuration Management Process  145\\nFigure 6.5 -2 Five Elements of Configuration Management\\n  .  .  .  .  .  .  .  .  .  .  .  . 145\\nFigure 6.5 -3 Evolution of Technical Baseline   .  .  . 147\\nFigure 6.5 -4 Typical Change Control Process    .  . 148\\nFigure 6.6 -1 Technical Data Management  \\nProcess   .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 151\\nFigure 6.7 -1 Technical Assessment Process   .  .  . 158\\nFigure 6.7 -2 Planning and Status  \\nReporting Feedback Loop    .  .  .  .  . 159\\nFigure 6.8 -1 Decision Analysis Process    .  .  .  .  . 165\\nFigure 6.8-2  Risk Analysis of Decision Alternatives  166\\nFigure G.1-1  PBS Example   .  .  .  .  .  .  .  .  .  .  .  .  208\\nFigure G.3-1  Technology Assessment Process   .  . 209\\nFigure G.3-2  Architectural Studies and  \\nTechnology Development   .  .  .  .  .  . 210\\nFigure G.4-1  Technology Readiness Levels   .  .  .  . 211\\nFigure G.4-2  TMA Thought Process    .  .  .  .  .  .  . 212\\nFigure G.4-3  TRL Assessment Matrix   .  .  .  .  .  .  . 213vi\\nNASA SYSTEMS ENGINEERING HANDBOOKTable of Tables\\nTable 2.1-1  Alignment of the 17 SE Processes to \\nAS9100   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 7\\nTable 2.2-1  Project Life Cycle Phases   .  .  .  .  .  .  . 9\\nTable 2.7-1  NASA System Engineering  \\nCompetency Model   .  .  .  .  .  .  .  .  .  .  14\\nTable 3.0-1  SE Product Maturity from NPR 7123  .1 19\\nTable 3.11-1  Example of Program/Project Types   .  .38\\nTable 3.11-2  Example of Tailoring NPR 7120  .5  \\nRequired Project Products    .  .  .  .  .  .39\\nTable 3.11-3  Example Use of a Compliance Matrix   . 41\\nTable 4.1-1  Stakeholder Identification  \\nthroughout the Life Cycle   .  .  .  .  .  .  .  47\\nTable 4.2-1  Benefits of Well-Written Requirements  59\\nTable 4.2 -2 Requirements Metadata    .  .  .  .  .  .  .59\\nTable 5.3-1  Example information in Verification Procedures and Reports\\n   .  .  .  .  .  .  .94\\nTable 6.1-1  Example Engineering Team  \\nDisciplines in Pre-Phase A for  \\nRobotic Infrared Observatory   .  .  .  . 118Table 6.1-2  Examples of Types of Facilities to Consider during Planning\\n  .  .  .  .  .  . 120\\nTable 6.6 -1 Technical Data Tasks   .  .  .  .  .  .  .  . 156\\nTable 6.7-1  Purpose and Results for Life-Cycle Reviews for Spaceflight Projects\\n   .  . 161\\nTable 6.8 -1 Typical Information to Capture in a Decision Report\\n  .  .  .  .  .  .  .  .  .  .  . 171\\nTable D-1  Requirements Verification Matrix    .  . 202\\nTable E-1  Validation Requirements Matrix   .  .  .  204\\nTable G.1-1  Products Provided by the TA as a Function of Program/Project Phase\\n  . 207\\nTable J-1  Guidance on SEMP Content per  \\nLife-Cycle Phase    .  .  .  .  .  .  .  .  .  . 233\\nTable K-1  Example of Expected Maturity of  \\nKey Technical Plans   .  .  .  .  .  .  .  .  .  235\\nTable R.2-1  HSI Activity, Product, or Risk Mitigation by Program/Project Phase\\n   .  .  .  .  . 250vii\\nNASA SYSTEMS ENGINEERING HANDBOOKTable of Boxes\\nThe Systems Engineer’s Dilemma    .  .  .  .  .  .  .  .  .  . 12\\nSpace Flight Program Formulation   .  .  .  .  .  .  .  .  .  . 20\\nSpace Flight Program Implementation   .  .  .  .  .  .  .  .  21\\nSpace Flight Pre -P hase A: Concept Studies   .  .  .  .  . 22\\nSpace Flight Phase A:  \\nConcept and Technology Development    .  .  .  .  .  .  . 24\\nSpace Flight Phase B:  \\nPreliminary Design and Technology Completion   .  .  . 26\\nSpace Flight Phase C: Final Design and Fabrication    .28\\nSpace Flight Phase D:  \\nSystem Assembly, Integration and Test, Launch   .  .  . 30\\nSpace Flight Phase E: Operations and Sustainment   .32\\nPhase F: Closeout   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 33\\nSystem Design Keys   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 44\\nConcept of Operations vs  .  Operations Concept    .  .  . 51\\nExample of Functional and Performance  \\nRequirements    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .56\\nRationale   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 60\\nProduct Realization Keys   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  78Differences between Verification and  \\nValidation Testing   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 89\\nDifferences between Verification, Qualification, \\nAcceptance and Certification   .  .  .  .  .  .  .  .  .  .  .  .  . 90\\nMethods of Verification   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 93\\nMethods of Validation    .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 101\\nCrosscutting Technical Management Keys   .  .  .  .  . 114\\nTypes of Hardware   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 124\\nEnvironments   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 127\\nDefinitions    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 130\\nTypes of Configuration Management Changes   .  .  . 149\\nData Collection Checklist   .  .  .  .  .  .  .  .  .  .  .  .  .  . 155\\nHSI Relevance   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 246\\nHSI Strategy   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 246\\nHSI Domains   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 247\\nHSI Requirements   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 247\\nHSI Implementation    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 249\\nHSI Plan Updates    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 250viii\\nNASA SYSTEMS ENGINEERING HANDBOOKPreface\\nSince the initial writing of NASA/SP-6105 in 1995 \\nand the following revision (Rev 1) in 2007, systems \\nengineering as a discipline at the National Aeronautics \\nand Space Administration (NASA) has undergone \\nrapid and continued evolution. Changes include \\nusing Model-Based Systems Engineering to improve \\ndevelopment and delivery of products, and accommo -\\ndating updates to NASA Procedural Requirements \\n(NPR) 7123.1. Lessons learned on systems engi -\\nneering were documented in reports such as those \\nby the NASA Integrated Action Team (NIAT), the \\nColumbia Accident Investigation Board (CAIB), and \\nthe follow-on Diaz Report. Other lessons learned were \\ngarnered from the robotic missions such as Genesis \\nand the Mars Reconnaissance Orbiter as well as from \\nmishaps from ground operations and the commercial \\nspace flight industry. Out of these reports came the \\nNASA Office of the Chief Engineer (OCE) initia -\\ntive to improve the overall Agency systems engineer -\\ning infrastructure and capability for the efficient and \\neffective engineering of NASA systems, to produce \\nquality products, and to achieve mission success. This \\nhandbook update is a part of that OCE-sponsored \\nAgency-wide systems engineering initiative.\\nIn 1995, SP-6105 was initially published to bring the \\nfundamental concepts and techniques of systems engi -\\nneering to NASA personnel in a way that recognized \\nthe nature of NASA systems and the NASA environ -\\nment. This revision (Rev 2) of SP-6105 maintains that \\noriginal philosophy while updating the Agency’s sys -\\ntems engineering body of knowledge, providing guid -\\nance for insight into current best Agency practices, \\nand maintaining the alignment of the handbook with \\nthe Agency’s systems engineering policy.The update of this handbook continues the methodol -\\nogy of the previous revision: a top-down compatibility \\nwith higher level Agency policy and a bottom-up infu -\\nsion of guidance from the NASA practitioners in the \\nfield. This approach provides the opportunity to obtain \\nbest practices from across NASA and bridge the infor -\\nmation to the established NASA systems engineering \\nprocesses and to communicate principles of good prac -\\ntice as well as alternative approaches rather than spec -\\nify a particular way to accomplish a task. The result \\nembodied in this handbook is a top-level implemen -\\ntation approach on the practice of systems engineer -\\ning unique to NASA. Material used for updating this \\nhandbook has been drawn from many sources, includ -\\ning NPRs, Center systems engineering handbooks and \\nprocesses, other Agency best practices, and external \\nsystems engineering textbooks and guides.\\nThis handbook consists of six chapters: (1) an intro -\\nduction, (2) a systems engineering fundamentals dis -\\ncussion, (3) the NASA program/project life cycles, \\n(4) systems engineering processes to get from a con -\\ncept to a design, (5) systems engineering processes to \\nget from a design to a final product, and (6) crosscut -\\nting management processes in systems engineering. \\nThe chapters are supplemented by appendices that \\nprovide outlines, examples, and further information \\nto illustrate topics in the chapters. The handbook \\nmakes extensive use of boxes and figures to define, \\nrefine, illustrate, and extend concepts in the chapters.\\nFinally, it should be noted that this handbook provides \\ntop-level guidance for good systems engineering prac -\\ntices; it is not intended in any way to be a directive.\\nNASA/SP-2016-6105 Rev2 supersedes SP-2007-6105 Rev 1 \\ndated December, 2007.ix\\nNASA SYSTEMS ENGINEERING HANDBOOKAcknowledgments\\nThe following individuals are recognized as contrib -\\nuting practitioners to the content of this expanded \\nguidance:\\nAlexander, Michael, NASA/Langley Research Center\\nAllen, Martha, NASA/Marshall Space Flight Center\\nBaumann, Ethan, NASA/Armstrong Flight Research Center\\nBixby, CJ, NASA/Armstrong Flight Research Center\\nBoland, Brian, NASA/Langley Research Center\\nBrady, Timothy, NASA/NASA Engineering and Safety Center\\nBromley, Linda, NASA/Headquarters/Bromley SE Consulting\\nBrown, Mark, NASA/Jet Propulsion Laboratory\\nBrumfield, Mark, NASA/Goddard Space Flight Center\\nCampbell, Paul, NASA/Johnson Space Center\\nCarek, David, NASA/Glenn Research Center\\nCox, Renee, NASA/Marshall Space Flight Center\\nCrable, Vicki, NASA/Glenn Research Center\\nCrocker, Alan, NASA/Ames Research Center\\nDeLoof, Richard, NASA/Glenn Research Center\\nDemo, Andrew/Ames Research Center\\nDezfuli, Homayoon, NASA/HQ\\nDiehl, Roger, NASA/Jet Propulsion Laboratory\\nDiPietro, David, NASA/Goddard Space Flight Center\\nDoehne, Thomas, NASA/Glenn Research Center\\nDuarte, Alberto, NASA/Marshall Space Flight Center\\nDurham, David, NASA/Jet Propulsion Laboratory\\nEpps, Amy, NASA/Marshall Space Flight Center\\nFashimpaur, Karen, Vantage Partners\\nFeikema, Douglas, NASA/Glenn Research Center\\nFitts, David, NASA/Johnson Space Flight Center\\nFoster, Michele, NASA/Marshall Space Flight Center\\nFuller, David, NASA/Glenn Research Center\\nGati, Frank, NASA/Glenn Research Center\\nGefert, Leon, NASA/Glenn Research Center\\nGhassemieh, Shakib, NASA/Ames Research CenterGrantier, Julie, NASA/Glenn Research Center\\nHack, Kurt, NASA/Glenn Research Center\\nHall, Kelly, NASA/Glenn Research Center\\nHamaker, Franci, NASA/Kennedy Space Center\\nHange, Craig, NASA/Ames Research Center\\nHenry, Thad, NASA/Marshall Space Flight Center\\nHill, Nancy, NASA/Marshall Space Flight Center\\nHirshorn, Steven, NASA/Headquarters\\nHolladay, Jon, NASA/NASA Engineering and Safety Center\\nHyatt, Mark, NASA/Glenn Research Center\\nKillebrew, Jana, NASA/Ames Research Center\\nJannette, Tony, NASA/Glenn Research Center\\nJenks, Kenneth, NASA/Johnson Space Center\\nJones, Melissa, NASA/Jet Propulsion Laboratory\\nJones, Ross, NASA/Jet Propulsion Laboratory\\nKillebrew, Jana, NASA/Ames Research Center\\nLeitner, Jesse, NASA/Goddard Space Flight Center\\nLin, Chi, NASA/Jet Propulsion Laboratory\\nMascia, Anne Marie, Graphic Artist\\nMcKay, Terri, NASA/Marshall Space Flight Center\\nMcNelis, Nancy, NASA/Glenn Research Center\\nMendoza, Donald, NASA/Ames Research Center\\nMiller, Scott, NASA/Ames Research Center\\nMontgomery, Patty, NASA/Marshall Space Flight Center\\nMosier, Gary, NASA/Goddard Space Flight Center\\nNoble, Lee, NASA/Langley Research Center\\nOleson, Steven, NASA/Glenn Research Center\\nParrott, Edith, NASA/Glenn Research Center\\nPowell, Christine, NASA/Stennis Space Center\\nPowell, Joseph, NASA/Glenn Research Center\\nPrice, James, NASA/Langley Research Center\\nRawlin, Adam, NASA/Johnson Space Center\\nRochlis-Zumbado, Jennifer, NASA/Johnson Space Center\\nRohn, Dennis, NASA/Glenn Research Center\\nRosenbaum, Nancy, NASA/Goddard Space Flight Centerx\\nNASA SYSTEMS ENGINEERING HANDBOOKAcknowledgments\\nRyan, Victoria, NASA/Jet Propulsion Laboratory\\nSadler, Gerald, NASA/Glenn Research Center\\nSalazar, George, NASA/Johnson Space Center\\nSanchez, Hugo, NASA/Ames Research Center\\nSchuyler, Joseph, NASA/Stennis Space Center\\nSheehe, Charles, NASA/Glenn Research Center\\nShepherd, Christena, NASA/Marshall Space Flight Center\\nShull, Thomas, NASA/Langley Research Center\\nSinger, Bart, NASA/Langley Research Center\\nSlywczak, Richard, NASA/Glenn Research Center\\nSmith, Scott, NASA/Goddard Space Flight Center\\nSmith, Joseph, NASA/Headquarters\\nSprague, George, NASA/Jet Propulsion LaboratoryTrase, Kathryn, NASA/Glenn Research Center\\nTrenkle, Timothy, NASA/Goddard Space Flight Center\\nVipavetz, Kevin, NASA/Langley Research Center\\nVoss, Linda, Dell Services\\nWalters, James Britton, NASA/Johnson Space Center\\nWatson, Michael, NASA/Marshall Space Flight Center\\nWeiland, Karen, NASA/Glenn Research Center\\nWiedeman, Grace, Dell Services\\nWiedenmannott, Ulrich, NASA/Glenn Research Center\\nWitt, Elton, NASA/Johnson Space Center\\nWoytach, Jeffrey, NASA/Glenn Research Center\\nWright, Michael, NASA/Marshall Space Flight Center\\nYu, Henry, NASA/Kennedy Space Center1\\nNASA SYSTEMS ENGINEERING HANDBOOK1.0Introduction\\n1.1 Purpose\\nThis handbook is intended to provide general \\nguidance and information on systems engineer -\\ning that will be useful to the NASA community. It \\nprovides a generic description of Systems Engineering \\n(SE) as it should be applied throughout NASA. A goal of the handbook is to increase awareness and consis -\\ntency across the Agency and advance the practice of SE. This handbook provides perspectives relevant to NASA and data particular to NASA.\\nThis handbook should be used as a companion for \\nimplementing NPR 7123.1, Systems Engineering Processes and Requirements, as well as the Center-specific handbooks and directives developed for \\nimplementing systems engineering at NASA. It pro -\\nvides a companion reference book for the various \\nsystems engineering-related training being offered under NASA’s auspices.\\n1.2 Scope and Depth\\nThis handbook describes systems engineering best practices that should be incorporated in the develop -\\nment and implementation of large and small NASA \\nprograms and projects. The engineering of NASA systems requires a systematic and disciplined set of \\nprocesses that are applied recursively and iteratively for the design, development, operation, maintenance, \\nand closeout of systems throughout the life cycle of \\nthe programs and projects. The scope of this hand -\\nbook includes systems engineering functions regard -\\nless of whether they are performed by a manager or an engineer, in-house or by a contractor.\\nThere are many Center-specific handbooks and direc-\\ntives as well as textbooks that can be consulted for in-depth tutorials. For guidance on systems engi -\\nneering for information technology projects, refer to Office of Chief Information Officer Information \\nTechnology Systems Engineering Handbook Version 2.0 . \\nFor guidance on entrance and exit criteria for mile -\\nstone reviews of software projects, refer to NASA-\\nHDBK-2203, NASA Software Engineering Handbook . \\nA NASA systems engineer can also participate in the NASA Engineering Network (NEN) Systems \\nEngineering Community of Practice, located at \\nhttps://nen.nasa.gov/web/se . This Web site includes \\nmany resources useful to systems engineers, includ -\\ning document templates for many of the work prod -\\nucts and milestone review presentations required by \\nthe NASA SE process.21.0 Introduction\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nThis handbook is applicable to NASA space flight \\nprojects of all sizes and to research and development \\nprograms and projects. While all 17 processes are \\napplicable to all projects, the amount of formality, \\ndepth of documentation, and timescales are varied as \\nappropriate for the type, size, and complexity of the \\nproject. References to “documents” are intended to \\ninclude not only paper or digital files but also models, graphics, drawings, or other appropriate forms that \\ncapture the intended information.\\nFor a more in-depth discussion of the principles pro -\\nvided in this handbook, refer to the NASA Expanded \\nGuidance for SE document which can be found at \\nhttps://nen.nasa.gov/web/se/doc-repository . This hand -\\nbook is an abridged version of that reference.3\\nNASA SYSTEMS ENGINEERING HANDBOOK2.0Fundamentals of  \\nSystems Engineering\\nAt NASA, “systems engineering” is defined as a \\nmethodical, multi-disciplinary approach for the \\ndesign, realization, technical management, opera -\\ntions, and retirement of a system. A “system” is the combination of elements that function together to produce the capability required to meet a need. The elements include all hardware, software, equip -\\nment, facilities, personnel, processes, and procedures needed for this purpose; that is, all things required to produce system-level results. The results include sys-tem-level qualities, properties, characteristics, func -\\ntions, behavior, and performance. The value added by the system as a whole, beyond that contributed inde -\\npendently by the parts, is primarily created by the relationship among the parts; that is, how they are interconnected.\\n1 It is a way of looking at the “big pic -\\nture” when making technical decisions. It is a way of achieving stakeholder functional, physical, and oper -\\national performance requirements in the intended use environment over the planned life of the system within cost, schedule, and other constraints. It is a methodology that supports the containment of the life cycle cost of a system. In other words, systems engineering is a logical way of thinking.\\n1 Eberhardt Rechtin, Systems Architecting of Organizations: \\nWhy Eagles Can’t Swim .Systems engineering is the art and science of devel -\\noping an operable system capable of meeting require -\\nments within often opposed constraints. Systems engineering is a holistic, integrative discipline, wherein the contributions of structural engineers, electrical engineers, mechanism designers, power engineers, human factors engineers, and many more disciplines are evaluated and balanced, one against another, to produce a coherent whole that is not dom -\\ninated by the perspective of a single discipline.\\n2\\nSystems engineering seeks a safe and balanced design in the face of opposing interests and multiple, some -\\ntimes conflicting constraints. The systems engineer should develop the skill for identifying and focusing efforts on assessments to optimize the overall design and not favor one system/subsystem at the expense of another while constantly validating that the goals of the operational system will be met. The art is in knowing when and where to probe. Personnel with these skills are usually tagged as “systems engineers.” They may have other titles—lead systems engineer, \\n2 Comments on systems engineering throughout Chapter 2  .0 \\nare extracted from the speech “System Engineering and the \\nTwo Cultures of Engineering” by Michael D  . G riffin, NASA \\nAdministrator  .42.0 Fundamentals of Systems Engineering\\nNASA SYSTEMS ENGINEERING HANDBOOK\\ntechnical manager, chief engineer—but for this doc -\\nument, the term “systems engineer” is used.\\nThe exact role and responsibility of the systems engi -\\nneer may change from project to project depending on \\nthe size and complexity of the project and from phase \\nto phase of the life cycle. For large projects, there may \\nbe one or more systems engineers. For small projects, \\nthe project manager may sometimes perform these \\npractices. But whoever assumes those responsibil -\\nities, the systems engineering functions should be \\nperformed. The actual assignment of the roles and \\nresponsibilities of the named systems engineer may \\nalso therefore vary. The lead systems engineer ensures \\nthat the system technically fulfills the defined needs \\nand requirements and that a proper systems engineer -\\ning approach is being followed. The systems engineer \\noversees the project’s systems engineering activities as \\nperformed by the technical team and directs, com -\\nmunicates, monitors, and coordinates tasks. The \\nsystems engineer reviews and evaluates the technical \\naspects of the project to ensure that the systems/sub -\\nsystems engineering processes are functioning prop -\\nerly and evolves the system from concept to product. \\nThe entire technical team is involved in the systems \\nengineering process.\\nThe systems engineer usually plays the key role in \\nleading the development of the concept of opera -\\ntions (ConOps) and resulting system architec -\\nture, defining boundaries, defining and allocating \\nrequirements, evaluating design tradeoffs, balanc -\\ning technical risk between systems, defining and \\nassessing interfaces, and providing oversight of ver -\\nification and validation activities, as well as many \\nother tasks. The systems engineer typically leads the \\ntechnical planning effort and has the prime respon -\\nsibility in documenting many of the technical plans, \\nrequirements and specification documents, verifica -\\ntion and validation documents, certification pack -\\nages, and other technical documentation.In summary, the systems engineer is skilled in the \\nart and science of balancing organizational, cost, \\nand technical interactions in complex systems. The \\nsystems engineer and supporting organization are \\nvital to supporting program and Project Planning \\nand Control (PP&C) with accurate and timely cost \\nand schedule information for the technical activities. \\nSystems engineering is about tradeoffs and compro -\\nmises; it uses a broad crosscutting view of the system \\nrather than a single discipline view. Systems engineer -\\ning is about looking at the “big picture” and not only \\nensuring that they get the design right (meet require -\\nments) but that they also get the right design (enable \\noperational goals and meet stakeholder expectations).\\nSystems engineering plays a key role in the project \\norganization. Managing a project consists of three \\nmain objectives: managing the technical aspects of \\nthe project, managing the project team, and manag -\\ning the cost and schedule. As shown in FIGURE 2.0-1 , \\nthese three functions are interrelated. Systems engi -\\nneering is focused on the technical characteristics \\nof decisions including technical, cost, and schedule \\nand on providing these to the project manager. The \\nProject Planning and Control (PP&C) function is \\nresponsible for identifying and controlling the cost \\nand schedules of the project. The project manager has \\noverall responsibility for managing the project team \\nand ensuring that the project delivers a technically \\ncorrect system within cost and schedule. Note that \\nthere are areas where the two cornerstones of project \\nmanagement, SE and PP&C, overlap. In these areas, \\nSE provides the technical aspects or inputs whereas \\nPP&C provides the programmatic, cost, and sched -\\nule inputs.\\nThis document focuses on the SE side of the dia -\\ngram. The practices/processes are taken from NPR \\n7123.1, NASA Systems Engineering Processes \\nand Requirements. Each process is described in \\nmuch greater detail in subsequent chapters of this 52.0 Fundamentals of Systems Engineering\\nNASA SYSTEMS ENGINEERING HANDBOOK\\ndocument, but an overview is given in the following \\nsubsections of this chapter.\\n2.1 The Common Technical \\nProcesses and the SE Engine\\nThere are three sets of common technical processes in \\nNPR 7123.1, NASA Systems Engineering Processes and Requirements: system design, product realiza -\\ntion, and technical management. The processes in each set and their interactions and flows are illustrated by the NPR systems engineering “engine” shown in \\nFIGURE 2.1-1 . The processes of the SE engine are used \\nto develop and realize the end products. This chap -\\nter provides the application context of the 17 com -\\nmon technical processes required in NPR7123.1. The system design processes, the product realization processes, and the technical management processes are discussed in more detail in \\nChapters 4.0 , 5.0, and \\n6.0, respectively. Processes 1 through 9 indicated in \\nFIGURE 2.1-1  represent the tasks in the execution of a \\nproject. Processes 10 through17 are crosscutting tools for carrying out the processes.PROJECT MANAGEMENT\\nPROJECT MANAGEMENT ACTIVITIES\\n• Setting up Project Team\\n•P\\nrogrammatic Stakeholders (non-technical, non-business)\\n•Programmatic Planning (non-technical, non-business)\\n•Identifying Programmatic (non-technical) requirements•Identifying Programmatic Risks\\n•Technology Transfer and Commercialization\\n•Integration of technical and non-technical activities\\n•Overall Approver/Decider\\nSystems Engineering\\nSystem Design Processes\\n• Stakeholder Expectations Definition\\n• Technical Requirement’s Definition\\n• Logical Decomposition\\n• Design Solution Definition\\nProduct Realization Processes\\n• Product Implementation\\n• Product Integration\\n• Product Verification\\n• Product Validation\\n• Product Transition\\nTechnical Management Processes\\n• Technical Planning\\n• Requirements Management\\n• Interface Management\\n• Technical Risk Management\\n• Configuration Management\\n• Technical Data Management\\n• Technical Assessment\\n• Decision AnalysesPP&C\\n•PP &C Integration\\n•Resource Management\\n•Scheduling\\n•Cost Estimation & Assessment\\n•Acquisition & Contract\\nManagement\\n•Risk Management\\n•CM/DMCommon  \\nAreas\\n•S takeholders\\n•Risks\\n•Configuration\\nManagement\\n•DataManagement\\n•Reviews\\n•Schedule\\nFIGURE 2.0-1  SE in C ontext of Overall Project Management62.0 Fundamentals of Systems Engineering\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nRequirements Definition\\nProcesses \\n1.  Stakeholders Expectations \\nDefinition\\n2.  Technical Requirements  \\nDefinition\\n3.  Logical Decomposition\\n4.  Design Solution DefinitionSystem Design \\nProcessesRequirements Flow Down \\nfrom Level above\\nTechnical Management \\nProcesses\\n10.  Technical Planning\\n11.  Requirement Management \\n12.  Interface Management 13.  Technical Risk Management \\n14.  Configuration Management \\n15.  Technical Data Management \\n16.  Technical Assessment \\nTechnical Decision  \\nAnalysis Process \\n17.  Decision Analysis 9.  Product Transition\\n8.  Product Validation\\n7.  Product Verification\\n6.  Product Integration\\n5.  Product ImplementationProduct Realization \\nProcesses\\nSystem Design Processes \\napplied to each product \\nlayer down through system \\nstructureProduct Realization \\nProcesses applied to each \\nproduct layer up through \\nsystem structureRequirements Flow Down \\nTo Level belowRealized Products \\nto Level above\\nRealized Products\\nFrom Level belowCross -\\ncuttingCross-cutting\\nTechnical Solution \\nDefinition Processes Technical Planning\\nProcesses \\nTechnical Control\\nProcesses Product Transition\\nProcesses \\nEvaluation Processes \\nDesign Realization\\nProcesses Technical Assessment\\nProcesses \\nFIGURE 2.1-1  The S ystems Engineering Engine (NPR 7123  .1)\\n• System Design Processes:  The four system \\ndesign processes shown in FIGURE 2.1-1  are used to \\ndefine and baseline stakeholder expectations, gen -\\nerate and baseline technical requirements, decom -\\npose the requirements into logical and behavioral \\nmodels, and convert the technical requirements into a design solution that will satisfy the base -\\nlined stakeholder expectations. These processes are applied to each product of the system struc -\\nture from the top of the structure to the bottom until the lowest products in any system structure branch are defined to the point where they can be built, bought, or reused. All other products in the system structure are realized by implementation or integration.\\n• Product Realization Processes:  The product real -\\nization processes are applied to each operational/mission product in the system structure starting from the lowest level product and working up to higher level integrated products. These processes are used to create the design solution for each product (through buying, coding, building, or reusing) and to verify, validate, and transition up to the next hierarchical level those products that satisfy their design solutions and meet stakeholder 72.0 Fundamentals of Systems Engineering\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nexpectations as a function of the applicable life \\ncycle phase.\\n• Technical Management Processes:  The techni -\\ncal management processes are used to establish and evolve technical plans for the project, to man -\\nage communication across interfaces, to assess progress against the plans and requirements for the system products or services, to control tech -\\nnical execution of the project through to comple -\\ntion, and to aid in the decision-making process.\\nTABLE 2.1-1 Alignment of the 17 SE Processes to AS9100\\nSE Process AS9100 Requirement\\nStakeholder Expectations Customer Requirements\\nTechnical Requirements \\nDefinitionPlanning of Product Realization\\nLogical Decomposition Design and Development Input\\nDesign Solution Definition Design and Development Output\\nProduct Implementation Control of Production\\nProduct Integration Control of Production\\nProduct Verification Verification\\nProduct Validation Validation\\nProduct Transition Control of Work Transfers; Post Delivery Support, Preservation of Product\\nTechnical Planning Planning of Product Realization; Review of Requirements; Measurement, Analysis and Improvement\\nRequirements Management Design and Development Planning; Purchasing\\nInterface Management Configuration Management\\nTechnical Risk Management Risk Management\\nConfiguration Management Configuration Management; Identification and Traceability; Control of Nonconforming Product\\nTechnical Data Management Control of Documents; Control of Records; Control of Design and Development Changes\\nTechnical Assessment Design and Development Review\\nDecision Analysis Measurement, Analysis and Improvement; Analysis of DataThe pro\\ncesses within the SE engine are used both \\niteratively and recursively. As defined in NPR 7123.1, \\n“iterative” is the “application of a process to the same product or set of products to correct a discovered disc\\nrepancy or other variation from requirements,” \\nwhereas “recursive” is defined as adding value to the system “by the repeated application of processes to design next lower layer system products or to real -\\nize next upper layer end products within the system structure. This also applies to repeating application of the same processes to the system structure in the next life cycle phase to mature the system definition and satisfy phase success criteria.” The technical processes are applied recursively and iteratively to break down the initializing concepts of the system to a level of detail concrete enough that the technical team can implement a product from the information. Then the processes are applied recursively and iteratively to 82.0 Fundamentals of Systems Engineering\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nintegrate the smallest product into greater and larger \\nsystems until the whole of the system or product has been assembled, verified, validated, and transitioned.\\nFor a detailed example of how the SE Engine could \\nbe used, refer to the NASA Expanded Guidance for SE document at\\n https://nen.nasa.gov/web/se/\\ndoc-repository .\\nAS9100 is a widely adopted and standardized qual -\\nity management system developed for the commer -\\ncial aerospace industry. Some NASA Centers have chosen to certify to the AS9100 quality system and may require their contractors to follow NPR 7123.1. \\nTABLE 2.1-1  shows how the 17 NASA SE processes \\nalign with AS9100.2.2 An Overview of the SE \\nEngine by Project Phase\\nFIGURE 2.2-1  conceptually illustrates how the SE \\nengine is used during each phase of a project (Pre-\\nPhase A through Phase F). The life cycle phases are described in \\nTABLE 2.2-1 . FIGURE 2.2-1  is a conceptual \\ndiagram. For full details, refer to the poster version of this figure, which is located at \\nhttps://nen.nasa.gov/\\nweb/se/doc-repository .\\nPre-Phase A:\\nConcept Studies Implementa tion ApprovalTechnical Development Technic al Management5.2Key De cision Points:\\nMajor Reviews:Feasible Concept Top-Level Architecture Functional BaselineAllocated\\nBaselineAs-Deployed  Baseline\\n6.1\\n6.86.1 \\n6.8 6.1 \\n6.8 6.1 \\n6.8 6.1 \\n6.8 6.1 \\n6.8 6.1 \\n6.8 ?\\n??\\n5.35.45.5\\n5.14.24.1\\n4.3\\n4.4\\n??\\n??\\n??\\n?4.24.1\\n4.3\\n4.4\\n5.15.35.45.5\\n5.2Product\\nBaselinePhase F:\\nCloseout Phase E:\\nOperations &\\nSustainment Phase D:\\nSystem Assembly, \\nIntegration & Test, LaunchPhase B:\\nPreliminary Design &\\nTechnology CompletionPhase A:\\nConcept & Technology\\nDevelopment Phase C:\\nFinal Design &\\nFabrication\\n6.2\\n6.76.66.56.46.3Formulation\\nFIGURE 2.2-1  Minia ture Version of the Poster-Size NASA Project Life Cycle  \\nProcess Flow for Flight and Ground Systems Accompanying this HandbookTh\\ne uppermost horizontal portion of this chart is \\nused as a reference to project system maturity, as the \\nproject progresses from a feasible concept to an as-de -\\nployed system; phase activities; Key Decision Points (KDPs); and major project reviews. The next major horizontal band shows the technical development 92.0 Fundamentals of Systems Engineering\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nprocesses (steps 1 through 9) in each project phase. \\nThe SE engine cycles five times from Pre-Phase A through Phase D. Note that NASA’s management has structured Phases C and D to “split” the tech -\\nnical development processes in half in Phases C and D to ensure closer management control. The engine is bound by a dashed line in Phases C and D. Once a project enters into its operational state (Phase E) and closes out (Phase F), the technical work shifts to activities commensurate with these last two proj -\\nect phases. The next major horizontal band shows the eight technical management processes (steps 10 through 17) in each project phase. The SE engine cycles the technical management processes seven times from Pre-Phase A through Phase F.TABLE 2.2-1  Pr oject Life Cycle Phases\\nPhase Purpose Typical OutcomesPre-FormulationPre-Phase A  \\nConcept \\nStudiesTo produce a broad spectrum of ideas and alternatives for missions from which new programs/projects can be selected. Determine feasibility of desired system, develop mission concepts, draft system-level requirements, assess performance, cost, and schedule feasibility; identify potential technology needs, and scope.Feasible system concepts in the form of simulations, analysis, study reports, models, and mock-upsFormulationPhase A \\nConcept and Technology DevelopmentTo determine the feasibility and desirability of a suggested new system and establish an initial baseline compatibility with NASA’s strategic plans. Develop final mission concept, system-level requirements, needed system technology developments, and program/project technical management plans.System concept definition in the form of simulations, analysis, engineering models and mock-ups, and trade study definition\\nPhase B \\nPreliminary Design and Technology CompletionTo define the project in enough detail to establish an initial baseline capable of meeting mission needs. Develop system structure end product (and enabling product) requirements and generate a preliminary design for each system structure end product.End products in the form of mock-ups, trade study results, specification and interface documents, and prototypesImplementationPhase C  \\nFinal Design and FabricationTo complete the detailed design of the system (and its associated subsystems, including its operations systems), fabricate hardware, and code software. Generate final designs for each system structure end product.End product detailed designs, end product component fabrication, and software development\\nPhase D  \\nSystem Assembly, Integration and Test, LaunchTo assemble and integrate the system (hardware, software, and humans), meanwhile developing confidence that it is able to meet the system requirements. Launch and prepare for operations. Perform system end product implementation, assembly, integration and test, and transition to use.Operations-ready system end product with supporting related enabling products\\nPhase E \\nOperations and SustainmentTo conduct the mission and meet the initially identified need and maintain support for that need. Implement the mission operations plan.Desired system\\nPhase F \\nCloseoutTo implement the systems decommissioning/disposal plan developed in Phase E and perform analyses of the returned data and any returned samples.Product closeout102.0 Fundamentals of Systems Engineering\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n2.3 Example of Using the SE Engine\\nIn Pre-Phase A, the SE engine is used to develop the \\ninitial concepts; clearly define the unique roles of humans, hardware, and software in performing the \\nmissions objectives; establish the system functional \\nand performance boundaries; develop/identify a pre -\\nliminary/draft set of key high-level requirements, define one or more initial Concept of Operations (ConOps) scenarios; realize these concepts through iterative modeling, mock-ups, simulation, or other \\nmeans; and verify and validate that these concepts \\nand products would be able to meet the key high-level requirements and ConOps. The operational concept must include scenarios for all significant operational situations, including known off-nominal situations. \\nTo develop a useful and complete set of scenarios, \\nimportant malfunctions and degraded-mode opera -\\ntional situations must be considered. The importance of early ConOps development cannot be underesti -\\nmated. As system requirements become more detailed and contain more complex technical information, \\nit becomes harder for the stakeholders and users to \\nunderstand what the requirements are conveying; i.e., it may become more difficult to visualize the end product. The ConOps can serve as a check in identi -\\nfying missing or conflicting requirements.\\nNote that this Pre-Phase A initial concepts develop -\\nment work is not the formal verification and valida -\\ntion program that is performed on the final product, \\nbut rather it is a methodical run through ensuring that the concepts that are being developed in this \\nPre-Phase A are able to meet the likely requirements \\nand expectations of the stakeholders. Concepts are developed to the lowest level necessary to ensure that they are feasible and to a level that reduces the risk low enough to satisfy the project. Academically, this \\nprocess could proceed down to the circuit board level \\nfor every system; however, that would involve a great deal of time and money. There may be a higher level or tier of product than circuit board level that would enable designers to accurately determine the feasibil -\\nity of accomplishing the project, which is the purpose of Pre-Phase A.\\nDuring Phase A, the recursive use of the SE engine \\nis continued, this time taking the concepts and draft key requirements that were developed and validated during Pre-Phase A and fleshing them out to become the set of baseline system requirements and ConOps. During this phase, key areas of high risk might be \\nsimulated to ensure that the concepts and require -\\nments being developed are good ones and to identify \\nverification and validation tools and techniques that will be needed in later phases.\\nDuring Phase B, the SE engine is applied recursively \\nto further mature requirements and designs for all products in the developing product tree and perform verification and validation of concepts to ensure that the designs are able to meet their requirements. Operational designs and mission scenarios are evalu -\\nated and feasibility of execution within design capa -\\nbilities and cost estimates are assessed.\\nPhase C again uses the left side of the SE engine to \\nfinalize all requirement updates, finalize the ConOps \\nvalidation, develop the final designs to the lowest \\nlevel of the product tree, and begin fabrication.\\nPhase D uses the right side of the SE engine to recur -\\nsively perform the final implementation, integration, \\nverification, and validation of the end product, and at \\nthe final pass, transition the end product to the user.\\nThe technical management processes of the SE engine \\nare used in Phases E and F to monitor performance; control configuration; and make decisions associ -\\nated with the operations, sustaining engineering, and closeout of the system. Any new capabilities or upgrades of the existing system reenter the SE engine as new developments.112.0 Fundamentals of Systems Engineering\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n2.4 Distinctions between \\nProduct Verification and Product Validation\\nFrom a process perspective, the Product Verification \\nand Product Validation processes may be similar in nature, but the objectives are fundamentally different:\\n• Verification of a product shows proof of compli -\\nance with requirements—that the product can meet each “shall” statement as proven though per -\\nformance of a test, analysis, inspection, or demon -\\nstration (or combination of these).\\n• Validation of a product shows that the prod -\\nuct accomplishes the intended purpose in the intended environment—that it meets the expec -\\ntations of the customer and other stakeholders as shown through performance of a test, analysis, inspection, or demonstration.\\nVerification testing relates back to the approved requirements set and can be performed at different \\nstages in the product life cycle. The approved specifi -\\ncations, drawings, parts lists, and other configuration \\ndocumentation establish the configuration baseline of that product, which may have to be modified at a later time. Without a verified baseline and appropri -\\nate configuration controls, later modifications could be costly or cause major performance problems.\\nValidation relates back to the ConOps document. \\nValidation testing is conducted under realistic con -\\nditions (or simulated conditions) on end products for \\nthe purpose of determining the effectiveness and suit -\\nability of the product for use in mission operations by \\ntypical users. Validation can be performed in each development phase using phase products (e.g., mod -\\nels) and not only at delivery using end products.It is appropriate for verification and validation meth -\\nods to differ between phases as designs advance. The ultimate success of a program or project may relate to the frequency and diligence of validation efforts during the design process, especially in Pre-Phase A and Phase A during which corrections in the direc -\\ntion of product design might still be made cost-effec -\\ntively. The question should be continually asked, “Are we building the right product for our users and other stakeholders?” The selection of the verification or val -\\nidation method is based on engineering judgment as to which is the most effective way to reliably show the \\nproduct’s conformance to requirements or that it will \\noperate as intended and described in the ConOps.\\n2.5 Cost Effectiveness \\nConsiderations\\nThe objective of systems engineering is to see that the \\nsystem is designed, built, and can be operated so that it accomplishes its purpose safely in the most cost-ef -\\nfective way possible considering performance, cost, schedule, and risk. A cost-effective and safe system should provide a particular kind of balance between effectiveness and cost. This causality is an indefinite one because there are usually many designs that meet the cost-effective condition.\\nDesign trade studies, an important part of the sys -\\ntems engineering process, often attempt to find \\ndesigns that provide the best combination of cost and effectiveness. At times there are alternatives that \\neither reduce costs without reducing effectiveness or \\nincrease effectiveness without increasing cost. In such “win-win” cases, the systems engineer’s decision is easy. When the alternatives in a design trade study require trading cost for effectiveness, the decisions become harder.122.0 Fundamentals of Systems Engineering\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nFIGURE 2.5-1  shows that the life cycle costs of a \\nprogram or project tend to get “locked in” early in \\ndesign and development. The cost curves clearly show that late identification of and fixes to problems cost considerably more later in the life cycle. Conversely, descopes taken later versus earlier in the project life cycle result in reduced cost savings. This figure, obtained from the Defense Acquisition University, is an example of how these costs are determined by the early concepts and designs. The numbers will vary from project to project, but the general shape of the curves and the message they send will be similar. For example, the figure shows that during design, only about 15% of the costs might be expended, but the design itself will commit about 75% of the life cycle costs. This is because the way the system is designed will determine how expensive it will be to test, man -\\nufacture, integrate, operate, and sustain. If these fac-tors have not been considered during design, they pose significant cost risks later in the life cycle. Also note that the cost to change the design increases as you get later in the life cycle. If the project waits until verification to do any type of test or analysis, any problems found will have a significant cost impact to redesign and reverify.THE SYSTEMS ENGINEER’S DILEMMA\\nAt each cost-effective solution:\\n• To re\\nduce cost at constant risk, performance must be reduced.\\n• To re\\nduce risk at constant cost, performance must be reduced.\\n• To re\\nduce cost at constant performance, higher risks must be accepted.\\n• To re\\nduce risk at constant performance, higher costs must be accepted.\\nIn this context, time in the schedule is often a critical resource, so that schedule  behaves like a kind of cost .\\nThe technical team may have to choose among designs \\nthat differ in terms of numerous attributes. A variety of methods have been developed that can be used to help uncover preferences between attributes and to quan -\\ntify subjective assessments of relative value. When this can be done, trades between attributes can be assessed quantitatively. Often, however, the attributes are incompatible. In the end, decisions need to be made in spite of the given variety of attributes. There are several decision analysis techniques (\\nSection 6.8 ) \\nthat can aid in complex decision analysis. The systems engineer should always keep in mind the information that needs to be available to help the decision-makers choose the most cost-effective option.\\n2.6 Human Systems Integration \\n(HSI) in the SE Process\\nAs noted at the beginning of NPR 7123.1, the “sys -\\ntems approach is applied to all elements of a system \\n(i.e., hardware, software, human systems integra -\\ntion. In short, the systems engineering approach must equally address and integrate these three key elements: hardware, software, and human systems 132.0 Fundamentals of Systems Engineering\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nintegration. Therefore, the human element is some -\\nthing that integration and systems engineering pro -\\ncesses must address. The definition of “system” in \\nNPR 7123.1 is inclusive; i.e., a system is “the combi -\\nnation of elements that function together to produce the capability required to meet a need. The elements include all hardware, software, equipment, facilities, personnel, processes, and procedures needed for this purpose. For additional information and guidance on his, refer to Section 2.6 of the NASA Expanded Guidance for Systems Engineering at \\nhttps://nen.\\nnasa.gov/web/se/doc-repository .0%10%20%\\nMCR SRR SDR PDR CDR SIR ORR DR/DRR30%40% 3–6×45%\\nConceptDesign\\nDevelopProd/TestOperations \\nthrough \\nDisposal75%90%\\n100%\\n50%\\n20%\\n15%8%20–100×500–1000×\\n50%60%70%80%90%100%Cumulative Percentage Life Cycle Cost against Time\\nTimeCommitted Life  Cycle Costs\\n% Completed  \\n(Costs Expended)Cost to Change Design Direction\\nMCR Mission Concept Review CDR Critical Design Review\\nSRR System Requirements Review SIR System Integration Review\\nSDR System Definition Review ORR Operational Readiness Review\\nPDR Preliminary Design Review DR/DRR Decommissioning/Disposal Readiness Review\\nAdapted from INCOSE-TP-2003-002-04, 2015\\nFIGURE 2.5-1  Lif e-Cycle Cost Impacts from Early Phase Decision-Making\\n2.7 Competency Model for \\nSystems Engineers\\nTABLE 2.7-1  provides a summary of the Competency \\nModel for Systems Engineering. For more informa -\\ntion on the NASA SE Competency model refer to: \\nhttp://appel.nasa.gov/competency-model/ .\\nThere are four levels of proficiencies associated with \\neach of these competencies:\\n• Team Practitioner/Technical Engineer\\n• Team Lead/Subsystem Lead\\n• Project Systems Engineer\\n• Chief Engineer142.0 Fundamentals of Systems Engineering\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nTABLE 2.7-1  NASA System Engineering Competency Model\\nCompetency \\nAreaCompetency Description\\nSE 1.0 \\nSystem \\nDesignSE 1.1 \\nStakeholder Expectation Definition & ManagementEliciting and defining use cases, scenarios, concept of operations and stakeholder expectations. This includes identifying stakeholders, establishing support strategies, establishing a set of Measures of Effectiveness (MOEs), validating stakeholder expectation statements, and obtaining commitments from the customer and other stakeholders, as well as using the baselined stakeholder expectations for product validation during product realization\\nSE 1.2 \\nTechnical Requirements DefinitionTransforming the baseline stakeholder expectations into unique, quantitative, and measurable technical requirements expressed as “shall” statements that can be used for defining the design solution. This includes analyzing the scope of the technical problems to be solved, defining constraints affecting the designs, defining the performance requirements, validating the resulting technical requirement statements, defining the Measures of Performance (MOPs) for each MOE, and defining appropriate Technical Performance Measures (TPMs) by which technical progress will be assessed.\\nSE 1.3   \\nLogical DecompositionTransforming the defined set of technical requirements into a set of logical decomposition models and their associated set of derived technical requirements for lower levels of the system, and for input to the design solution efforts. This includes decomposing and analyzing by function, time, behavior, data flow, object, and other models. It also includes allocating requirements to these decomposition models, resolving conflicts between derived requirements as revealed by the models, defining a system architecture for establishing the levels of allocation, and validating the derived technical requirements.\\nSE 1.4  \\nDesign Solution DefinitionTranslating the decomposition models and derived requirements into one or more design solutions, and using the Decision Analysis process to analyze each alternative and for selecting a preferred alternative that will satisfy the technical requirements. A full technical data package is developed describing the selected solution. This includes generating a full design description for the selected solution; developing a set of ‘make-to,’ ‘buy-to,’ ‘reuse-to,’ specifications; and initiating the development or acquisition of system products and enabling products.\\nSE 2.0 \\nProduct RealizationSE 2.1  \\nProduct ImplementationGenerating a specific product through buying, making, or reusing so as to satisfy the design requirements. This includes preparing the implementation strategy; building or coding the produce; reviewing vendor technical information; inspecting delivered, built, or reused products; and preparing product support documentation for integration.\\nSE 2.2  \\nProduct IntegrationAssembling and integrating lower-level validated end products into the desired end product of the higher-level product. This includes preparing the product integration strategy, performing detailed planning, obtaining products to integrate, confirming that the products are ready for integration, preparing the integration environment, and preparing product support documentation.\\nSE 2.3  \\nProduct VerificationProving the end product conforms to its requirements. This includes preparing for the verification efforts, analyzing the outcomes of verification (including identifying anomalies and establishing recommended corrective actions), and preparing a product verification report providing the evidence of product conformance with the applicable requirements.\\n(continued)152.0 Fundamentals of Systems Engineering\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nCompetency \\nAreaCompetency Description\\nSE 2.0 \\nProduct \\nRealizationSE 2.4  \\nProduct ValidationConfirming that a verified end product satisfies the stakeholder expectations for its intended use when placed in its intended environment and ensuring that any anomalies discovered during validation are appropriately resolved prior to product transition. This includes preparing to conduct product validation, performing the product validation, analyzing the results of validation (including identifying anomalies and establishing recommended corrective actions), and preparing a product validation report providing the evidence of product conformance with the stakeholder expectations baseline.\\nSE 2.5  \\nProduct TransitionTransitioning the verified and validated product to the customer at the next level in the system structure. This includes preparing to conduct product transition, evaluating the product and enabling product readiness for product transition, preparing the product for transition (including handling, storing, and shipping preparation), preparing sites, and generating required documentation to accompany the product\\nSE 3.0 \\nTechnical ManagementSE 3.1 \\nTechnical PlanningPlanning for the application and management of each common technical process, as well as identifying, defining, and planning the technical effort necessary to meet project objectives. This includes preparing or updating a planning strategy for each of the technical processes, and determining deliverable work products from technical efforts; identifying technical reporting requirements; identifying entry and success criteria for technical reviews; identifying product and process measures to be used; identifying critical technical events; defining cross domain interoperability and collaboration needs; defining the data management approach; identifying the technical risks to be addressed in the planning effort; identifying tools and engineering methods to be employed; and defining the approach to acquire and maintain technical expertise needed. This also includes preparing the Systems Engineering Management Plan (SEMP) and other technical plans; obtaining stakeholder commitments to the technical plans; and issuing authorized technical work directives to implement the technical work\\nSE 3.2 \\nRequirements ManagementManaging the product requirements, including providing bidirectional traceability, and managing changes to establish requirement baselines over the life cycle of the system products. This includes preparing or updating a strategy for requirements management; selecting an appropriate requirements management tool; training technical team members in established requirement management procedures; conducting expectation and requirements traceability audits; managing expectation and requirement changes; and communicating expectation and requirement change information\\nSE 3.3 \\nInterface ManagementEstablishing and using formal interface management to maintain internal and external interface definition and compliance among the end products and enabling products. This includes preparing interface management procedures, identifying interfaces, generating and maintaining interface documentation, managing changes to interfaces, disseminating interface information, and conducting interface control\\nSE 3.4 \\nTechnical Risk ManagementExamining on a continual basis the risks of technical deviations from the plans, and identifying potential technical problems before they occur. Planning, invoking, and performing risk-handling activities as needed across the life of the product or project to mitigate impacts on meeting technical objectives. This includes developing the strategy for technical risk management, identifying technical risks, and conducting technical risk assessment; preparing for technical risk mitigation, monitoring the status of each technical risk, and implementing technical risk mitigation and contingency action plans when applicable thresholds have been triggered.\\n(continued)162.0 Fundamentals of Systems Engineering\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nCompetency \\nAreaCompetency Description\\nSE 3.0 \\nTechnical \\nManagementSE 3.5 \\nConfiguration ManagementIdentifying the configuration of the product at various points in time, systematically controlling changes to the configuration of the product, maintaining the integrity and traceability of product configuration, and preserving the records of the product configuration throughout its life cycle. This includes establishing configuration management strategies and policies, identifying baselines to be under configuration control, maintaining the status of configuration documentation, and conducting configuration audits\\nSE 3.6 \\nTechnical Data ManagementIdentifying and controlling product-related data throughout its life cycle; acquiring, accessing, and distributing data needed to develop, manage, operate, support, and retire system products; managing and disposing data as records; analyzing data use; obtaining technical data feedback for managing the contracted technical efforts; assessing the collection of appropriate technical data and information; maintaining the integrity and security of the technical data, effectively managing authoritative data that defines, describes, analyzes, and characterizes a product life cycle; and ensuring consistent, repeatable use of effective Product Data and Life-cycle Management processes, best practices, interoperability approaches, methodologies, and traceability. This includes establishing technical data management strategies \\nand policies; maintaining revision, status, and history of stored technical data and \\nassociated metadata; providing approved, published technical data; providing technical data to authorized parties; and collecting and storing required technical data.\\nSE 3.7 \\nTechnical AssessmentMonitoring progress of the technical effort and providing status information for support of the system design, product realization, and technical management efforts. This includes developing technical assessment strategies and policies, assessing technical work productivity, assessing product quality, tracking and trending technical metrics, and conducting technical, peer, and life cycle reviews.\\nSE 3.8 \\nTechnical Decision AnalysisEvaluating technical decision issues, identifying decision criteria, identifying alternatives, analyzing alternatives, and selecting alternatives. Performed throughout the system life cycle to formulate candidate decision alternatives, and evaluate their impacts on health and safety, technical, cost, and schedule performance. This includes establishing guidelines for determining which technical issues are subject to formal analysis processes; defining the criteria for evaluating alternative solutions; identifying alternative solutions to address decision issues; selecting evaluation methods; selecting recommended solutions; and reporting the results and findings with recommendations, impacts, and corrective actions.17\\nNASA SYSTEMS ENGINEERING HANDBOOK3.0NASA Program/Project Life Cycle\\nOne of the fundamental concepts used within \\nNASA for the management of major systems \\nis the program/project life cycle, which categorizes everything that should be done to accomplish a pro -\\ngram or project into distinct phases that are separated by Key Decision Points (KDPs). KDPs are the events at which the decision authority determines the readi -\\nness of a program/project to progress to the next phase of the life cycle (or to the next KDP).  Phase boundaries \\nare defined so that they provide natural points for “go” or “no-go” decisions. Decisions to proceed may be qualified by liens that should be removed within an agreed-to time period. A program or project that fails to pass a KDP may be allowed to try again later after addressing deficiencies that precluded passing the KDP, or it may be terminated.\\nAll systems start with the recognition of a need or the \\ndiscovery of an opportunity and proceed through var -\\nious stages of development to the end of the project. While the most dramatic impacts of the analysis and optimization activities associated with systems engineer -\\ning are obtained in the early stages, decisions that affect cost continue to be amenable to the systems approach even as the end of the system lifetime approaches.Decomposing the program/project life cycle into phases organizes the entire process into more man -\\nageable pieces. The program/project life cycle should provide managers with incremental visibility into the progress being made at points in time that fit with the management and budgetary environments.\\nFor NASA projects, the life cycle is defined in the \\napplicable governing document:\\n• For space flight projects:  NPR 7120.5, NASA \\nSpace Flight Program and Project Management Requirements\\n• For information technology:  NPR 7120.7, \\nNASA Information Technology and Institutional Infrastructure Program and Project Management Requirements\\n• For NASA research and technology : NPR 7120.8, \\nNASA Research and Technology Program and Project Management Requirements\\n• For software:  NPR 7150.2 NASA Software \\nEngineering Requirements183.0 NASA Program/Project Life Cycle\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nFor example, NPR 7120.5 defines the major NASA \\nlife cycle phases as Formulation and Implementation. For space flight systems projects, the NASA life cycle phases of Formulation and Implementation divide into the following seven incremental pieces. The phases of the project life cycle are:\\nProgram Pre-Formulation:\\n• Pre-Phase A:  Concept Studies\\nProgram Formulation\\n• Phase A:  Concept and Technology Development\\n• Phase B: Preliminary Design and Technology \\nCompletion\\nProgram Implementation:\\n• Phase C:  Final Design and Fabrication• Phase D: System Assembly, Integration and Test, \\nLaunch\\n• Phase E:  Operations and Sustainment\\n• Phase F:  Closeout\\nFIGURE 3.0-1  is taken from NPR 7120.5 and provides \\nthe life cycle for NASA space flight projects and identi -\\nfies the KDPs and reviews that characterize the phases. More information concerning life cycles can be found in the NASA Expanded Guidance for SE document at \\nhttps://nen.nasa.gov/web/se/doc-repository  and in the \\nSP-2014-3705, NASA Space Flight Program and Project Management Handbook .\\nTABLE 3.0-1  is taken from NPR 7123.1 and represents \\nthe product maturity for the major SE products developed and matured during the product life cycle.\\nNASA Life-Cycle \\nPhasesFORMULATION IMPLEMENTATION\\nProject Life-Cycle \\nPhasesPre-Phase A:\\nConcept StudiesPhase A:\\nConcept and \\nTechnology \\nDevelopmentPhase B:\\nPreliminary Design \\nand Technology \\nCompletionPhase C:\\nFinal Design and \\nFabricationPhase D:\\nSystem Assembly, \\nIntegration & Test, \\nLaunch & CheckoutPhase E:\\nOperations and \\nSustainmentPhase F:\\nCloseout\\nProject Life-Cycle Gates, Documents, and Major Events\\nAgency Reviews\\nHuman Space \\nFlight Project Life-Cycle Reviews\\n1,2\\n Re-flights\\nRob\\notic Mission \\nProject Life Cycle Reviews\\n1,2\\nOther Reviews\\nSupporting \\nReviews\\nFOOTNOTES\\n1. Fl\\nexibility is allowed as to the timing, number, and content of reviews as long as the equivalent \\ninformation is provided at each KDP and the approach is fully documented in the Project Plan.\\n2. Li\\nfe-cycle review objectives and expected maturity states for these reviews and the attendant \\nKDPs are contained in Table 2-5 and Appendix D Table D-3 of this handbook\\n3. PR\\nR is needed only when there are multiple copies of systems. It does not require an SRB. Timing \\nis notional.\\n4. CE\\nRRs are established at the discretion of program .\\n5. Fo\\nr robotic missions, the SRR and the MDR may be combined.\\n6. SA\\nR generally applies to human space flight.\\n7. Ti\\nming of the ASM is determined by the MDAA. It may take place at any time during Phase A.\\n  Red triangles represent life-cycle reviews that require SRBs. The Decision Authority, A\\ndministrator, MDAA, or Center Director may request the SRB to conduct other reviews.ACRONYMSASM – Acquisition Strategy MeetingCDR – Critical Design ReviewCERR – Critical Events Readiness ReviewDR – Decommissioning ReviewDRR – Disposal Readiness ReviewFA – Formulation AgreementFAD – Formulation Authorization DocumentFRR – Flight Readiness ReviewKDP – Key Decision PointLRR – Launch Readiness ReviewLV – Launch VehicleMCR – Mission Concept ReviewMDR – Mission Definition ReviewMRR – Mission Readiness ReviewORR – Operational Readiness ReviewPDR – Preliminary Design ReviewPFAR – Post-Flight Assessment ReviewPLAR – Post-Launch Assessment ReviewPRR – Production Readiness ReviewSAR – System Acceptance ReviewSDR – System Definition ReviewSIR – System Integration ReviewSMSR – Safety and Mission Success ReviewSRB – Standing Review BoardSRR – System Requirements Review\\nFIGURE 3.0-1 NASA Space Flight Project Life Cycle from NPR 7120 .5EApproval for  \\nFormulationApproval for  \\nImplementation\\nKDP A\\nPreliminary Project \\nRequirementsPreliminary  \\nProject PlanBaseline  \\nProject PlanLaunch End of Mission Final Archival \\nof Data\\nInspections and \\nRefurbishment\\n Re-enters appropriate life-cycle \\nphase if modifications are \\nneeded between flightsEnd of FlightFAD\\nMCR\\nMCRASM7\\nSRR\\nSRRSDR\\nMDR\\n5PDR\\nPDRSIR\\nSIRORR\\nORRFRR\\nMRRPLAR\\nPLAR\\nSMSR,LRR (LV), FRR (LV) SAR\\n6CERR4\\nCERR4PFARDR\\nDRDRR\\nDRRCDR/  \\nPRR\\n3\\nCDR/  \\nPRR3FAKDP B KDP C KDP D KDP F KDP E\\nPeer Reviews, Subsystem PDFs, Subsystem CDRs, and System Reviews193.0 NASA Program/Project Life Cycle\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nTABLE 3.0-1  SE Product Maturity from NPR 7123  .1\\nFormulation ImplementationProductsUncoupled/  \\nLoosely CoupledKDP 0 KDP I Periodic KDPs\\nTightly Coupled \\nProgramsKDP 0 KDP I KDP II KDP III Periodic KDPs\\nProjects and Single Project ProgramsPre-\\nPhase APhase A Phase B Phase C Phase D Phase E Phase F\\nKDP A KDP B KDP C KDP D KDP E KDP F\\nMCR SRR MDR/SDR PDR CDR SIR ORR FRR DR DRR\\nStakeholder identification and **Baseline Update Update Update\\nConcept definition **Baseline Update Update Update Update\\nMeasure of effectiveness \\ndefinition**Approve\\nCost and schedule for technical Initial Update Update Update Update Update Update Update Update\\nSEMP1Preliminary **Baseline **Baseline Update Update Update\\nRequirements Preliminary **Baseline Update Update Update\\nTechnical Performance Measures definition**Approve\\nArchitecture definition **Baseline\\nAllocation of requirements to \\nnext lower level**Baseline\\nRequired leading indicator trends**Initial Update Update Update\\nDesign solution definition Preliminary **Preliminary **Baseline Update Update\\nInterface definition(s) Preliminary Baseline Update Update\\nImplementation plans (Make/code, buy, reuse)Preliminary Baseline Update\\nIntegration plans Preliminary Baseline Update **Update\\nVerification and validation \\nplansApproach Preliminary Baseline Update Update\\nVerification and validation results**Initial **Preliminary **Baseline\\nTransportation criteria and instructionsInitial Final Update\\nOperations plans\\nBaseline Update Update **Update\\nOperational procedures Preliminary Baseline **Update Update\\nCertification (flight/use) Preliminary **Final\\nDecommissioning plans Preliminary Preliminary Preliminary **Baseline Update **Update\\nDisposal plans Preliminary Preliminary Preliminary **Baseline Update Update **Update\\n** Item is a required product for that review\\n1  S EMP is baselined at SRR for projects, tightly coupled programs and single-project programs, and at MDR/SDR for uncoupled, \\nand loosely coupled programs  .203.0 NASA Program/Project Life Cycle\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n3.1 Program Formulation\\nThe program Formulation Phase establishes a cost-ef -\\nfective program that is demonstrably capable of \\nmeeting Agency and mission directorate goals and objectives. The program Formulation Authorization Document (FAD) authorizes a Program Manager (PM) to initiate the planning of a new program and to perform the analyses required to formulate a sound program plan. The lead systems engineer provides the technical planning and concept development or this phase of the program life cycle. Planning includes identifying the major technical reviews that are needed and associated entrance and exit criteria. Major reviews leading to approval at KDP I are the SRR, SDR, PDR, and governing Program Management Council (PMC) review. A summary of the required gate products for the program Formulation Phase can be found in the governing NASA directive (e.g., NPR 7120.5 for space flight programs, NPR 7120.7 for IT projects, NPR 7120.8 for research and tech -\\nnology projects). Formulation for all program types is the same, involving one or more program reviews followed by KDP I where a decision is made approv -\\ning a program to begin implementation.\\n3.2 Program Implementation\\nDuring the program Implementation phase, the PM works with the Mission Directorate Associate Administrator (MDAA) and the constituent project \\nSPACE FLIGHT PROGRAM FORMULATION\\nPurpose\\nTo establish a cost-effective program that is demonstrably capable of meeting Agency and mission directorate goals and objectives\\nTypical Activities and Their Products for Space Flight Programs\\n• Identify program stakeholders and users\\n• Develop program requirements based on user expectations and allocate them to initial projects• Identify NASA risk classification\\n• Define and approve program acquisition strategies• Develop interfaces to other programs• Start developing technologies that cut across multiple projects within the program\\n• Derive initial cost estimates and approve a program budget based on the project’s life cycle costs• Perform required program Formulation technical activities defined in NPR 7120.5• Satisfy program Formulation reviews’ entrance/success criteria detailed in NPR 7123.1• Develop a clear vision of the program’s benefits and usage in the operational era and document it in a ConOps\\nRev\\niews\\n• MCR\\n (pre-Formulation)\\n• SRR\\n• SDR213.0 NASA Program/Project Life Cycle\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nmanagers to execute the program plan cost-effec-\\ntively. Program reviews ensure that the program continues to contribute to Agency and mission directorate goals and objectives within funding con -\\nstraints. A summary of the required gate products for the program Implementation Phase can be found in the governing NASA directive; e.g., NPR 7120.5 for space flight programs. The program life cycle has two different implementation paths, depending on pro -\\ngram type. Each implementation path has different types of major reviews. It is important for the sys -\\ntems engineer to know what type of program a proj -\\nect falls under so that the appropriate scope of the technical work, documentation requirements, and set of reviews can be determined.SPACE FLIGHT PROGRAM IMPLEMENTATION\\nPurpose\\nTo execute the program and constituent projects and ensure that the program continues to contribute to Agency goals and objectives within funding constraints\\nTypical Activities and Their Products\\n• Initiate projects through direct assignment or competitive process (e.g., Request for Proposal (RFP), \\nAnnouncement of Opportunity (AO)\\n• Monitor project’s formulation, approval, implementation, integration, operation, and ultimate \\ndecommissioning\\n• Adjust program as resources and requirements change• Perform required program Implementation technical activities from NPR 7120.5• Satisfy program Implementation reviews’ entrance/success criteria from NPR 7123.1\\nRev\\niews\\n• PS\\nR/PIR (uncoupled and loosely coupled programs only)\\n• Re\\nviews synonymous (not duplicative) with the project reviews in the project life cycle (see FIGURE 3.0-4 ) \\nthrough Phase D (single-project and tightly coupled programs only)\\n3.3 Project Pre-Phase A: \\nConcept Studies\\nThe purpose of Pre-Phase A is to produce a broad \\nspectrum of ideas and alternatives for missions from which new programs/projects can be selected. During Pre-Phase A, a study or proposal team anal -\\nyses a broad range of mission concepts that can fall within technical, cost, and schedule constraints and that contribute to program and Mission Directorate goals and objectives. Pre-Phase A effort could include focused examinations on high-risk or high tech -\\nnology development areas. These advanced studies, along with interactions with customers and other potential stakeholders, help the team to identify promising mission concept(s). The key stakehold -\\ners (including the customer) are determined and 223.0 NASA Program/Project Life Cycle\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nSPACE FLIGHT PRE‑PHASE A: CONCEPT STUDIES\\nPurpose\\nTo produce a broad spectrum of ideas and alternatives for missions from which new programs and projects can be selected. Determine feasibility of desired system; develop mission concepts; draft system-level requirements; assess performance, cost, and schedule feasibility; identify potential technology needs and scope.\\nTypical Activities and Products\\n• Review/identify any initial customer requirements or scope of work, which may include:\\n >Mission\\n >Science\\n >Top-level system\\n• Identify and involve users and other stakeholders\\n >Identify key stakeholders for each phase of the life cycle\\n >Capture and baseline expectations as Needs, Goals, and Objectives (NGOs)\\n >Define measures of effectiveness\\n• Develop and baseline the Concept of Operations\\n >Identify and perform trade-offs and analyses of alternatives (AoA)\\n >Perform preliminary evaluations of possible missions\\n• Identify risk classification\\n• Identify initial technical risks\\n• Identify the roles and responsibilities in performing mission objectives (i.e., technical team, flight, and \\nground crew) including training\\n• Develop plans\\n >Develop preliminary SEMP\\n >Develop and baseline Technology Development Plan\\n >Define preliminary verification and validation approach\\n• Prepare program/project proposals, which may include:\\n >Mission justification and objectives;\\n >A ConOps that exhibits clear understanding of how the program’s outcomes will cost-effectively satisfy mission objectives;\\n >High-level Work Breakdown Structures (WBSs);\\n >Life cycle rough order of magnitude (ROM) cost, schedule, and risk estimates; and\\n >Technology assessment and maturation strategies.\\n• Satisfy MCR entrance/success criteria from NPR 7123.1\\nReviews\\n• MCR\\n• Informal proposal review233.0 NASA Program/Project Life Cycle\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nexpectations for the project are gathered from them. \\nIf feasible concepts can be found, one or more may be selected to go into Phase A for further development. \\nTypically, the system engineers are heavily involved \\nin the development and assessment of the concept options. In projects governed by NPR 7120.5, the descope options define what the system can accom -\\nplish if the resources are not available to accomplish the entire mission. This could be in the form of fewer \\ninstruments, a less ambitious mission profile, accom -\\nplishing only a few goals, or using cheaper, less capa -\\nble technology. Descope options can also reflect what \\nthe mission can accomplish in case a hardware fail -\\nure results in the loss of a portion of the spacecraft \\narchitecture; for example, what an orbiter can accom -\\nplish after the loss of a lander. The success criteria are \\nreduced to correspond with a descoped mission.\\nDescope options are developed when the NGOs \\nor other stakeholder expectation documentation is \\ndeveloped. The project team develops a preliminary \\nset of mission descope options as a gate product for the MCR, but these preliminary descope options are not baselined or maintained. They are kept in the documentation archive in case they are needed later \\nin the life cycle.\\nIt is important in Pre-Phase A to define an accurate \\ngroup of stakeholders and users to help ensure that mission goals and operations concepts meet the needs and expectations of the end users. In addition, it is \\nimportant to estimate the composition of the techni -\\ncal team and identify any unique facility or personnel \\nrequirements.\\nAdvanced studies may extend for several years and \\nare typically focused on establishing mission goals \\nand formulating top-level system requirements and ConOps. Conceptual designs may be developed to demonstrate feasibility and support programmatic estimates. The emphasis is on establishing feasibility and desirability rather than optimality. Analyses and designs are accordingly limited in both depth and number of options, but each option should be eval -\\nuated for its implications through the full life cycle, i.e., through Operations and Disposal. It is important in Pre-Phase A to develop and mature a clear vision of what problems the proposed program will address, how it will address them, and how the solution will be feasible and cost-effective.\\n3.4 Project Phase A: Concept \\nand Technology Development\\nThe purpose of Phase A is to develop a proposed \\nmission/system architecture that is credible and responsive to program expectations, requirements, \\nand constraints on the project, including resources. \\nDuring Phase A, activities are performed to fully develop a baseline mission concept, begin or assume responsibility for the development of needed tech -\\nnologies, and clarify expected reliance on human elements to achieve full system functionality or \\nautonomous system development. This work, along \\nwith interactions with stakeholders, helps mature the mission concept and the program requirements on the project. Systems engineers are heavily involved during this phase in the development and assessment \\nof the architecture and the allocation of requirements \\nto the architecture elements.\\nIn Phase A, a team—often associated with a program \\nor informal project office—readdresses the mission concept first developed in Pre-Phase A to ensure that \\nthe project justification and practicality are sufficient \\nto warrant a place in NASA’s budget. The team’s effort focuses on analyzing mission requirements and estab -\\nlishing a mission architecture. Activities become for -\\nmal, and the emphasis shifts toward optimizing the \\nconcept design. The effort addresses more depth and \\nconsiders many alternatives. Goals and objectives are 243.0 NASA Program/Project Life Cycle\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nSPACE FLIGHT PHASE A:  \\nCONCEPT AND TECHNOLOGY DEVELOPMENT\\nPurpose\\nTo determine the feasibility and desirability of a suggested new system and establish an initial baseline compatibility with NASA’s strategic plans. Develop final mission concept, system-level requirements, needed system technology developments, and program/project technical management plans.\\nTypical Activities and Their Products\\n• Review and update documents baselined in Pre-Phase A if needed\\n• Monitor progress against plans\\n• Develop and baseline top-level requirements and constraints including internal and external interfaces, \\nintegrated logistics and maintenance support, and system software functionality\\n• Allocate system requirements to functions and to next lower level\\n• Validate requirements\\n• Baseline plans\\n >Systems Engineering Management Plan\\n >Human Systems Integration Plan\\n >Control plans such as the Risk Management Plan, Configuration Management Plan, Data Management Plan, Safety and Mission Assurance Plan, and Software Development or Management Plan (See NPR 7150.2)\\n >Other crosscutting and specialty plans such as environmental compliance documentation, acquisition surveillance plan, contamination control plan, electromagnetic interference/electromagnetic compatibility control plan, reliability plan, quality control plan, parts management plan, logistics plan\\n• Develop preliminary Verification and Validation Plan\\n• Establish human rating plan and perform initial evaluations\\n• Develop and baseline mission architecture\\n >Develop breadboards, engineering units or models identify and reduce high risk concepts\\n >Demonstrate that credible, feasible design(s) exist\\n >Perform and archive trade studies\\n >Initiate studies on human systems interactions\\n• Initiate environmental evaluation/National Environmental Policy Act process\\n• Develop initial orbital debris assessment (NASA-STD-8719.14)\\n• Perform technical management\\n >Provide technical cost estimate and range and develop system-level cost-effectiveness model\\n >Define the WBS\\n >Develop SOWs\\n >Acquire systems engineering tools and models\\n(continued)253.0 NASA Program/Project Life Cycle\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nsolidified, and the project develops more definition \\nin the system requirements, top-level system architec -\\nture, and ConOps. Conceptual designs and analyses \\n(including engineering units and physical models, as \\nappropriate) are developed and exhibit more engi -\\nneering detail than in Pre-Phase A. Technical risks are identified in more detail, and technology develop -\\nment needs become focused. A Systems Engineering Management Plan (SEMP) is baselined in Phase A to \\ndocument how NASA systems engineering require -\\nments and practices of NPR 7123.1 will be addressed \\nthroughout the program life cycle.\\nIn Phase A, the effort focuses on allocating func -\\ntions to particular items of hardware, software, and \\nto humans. System functional and performance requirements, along with architectures and designs, become firm as system tradeoffs and subsystem tradeoffs iterate back and forth, while collaborating with subject matter experts in the effort to seek out \\nmore cost-effective designs. A method of determin -\\ning life cycle cost (i.e., system-level cost-effectiveness \\nmodel) is refined in order to compare cost impacts for each of the different alternatives. (Trade studies should precede—rather than follow—system design \\ndecisions.) Major products to this point include an \\naccepted functional baseline for the system and its major end items. The project team conducts the secu -\\nrity categorization of IT systems required by NPR 2810.1 and Federal Information Processing Standard Publication (FIPS PUB) 199. The effort also pro -\\nduces various engineering and management plans to prepare for managing the project’s downstream pro -\\ncesses such as verification and operations.\\n3.5 Project Phase B:  \\nPreliminary Design and \\nTechnology Completion\\nThe purpose of Phase B is for the project team to \\ncomplete the technology development, engineering prototyping, heritage hardware and software assess -\\nments, and other risk-mitigation activities identified in the project Formulation Agreement (FA) and the preliminary design. The project demonstrates that its planning, technical, cost, and schedule baselines developed during Formulation are complete and con -\\nsistent; that the preliminary design complies with its \\nrequirements; that the project is sufficiently mature \\nto begin Phase C; and that the cost and schedule are adequate to enable mission success with acceptable risk. It is at the conclusion of this phase that the project and the Agency commit to accomplishing \\nthe project’s objectives for a given cost and schedule. \\nFor projects with a Life Cycle Cost (LCC) greater than $250 million, this commitment is made with the Congress and the U.S. Office of Management and Budget (OMB). This external commitment is the Agency Baseline Commitment (ABC). Systems  >Establish technical resource estimates\\n• Identify, analyze and update risks\\n• Perform required Phase A technical activities from NPR 7120.5 as applicable\\n• Satisfy Phase A reviews’ entrance/success criteria from NPR 7123.1\\nReviews\\n• SRR\\n• MDR/SDR263.0 NASA Program/Project Life Cycle\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nSPACE FLIGHT PHASE B:  \\nPRELIMINARY DESIGN AND TECHNOLOGY COMPLETION\\nPurpose\\nTo define the project in enough detail to establish an initial baseline capable of meeting mission needs. Develop system structure end product (and enabling product) requirements and generate a preliminary design for each system structure end product.\\nTypical Activities and Their Products\\n• Review and update documents baselined in previous phases\\n• Monitor progress against plans\\n• Develop the preliminary design\\n >Identify one or more feasible preliminary designs including internal and external interfaces\\n >Perform analyses of candidate designs and report results\\n >Conduct engineering development tests as needed and report results\\n >Perform human systems integration assessments\\n >Select a preliminary design solution\\n• Develop operations plans based on matured ConOps\\n >Define system operations as well as Principal Investigator (PI)/contract proposal management, \\nreview, and access and contingency planning\\n• Report technology development results\\n• Update cost range estimate and schedule data (Note that after PDR changes are incorporated and costed, at KDP C this will turn into the Agency Baseline Commitment)\\n• Improve fidelity of models and prototypes used in evaluations\\n• Identify and update risks\\n• Develop appropriate level safety data package and security plan\\n• Develop preliminary plans\\n >Orbital Debris Assessment\\n >Decommissioning Plan\\n >Disposal Plan\\n• Perform required Phase B technical activities from NPR 7120.5 as applicable\\n• Satisfy Phase B reviews’ entrance/success criteria from NPR 7123.1\\nReviews\\n• PDR\\n• Safety review273.0 NASA Program/Project Life Cycle\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nengineers are involved in this phase to ensure the \\npreliminary designs of the various systems will work together, are compatible, and are likely to meet the \\ncustomer expectations and applicable requirements.\\nDuring Phase B, activities are performed to estab -\\nlish an initial project baseline, which (according to \\nNPR 7120.5 and NPR 7123.1) includes “a formal flow down of the project-level performance require -\\nments to a complete set of system and subsystem design specifications for both flight and ground ele -\\nments” and “corresponding preliminary designs.” The technical requirements should be sufficiently detailed to establish firm schedule and cost estimates \\nfor the project. It also should be noted, especially for \\nAO-driven projects, that Phase B is where the top-level requirements and the requirements flowed down to the next level are finalized and placed under con -\\nfiguration control. While the requirements should be baselined in Phase A, changes resulting from the \\ntrade studies and analyses in late Phase A and early \\nPhase B may result in changes or refinement to sys -\\ntem requirements.\\nIt is important in Phase B to validate design decisions \\nagainst the original goals and objectives and ConOps. \\nAll aspects of the life cycle should be considered, including design decisions that affect training, oper -\\nations resource management, human factors, safety, habitability and environment, and maintainability and supportability.\\nThe Phase B baseline consists of a collection of evolv -\\ning baselines covering technical and business aspects \\nof the project: system (and subsystem) requirements and specifications, designs, verification and opera -\\ntions plans, and so on in the technical portion of the baseline, and schedules, cost projections, and man -\\nagement plans in the business portion. Establishment of baselines implies the implementation of configura -\\ntion management procedures. (See \\nSection 6.5 .)Phase B culminates in a series of PDRs, containing the system-level PDR and PDRs for lower level end items as appropriate. The PDRs reflect the succes -\\nsive refinement of requirements into designs. Design issues uncovered in the PDRs should be resolved \\nso that final design can begin with unambiguous \\ndesign-to specifications. From this point on, almost all changes to the baseline are expected to represent successive refinements, not fundamental changes. As noted in \\nFIGURE 2.5-1 , significant design changes at \\nand beyond Phase B become increasingly expensive.\\n3.6 Project Phase C:  \\nFinal Design and Fabrication\\nThe purpose of Phase C is to complete and docu -\\nment the detailed design of the system that meets the detailed requirements and to fabricate, code, \\nor otherwise realize the products. During Phase \\nC, activities are performed to establish a complete design (product baseline), fabricate or produce hard -\\nware, and code software in preparation for integra -\\ntion. Trade studies continue and results are used to validate the design against project goals, objectives, \\nand ConOps. Engineering test units more closely \\nresembling actual hardware are built and tested to establish confidence that the design will function in the expected environments. Human subjects repre -\\nsenting the user population participate in operations \\nevaluations of the design, use, maintenance, training \\nprocedures, and interfaces. Engineering specialty and crosscutting analysis results are integrated into the design, and the manufacturing process and controls are defined and valid. Systems engineers are involved in this phase to ensure the final detailed designs of \\nthe various systems will work together, are compati -\\nble, and are likely to meet the customer expectations \\nand applicable requirements. During fabrication, the systems engineer is available to answer questions and work any interfacing issues that might arise.283.0 NASA Program/Project Life Cycle\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nSPACE FLIGHT PHASE C: FINAL DESIGN AND FABRICATION\\nPurpose\\nTo complete the detailed design of the system (and its associated subsystems, including its operations systems), fabricate hardware, and code software. Generate final designs for each system structure end product.\\nTypical Activities and Their Products\\n• Review and update documents baselined in previous phases\\n• Monitor progress against plans\\n• Develop and document hardware and software detailed designs\\n >Fully mature and define selected preliminary designs\\n >Add remaining lower level design specifications to the system architecture\\n >Perform and archive trade studies\\n >Perform development testing at the component or subsystem level\\n >Fully document final design and develop data package\\n• Develop/refine and baseline plans\\n >Interface definitions\\n >Implementation plans\\n >Integration plans\\n >Verification and validation plans\\n >Operations plans\\n• Develop/refine preliminary plans\\n >Decommissioning and disposal plans, including human capital transition\\n >Spares\\n >Communications (including command and telemetry lists)\\n• Develop/refine procedures for\\n >Refine integration\\n >Manufacturing and assembly\\n >Verification and validation\\n• Fabricate (or code) the product\\n• Identify and update risks\\n• Monitor project progress against project plans\\n• Prepare launch site checkout and post launch activation and checkout\\n• Finalize appropriate level safety data package and updated security plan\\n• Identify opportunities for preplanned product improvement\\n• Refine orbital debris assessment\\n• Perform required Phase C technical activities from NPR 7120.5 as applicable\\n• Satisfy Phase C review entrance/success criteria from NPR 7123.1\\n(continued)293.0 NASA Program/Project Life Cycle\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nReviews\\n• CDR\\n• PRR\\n• SIR\\n• Safety review\\nAll the planning initiated back in Phase A for the \\ntesting and operational equipment, processes and analysis, integration of the crosscutting and engineer -\\ning specialty analysis, and manufacturing processes and controls is implemented. Configuration manage -\\nment continues to track and control design changes as detailed interfaces are defined. At each step in the successive refinement of the final design, correspond -\\ning integration and verification activities are planned in greater detail. During this phase, technical param -\\neters, schedules, and budgets are closely tracked to ensure that undesirable trends (such as an unexpected growth in spacecraft mass or increase in its cost) are recognized early enough to take corrective action. These activities focus on preparing for the CDR, Production Readiness Review (PRR) (if required), and the SIR.\\nPhase C contains a series of CDRs containing the sys-\\ntem-level CDR and CDRs corresponding to the dif -\\nferent levels of the system hierarchy. A CDR for each end item should be held prior to the start of fabrica -\\ntion/production for hardware and prior to the start of coding of deliverable software products. Typically, the sequence of CDRs reflects the integration pro -\\ncess that will occur in the next phase; that is, from lower level CDRs to the system-level CDR. Projects, however, should tailor the sequencing of the reviews to meet the needs of the project. If there is a pro -\\nduction run of products, a PRR will be performed to ensure the production plans, facilities, and personnel are ready to begin production. Phase C culminates with an SIR. Training requirements and preliminary mission operations procedures are created and base -\\nlined. The final product of this phase is a product ready for integration.\\n3.7 Project Phase D:  \\nSystem Assembly, \\nIntegration and Test, Launch\\nThe purpose of Phase D is to assemble, integrate, ver -\\nify, validate, and launch the system. These activities \\nfocus on preparing for the Flight Readiness Review (FRR)/Mission Readiness Review (MRR). Activities include assembly, integration, verification, and vali -\\ndation of the system, including testing the flight sys -\\ntem to expected environments within margin. Other activities include updating operational procedures, rehearsals and training of operating personnel and crew members, and implementation of the logistics and spares planning. For flight projects, the focus of activities then shifts to prelaunch integration and launch. System engineering is involved in all aspects of this phase including answering questions, provid -\\ning advice, resolving issues, assessing results of the verification and validation tests, ensuring that the V&V results meet the customer expectations and applicable requirements, and providing information to decision makers for go/no-go decisions.\\nThe planning for Phase D activities was initiated \\nin Phase A. For IT projects, refer to the IT Systems \\nEngineering Handbook . The planning for the activ -\\nities should be performed as early as possible since 303.0 NASA Program/Project Life Cycle\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nSPACE FLIGHT PHASE D:  \\nSYSTEM ASSEMBLY, INTEGRATION AND TEST, LAUNCH\\nPurpose\\nTo assemble and integrate the system (hardware, software, and humans), meanwhile developing confidence that it will be able to meet the system requirements. Launch and prepare for operations. Perform system end product implementation, assembly, integration and test, and transition to use.\\nTypical Activities and Their Products\\n• Update documents developed and baselined in previous phases\\n• Monitor project progress against plans\\n• Identify and update risks\\n• Integrate/assemble components according to the integration plans\\n• Perform verification and validation on assemblies according to the V&V Plan and procedures\\n >Perform system qualification verifications, including environmental verifications\\n >Perform system acceptance verifications and validation(s) (e.g., end-to-end tests encompassing all \\nelements; i.e., space element, ground system, data processing system)\\n >Assess and approve verification and validation results\\n >Resolve verification and validation discrepancies\\n >Archive documentation for verifications and validations performed\\n >Baseline verification and validation report\\n• Prepare and baseline\\n >Operator’s manuals\\n >Maintenance manuals\\n >Operations handbook\\n• Prepare launch, operations, and ground support sites including training as needed\\n >Train initial system operators and maintainers\\n >Train on contingency planning\\n >Confirm telemetry validation and ground data processing\\n >Confirm system and support elements are ready for flight\\n >Provide support to the launch and checkout of the system\\n >Perform planned on-orbit operational verification(s) and validation(s)\\n• Document lessons learned. Perform required Phase D technical activities from NPR 7120.5\\n• Satisfy Phase D reviews’ entrance/success criteria from NPR 7123.1\\nReviews\\n• Test Readiness Reviews (TRRs)\\n• System Acceptance Review (SAR) or pre-Ship Review\\n• ORR\\n(continued)313.0 NASA Program/Project Life Cycle\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n• FRR\\n• System functional and physical configuration audits\\n• Safety review\\nchanges at this point can become costly. Phase D \\nconcludes with a system that has been shown to be capable of accomplishing the purpose for which it was created.\\n3.8 Project Phase E:  \\nOperations and Sustainment\\nThe purpose of Phase E is to conduct the prime mission to meet the initially identified need and to maintain support for that need. The products of the phase are the results of the mission and performance of the system.\\nSystems engineering personnel continue to play a \\nrole during this phase since integration often overlaps with operations for complex systems. Some programs have repeated operations/flights which require con -\\nfiguration changes and new mission objectives with each occurrence. And systems with complex sustain -\\nment needs or human involvement will likely require evaluation and adjustments that may be beyond the scope of operators to perform. Specialty engineering disciplines, like maintainability and logistics servic -\\ning, will be performing tasks during this phase as well. Such tasks may require reiteration and/or recur -\\nsion of the common systems engineering processes.\\nSystems engineering personnel also may be involved \\nin in-flight anomaly resolution. Additionally, soft -\\nware development may continue well into Phase E. For example, software for a planetary probe may be developed and uplinked while in-flight. Another example would be new hardware developed for space station increments.This phase encompasses the evolution of the system only insofar as that evolution does not involve major changes to the system architecture. Changes of that scope constitute new “needs,” and the project life cycle starts over. For large flight projects, there may be an extended period of cruise, orbit insertion, on-orbit assembly, and initial shakedown operations. Near the end of the prime mission, the project may apply for a mission extension to continue mission activities or attempt to perform additional mission objectives.\\nFor additional information on systems engineering in \\nPhase E, see \\nAppendix T .\\n3.9 Project Phase F: Closeout\\nThe purpose of Phase F is to implement the systems decommissioning and disposal planning and analyze any returned data and samples. The products of the phase are the results of the mission. The system engi -\\nneer is involved in this phase to ensure all technical information is properly identified and archived, to answer questions, and to resolve issues as they arise.\\nPhase F deals with the final closeout of the sys-\\ntem when it has completed its mission; the time at which this occurs depends on many factors. For a flight system that returns to Earth after a short mis-sion duration, closeout may require little more than de-integrating the hardware and returning it to its owner. On flight projects of long duration, closeout may proceed according to established plans or may begin as a result of unplanned events, such as fail -\\nures. Refer to NASA Policy Directive (NPD) 8010.3, Notification of Intent to Decommission or Terminate Operating Space Systems and Terminate Missions, 323.0 NASA Program/Project Life Cycle\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nSPACE FLIGHT PHASE E: OPERATIONS AND SUSTAINMENT\\nPurpose\\nTo conduct the mission and meet the initially identified need and maintain support for that need. Implement the mission operations plan.\\nTypical Activities and Their Products\\n• Conduct launch vehicle performance assessment. Commission and activate science instruments\\n• Conduct the intended prime mission(s)\\n• Provide sustaining support as planned\\n >Implement spares plan\\n >Collect engineering and science data\\n >Train replacement operators and maintainers\\n >Train the flight team for future mission phases (e.g., planetary landed operations)\\n >Maintain and approve operations and maintenance logs\\n >Maintain and upgrade the system\\n >Identify and update risks\\n >Address problem/failure reports\\n >Process and analyze mission data\\n >Apply for mission extensions, if warranted\\n• Prepare for deactivation, disassembly, decommissioning as planned (subject to mission extension)\\n• Capture lessons learned\\n• Complete post-flight evaluation reports\\n• Develop final mission report\\n• Perform required Phase E technical activities from NPR 7120.5\\n• Satisfy Phase E reviews’ entrance/success criteria from NPR 7123.1\\nReviews\\n• Post-Launch Assessment Review (PLAR)\\n• Critical Event Readiness Review (CERR)\\n• Post-Flight Assessment Review (PFAR) (human space flight only)\\n• DR\\n• System upgrade review\\n• Safety review333.0 NASA Program/Project Life Cycle\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nfor terminating an operating mission. Alternatively, \\ntechnological advances may make it uneconomical to continue operating the system either in its current configuration or an improved one.\\nTo limit space debris, NPR 8715.6, NASA Procedural \\nRequirements for Limiting Orbital Debris, provides requirements for removing Earth-orbiting robotic satellites from their operational orbits at the end of their useful life. For Low Earth Orbit (LEO) mis -\\nsions, the satellite is usually deorbited. For small sat -\\nellites, this is accomplished by allowing the orbit to slowly decay until the satellite eventually burns up in Earth’s atmosphere. Larger, more massive satel -\\nlites and observatories should be designed to demise or deorbit in a controlled manner so that they can be safely targeted for impact in a remote area of the ocean. The Geostationary (GEO) satellites at 35,790 km above the Earth cannot be practically deorbited, so they are boosted to a higher orbit well beyond the crowded operational GEO orbit.\\nIn addition to uncertainty about when this part of \\nthe phase begins, the activities associated with safe closeout of a system may be long and complex and may affect the system design. Consequently, different options and strategies should be considered during the project’s earlier phases along with the costs and risks associated with the different options.\\n3.10 Funding: The Budget Cycle\\nFor a description of the NASA Budget Cycle, refer to the NASA Expanded Guidance for Systems Engineering document found at \\nhttps://nen.nasa.gov/\\nweb/se/doc-repository . See also Section 5.8 of NASA/\\nSP-2014-3705, NASA Space Flight Program and Project Management Handbook .PHASE F: CLOSEOUT\\nPurpose\\nTo implement the systems decommissioning/disposal plan developed in Phase E and perform analyses of the returned data and any returned samples.\\nTypical Activities and Their Products\\n• Dispose of the system and supporting processes\\n• Document lessons learned\\n• Baseline mission final report\\n• Archive data\\n• Capture lessons learned\\n• Perform required Phase F technical activities from NPR 7120.5\\n• Satisfy Phase F reviews’ entrance/success criteria from NPR 7123.1\\nReviews\\n• DRR343.0 NASA Program/Project Life Cycle\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n3.11 Tailoring and Customization \\nof NPR 7123.1 Requirements\\nIn this section, the term requirements  refers to the \\n“shall” statements imposed from Agency direc -\\ntives. This discussion focuses on the tailoring of the \\nrequirements contained in NPR 7123.1.\\n3.11.1  Introduction\\nNASA policy recognizes the need to accommodate the \\nunique aspects of each program or project to achieve mission success in an efficient and economical man -\\nner. Tailoring is a process used to accomplish this.\\nNPR 7123.1 defines tailoring  as “the process used \\nto seek relief from SE NPR  requirements  consistent \\nwith program  or project  objectives, allowable risk, \\nand constraints .” Tailoring results in deviations or \\nwaivers (see NPR 7120.5, Section 3.5) to SE require -\\nments and is documented in the next revision of the \\nSEMP (e.g., via the Compliance Matrix).\\nSince NPR 7123.1 was written to accommodate pro -\\ngrams and projects regardless of size or complexity, \\nthe NPR requirements leave considerable latitude for interpretation. Therefore, the term “customization” is introduced and is defined as “the modification of recommended SE practices that are used to accom -\\nplish the SE requirements.” Customization does not \\nrequire waivers or deviations, but significant custom -\\nization should be documented in the SEMP.\\nTailoring and customization are essential systems \\nengineering tools that are an accepted and expected \\npart of establishing the proper SE NPR require -\\nments for a program or project. Although tailoring is \\nexpected for all sizes of projects and programs, small projects present opportunities and challenges that are different from those of large, traditional projects such as the Shuttle, International Space Station, Hubble \\nSpace Telescope, and Mars Science Laboratory.While the technical aspects of small projects are gen -\\nerally narrower and more focused, they can also be \\nchallenging when their objectives are to demonstrate advanced technologies or provide “one of a kind” capabilities. At the same time, their comparatively small budgets and restricted schedules dictate lean \\nand innovative implementation approaches to proj -\\nect management and systems engineering. Tailoring \\nand customization allow programs and projects to be successful in achieving technical objectives within cost and schedule constraints. The key is effective tai -\\nloring that reflects lessons learned and best practices. Tailoring the SE requirements and customizing the SE best practices to the specific needs of the project helps to obtain the desired benefits while eliminating unnecessary overhead. To accomplish this, an accept -\\nable risk posture must be understood and agreed \\nupon by the project, customer/stakeholder, Center \\nmanagement, and independent reviewers. Even with this foundation, however, the actual process of appro -\\npriately tailoring SE requirements and customizing NPR 7123.1 practices to a specific project can be \\ncomplicated and arduous. Effective approaches and \\nexperienced mentors make the tailoring process for any project more systematic and efficient.\\nChapter 6 of the NASA Software Engineering \\nHandbook  provides guidance on tailoring SE require -\\nments for software projects.\\n3.11.2  Criteria for Tailoring\\nNPR 8705.4, Risk Classification for NASA Payloads, is intended for assigning a risk classification to projects and programs. It establishes baseline criteria that enable \\nusers to define the risk classification level for NASA \\npayloads on human or non-human-rated launch sys -\\ntems or carrier vehicles. It is also a starting point for understanding and defining criteria for tailoring.\\nThe extent of acceptable tailoring depends on several char -\\nacteristics of the program/project such as the following:353.0 NASA Program/Project Life Cycle\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n1. Type of mission. For example, the requirements \\nfor a human space flight mission are much more \\nrigorous than those for a small robotic mission.\\n2. Criticality of the mission in meeting the Agency \\nStrategic Plan. Critical missions that absolutely must be successful may not be able to get relief \\nfrom NPR requirements.\\n3. Acceptable risk level.  If the Agency and the \\ncustomer are willing to accept a higher risk of \\nfailure, some NPR requirements may be waived.\\n4. National significance.  A project that has great \\nnational significance may not be able to get relief from NPR requirements.\\n5. Complexity.  Highly complex missions may \\nrequire more NPR requirements in order to keep systems compatible, whereas simpler ones may \\nnot require the same level of rigor.\\n6. Mission lifetime.  Missions with a longer lifetime \\nneed to more strictly adhere to NPR require -\\nments than short-lived programs/projects.\\n7. Cost of mission.  Higher cost missions may \\nrequire stricter adherence to NPR requirements \\nto ensure proper program/project control.\\n8. Launch constraints.  If there are several launch \\nconstraints, a project may need to be more fully compliant with Agency requirements.\\n3.11.3   Tailoring SE NPR Requirements \\nUsing the Compliance Matrix\\nNPR 7123.1 includes a Compliance Matrix \\n(Appendix H.2) to assist programs and projects in verifying that they meet the specified NPR require -\\nments. The Compliance Matrix documents the pro -\\ngram/project’s compliance or intent to comply with the requirements of the NPR or justification for tai -\\nloring. The Compliance Matrix can be used to assist in identifying where major customization of the way (e.g., formality and rigor) the NPR requirements will be accomplished and to communicate that cus -\\ntomization to the stakeholders. The tailoring pro -\\ncess (which can occur at any time in the program or project’s life cycle) results in deviations or waivers to the NPR requirements depending on the timing of the request. Deviations and waivers of the require -\\nments can be submitted separately to the Designated Governing Authority or via the Compliance Matrix. \\nThe Compliance Matrix is attached to the Systems \\nEngineering Management Plan (SEMP) when sub -\\nmitted for approval. Alternatively, if there is no stand-alone SEMP and the contents of the SEMP are incorporated into another document such as the \\nproject plan, the Compliance Matrix can be captured \\nwithin that plan.\\nFIGURE 3.11-1  illustrates a notional tailoring process \\nfor a space flight project. Project management (such as the project manager/the Principal Investigator/\\nthe task lead, etc.) assembles a project team to tailor \\nthe NPR requirements codified in the Compliance Matrix. To properly classify the project, the team (chief engineer, lead systems engineer, safety and mis -\\nsion assurance, etc.) needs to understand the building blocks of the project such as the needs, goals, and \\nobjectives as well as the appropriate risk posture.\\nThrough an iterative process, the project team goes \\nthrough the NPR requirements in the Compliance Matrix to tailor the requirements. A tailoring tool \\nwith suggested guidelines may make the tailoring \\nprocess easier if available. Several NASA Centers including LaRC and MSFC have developed tools for use at their Centers which could be adapted for other Centers. Guidance from Subject Matter Experts (SMEs) should be sought to determine the \\nappropriate amount of tailoring for a specific project. 363.0 NASA Program/Project Life Cycle\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nThe Compliance Matrix provides rationales for each \\nof the NPR requirements to assist in understanding. Once the tailoring is finalized and recorded in the Compliance Matrix with appropriate rationales, the requested tailoring proceeds through the appropriate governance model for approval.\\n3.11.4   Ways to Tailor a SE \\nRequirement\\nTailoring often comes in three areas:\\n1. Eliminating a requirement that does not apply to \\nthe specific program/project.\\n2. Eliminating a requirement that is overly bur -\\ndensome (i.e., when the cost of implementing the requirement adds more risk to the project by diverting resources than the risk of not comply -\\ning with the requirement).Inputs Outputs\\nProject \\nNeeds, Goals, \\nObjectives\\nTailoring  \\nTool(s)\\nApproved \\nCompliance Matrix \\nAttached to SEMP or \\nProject P lanCenter-level\\nProgram Office\\nEngine ering/Projects Direct orate\\nPM\\nS&MA\\nCE\\nLSERisk  \\nPostureReview/\\nApprove\\nReview/Approve\\nFinal ize/\\nUpdate \\nProject-\\nSpecific \\nTailoring \\nand Capt ure \\nWaiver \\nRationales\\nAdvi sory \\nTeams as \\nNecessarySuggest \\nTailoring N\\nN\\nN YYY\\nProject Team \\nReview \\nand Refine \\nTailoringReview/Approve\\nFIGURE 3.11-1  No tional Space Flight Products Tailoring Process\\n3. Scaling the requirement in a manner that bet -\\nter balances the cost of implementation and the \\nproject risk.\\nCustomizing SE practices can include the following:\\n1. Adjusting the way each of the 17 SE processes is implemented.\\n2. Adjusting the formality and timing of reviews.\\n3.11.4.1   Non-Applicable NPR Requirements\\nEach requirement in NPR 7123.1 is assessed for applicability to the individual project or program. For example, if the project is to be developed com -\\npletely in-house, the requirements of the NPR’s Chapter 4 on contracts would not be applicable. If a system does not contain software, then none of the NPR requirements for developing and maintaining software would be applicable.373.0 NASA Program/Project Life Cycle\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n3.11.4.2   Adjusting the Scope\\nDepending on the project or program, some relief on \\nthe scope of a requirement may be appropriate. For example, although the governing project management \\ndirective (e.g., NPR 7120.5, 7150.2, 7120.7, 7120.8) \\nfor a program/project may require certain documents to be standalone, the SE NPR does not require any additional stand-alone documents. For small proj -\\nects, many of the plans can be described in just a few paragraphs or pages. In these types of projects, any \\nNPR requirements stating that the plans need to be \\nstand-alone document would be too burdensome. In these cases, the information can simply be written and included as part of the project plan or SEMP. If the applicable project management directive (e.g., \\nNPR 7120.5 or NPR 7120.8) requires documents to \\nbe stand-alone, a program/project waiver/deviation is needed. However, if there is no requirement or Center expectation for a stand-alone document, a project can customize where that information is recorded and no waiver or deviation is required. Capturing where this \\ninformation is documented within the systems engi -\\nneering or project management Compliance Matrix \\nwould be useful for clarity.\\n3.11.4.3   Formality and Timing of Reviews\\nThe governing project management directive iden -\\ntifies the required or recommended life cycle for the specific type of program/project. The life cycle \\ndefines the number and timing of the various \\nreviews; however, there is considerable discretion concerning the formality of the review and how to conduct it. NPR 7123.1, Appendix G, provides extensive guidance for suggested review entrance and success criteria. It is expected that the program/\\nproject will customize these criteria in a manner \\nthat makes sense for their program/project. The SE NPR does not require a waiver/deviation for this customization; however, departures from review ele -\\nments required by other NPRs need to be addressed \\nby tailoring those documents.If a program/project decides it does not need one of \\nthe required reviews, a waiver or deviation is needed. \\nHowever, the SE NPR does not specify a minimum \\namount of spacing for these reviews. A small project may decide to combine the SRR and the SDR (or Mission Definition Review (MDR)) for example. As long as the intent for both reviews is accomplished, \\nthe SE NPR does not require a waiver or deviation. (Note that even though the SE NPR does not require it, a waiver or deviation may still be required in the governing project management NPR.) This customi -\\nzation and/or tailoring should be documented in the Compliance Matrix and/or the review plan or SEMP.\\nUnless otherwise required by the governing project \\nmanagement directives, the formality of the review can be customized as appropriate for the type of program/project. For large projects, it might be \\nappropriate to conduct a very formal review with a \\nformal Review Item Discrepancy (RID)/Request for Action (RFA) process, a summary, and detailed  \\npresentations to a wide audience including boards and  \\npre-boards over several weeks. For small projects, that same review might be done in a few hours across \\na tabletop with a few stakeholders and with issues  \\nand actions simply documented in a word or \\nPowerPoint document.\\nThe NASA Engineering Network Systems \\nEngineering Community of Practice, located at \\nhttps://nen.nasa.gov/web/se  includes document tem -\\nplates for milestone review presentations required by \\nthe NASA SE process.\\n3.11.5   Examples of Tailoring and \\nCustomization\\nTABLE 3.11-1  shows an example of the types of mis -\\nsions that can be defined based on a system that \\nbreaks projects into various types ranging from a very \\ncomplex type A to a much simpler type F. When tai -\\nloring a project, the assignment of specific projects to 383.0 NASA Program/Project Life Cycle\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nTABLE 3.11-1  Example of Program/Project Types\\nCriteria Type A Type B Type C Type D Type E Type F\\nDescription of the \\nTypes of MissionHuman Space Flight or Very Large Science/Robotic MissionsNon-Human Space Flight or Science/Robotic MissionsSmall Science or Robotic MissionsSmaller Science or Technology Missions (ISS payload)Suborbital or Aircraft or Large Ground based MissionsAircraft or Ground based technology demonstrations\\nPriority (Criticality to Agency Strategic Plan) and Acceptable Risk LevelHigh priority, very low (minimized) riskHigh priority,  \\nlow riskMedium priority, medium riskLow priority,  \\nhigh riskLow priority,  \\nhigh riskLow to very low priority, high risk\\nNational Significance Very high High Medium Medium to Low Low Very Low\\nComplexity Very high to high High to Medium Medium to Low Medium to Low Low Low to Very Low\\nMission Lifetime (Primary Baseline Mission)Long. >5 years Medium. 2–5 yearsShort. <2 years Short. <2 years N/A N/A\\nCost Guidance  \\n(estimate LCC)High \\n(greater than ~$1B)High to Medium (~$500M–$1B)Medium to Low (~$100M–$500M)Low \\n(~$50M–$100M) (~$10–50M) (less than $10–15M)\\nLaunch Constraints Critical Medium Few Few to none Few to none N/A\\nAlternative Research \\nOpportunities \\nor Re-flight OpportunitiesNo alternative \\nor re-flight \\nopportunitiesFew or no \\nalternative \\nor re-flight opportunitiesSome or few \\nalternative \\nor re-flight opportunitiesSignificant \\nalternative \\nor re-flight opportunitiesSignificant \\nalternative \\nor re-flight opportunitiesSignificant \\nalternative \\nor re-flight opportunities\\nAchievement of Mission Success CriteriaAll practical \\nmeasures are taken to achieve minimum risk to mission success. The highest assurance standards are used.Stringent assurance standards with only minor compromises in application to maintain a low risk to mission success.Medium risk of not achieving mission success may be acceptable. Reduced assurance standards are permitted.Medium or significant risk of not achieving mission success is permitted. Minimal assurance standards are permitted.Significant risk of not achieving mission success is permitted. Minimal assurance standards are permitted.Significant risk of not achieving mission success is permitted. Minimal assurance standards are permitted.\\nExamples HST, Cassini, JIMO, JWST, MPCV, SLS, ISSMER, MRO, Discovery payloads, ISS Facility Class payloads, Attached ISS payloadsESSP, Explorer payloads, MIDES, ISS complex subrack payloads, PA-1,  \\nARES 1-X, MEDLI, CLARREO,  \\nSAGE III, CalipsoSPARTAN, GAS Can, technology demonstrators, simple ISS, express middeck and subrack payloads, SMEX, MISSE-X, EV-2\\nIRVE-2, IRVE -3 , \\nHiFIRE, HyBoLT, ALHAT, STORRM, Earth Venture IDAWNAir, InFlame, Research, technology demonstrations393.0 NASA Program/Project Life Cycle\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nTABLE 3.11-2  Example of Tailoring NPR 7120  .5 Re quired Project Products\\nType A Type B Type C Type D Type E Type F\\nExample Project Technical Products\\nConcept \\nDocumentationFully CompliantFully CompliantFully CompliantTailor Tailor Tailor\\nMission, Spacecraft, Ground, and Payload ArchitecturesFully CompliantFully CompliantFully CompliantTailor Tailor Tailor\\nProject-Level, System and Subsystem RequirementsFully CompliantFully CompliantFully CompliantFully CompliantTailor Tailor\\nDesign DocumentationFully CompliantFully CompliantFully CompliantFully CompliantTailor Tailor\\nOperations Concept Fully CompliantFully CompliantFully CompliantTailor Tailor Tailor\\nTechnology Readiness Assessment DocumentationFully CompliantFully CompliantFully CompliantTailor Tailor Tailor\\nHuman Systems Integration PlanFully CompliantFully CompliantFully CompliantTailor Tailor Tailor\\nHeritage Assessment DocumentationFully CompliantFully CompliantFully CompliantTailor Tailor Tailor\\nSafety Data PackagesFully CompliantFully CompliantFully CompliantFully CompliantTailor Tailor\\nELV Payload Safety Process DeliverablesFully CompliantFully CompliantFully CompliantFully CompliantFully CompliantNot Applicable\\n Verification and Validation ReportFully CompliantFully CompliantFully \\nCompliantTailor Tailor Tailor\\nOperations HandbookFully CompliantFully CompliantFully CompliantTailor Tailor Not Applicable\\nEnd of Mission Plans Fully CompliantFully CompliantFully CompliantTailor Tailor Tailor\\nMission Report Fully CompliantFully CompliantTailor Tailor Tailor Tailorparticular types should be viewed as guidance, not as \\nrigid characterization. Many projects will have char -\\nacteristics of multiple types, so the tailoring approach may permit more tailoring for those aspects of the project that are simpler and more open to risk and less tailoring for those aspects of the project where complexity and/or risk aversion dominate. These tailoring criteria and definitions of project “types” may vary from Center to Center and from Mission Directorate to Mission Directorate according to what is appropriate for their missions. \\nTABLE 3.11-2  shows \\nan example of how the documentation required of \\n(continued)403.0 NASA Program/Project Life Cycle\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nType A Type B Type C Type D Type E Type F\\nExample Project Plan Control Plans\\nRisk Management \\nPlanFully CompliantFully CompliantFully CompliantTailor Tailor Not Applicable\\nTechnology Development planFully CompliantFully CompliantFully CompliantFully CompliantNot ApplicableNot Applicable\\nSystems Engineering Management PlanFully CompliantFully CompliantFully CompliantTailor Tailor Tailor\\nSoftware Management planFully CompliantFully CompliantTailor Tailor Tailor Tailor\\nVerification and Validation PlanFully CompliantFully CompliantTailor Tailor Tailor Tailor\\nReview Plan Fully \\nCompliantFully CompliantFully CompliantTailor Tailor Tailor\\nIntegrated Logistics \\nSupport PlanFully CompliantFully CompliantFully CompliantTailor Tailor Not Applicable\\nScience Data Management PlanFully CompliantFully CompliantFully CompliantTailor Tailor Not Applicable\\nIntegration Plan Fully CompliantFully CompliantFully CompliantFully CompliantTailor Tailor\\nConfiguration Management PlanFully CompliantFully CompliantFully CompliantFully CompliantTailor Tailor\\nTechnology Transfer (formerly Export) Control PlanFully CompliantFully CompliantFully CompliantFully CompliantTailor Tailor\\nLessons Learned PlanFully CompliantFully CompliantFully CompliantFully CompliantTailor Tailor\\nHuman Rating Certification PackageFully CompliantNot ApplicableNot ApplicableNot ApplicableNot ApplicableNot Applicable\\na program/project might also be tailored or custom -\\nized. The general philosophy is that the simpler, less \\ncomplex projects should require much less documen -\\ntation and fewer formal reviews. Project products should be sensibly scaled.\\n3.11.6   Approvals for Tailoring\\nDeviations and waivers of the requirements for the SE NPR can be submitted separately to the require -\\nments owners or in bulk using the appropriate \\nCompliance Matrix found in NPR 7123.1 Appendix H. If it is a Center that is requesting tailoring of the \\nNPR requirements for standard use at the Center, Appendix H.1 is completed and submitted to the OCE for approval upon request or as changes to the Center processes occur. If a program/project whose \\nresponsibility has been delegated to a Center is seek -\\ning a waiver/deviation from the NPR requirements, \\nthe Compliance Matrix in Appendix H.2 is used. In these cases, the Center Director or designee will approve the waiver/deviation.413.0 NASA Program/Project Life Cycle\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nThe result of this tailoring, whether for a Center or \\nfor a program/project, should also be captured in the next revision of the SEMP along with support -\\ning rationale and documented approvals from the requirement owner. This allows communication of the approved waivers/deviations to the entire project team as well as associated managers. If an indepen -\\ndent assessment is being conducted on the program/project, this also allows appropriate modification of expectations and assessment criteria. \\nTABLE 3.11-3 \\nprovides some examples of tailoring captured within the H.2 Compliance Matrix.\\nTABLE 3.11-3  Example Use  of a Compliance Matrix\\nReq\\nIDSE NPR \\nSectio\\nnRequirement \\nStatementRationale Req. OwnerComply? Justification\\nSE-05 2.1.5.2 For those \\nrequirements owned by Center \\nDirectors, the \\ntechnical team \\nshall complete the \\nCompliance Matrix \\nin Appendix H.2  \\nand include it in the \\nSEMP.For programs and projects, the \\nCompliance Matrix in Appendix H.2 is filled out showing that the \\nprogram/project is compliant with the requirements of this NPR (or a particular Center’s implementation of NPR 7123.1, whichever is applicable) or any tailoring thereof is identified and approved by the \\nCenter Director or designee as part \\nof the program/project SEMP.CD Fully \\nCompliant\\nSE-06 2.1.6.1 The DGA shall \\napprove the \\nSEMP, waiver  \\nauthorizations, and other key technical \\ndocuments to \\nensure independent \\nassessment  of \\ntechnical content.The DGA, who is often the TA, \\nprovides an approval of the SEMPs, waivers to technical requirements \\nand other key technical document \\nto provide assurance of the \\napplicability and technical quality of the products.CD Fully \\nComplaint\\nSE-24 4.2.1 The NASA technical \\nteam  shall  define \\nthe engineering activities for the \\nperiods before \\ncontract  award , \\nduring contract \\nperformance , and \\nupon contract completion in the \\nSEMP.It is important for both the \\ngovernment and contractor \\ntechnical teams to understand what activities will be handled by which organization throughout the \\nproduct life cycle. The contractor(s) will typically develop a SEMP or its \\nequivalent to describe the technical \\nactivities in their portion of the \\nproject, but an overarching SEMP \\nis needed that will describe all \\ntechnical activities across the life \\ncycle whether contracted or not.CD Not ApplicableProject is \\nconducted \\nentirely \\nin-house and \\ntherefore there are no \\ncontracts \\ninvolved423.0 NASA Program/Project Life Cycle\\nNASA SYSTEMS ENGINEERING HANDBOOK43\\nNASA SYSTEMS ENGINEERING HANDBOOK4.0System Design Processes\\nThis chapter describes the activities in the system \\ndesign processes listed in FIGURE 2.1-1 . The chap -\\nter is separated into sections corresponding to pro -\\ncesses 1 to 4 listed in FIGURE 2.1-1 . The tasks within \\neach process are discussed in terms of inputs, activ -\\nities, and outputs. Additional guidance is provided \\nusing examples that are relevant to NASA  projects.\\nThe system design processes are interdependent, highly iterative and recursive processes resulting in a validated set of requirements and a design solution that satisfies a set of stakeholder expectations. There are four system design processes: developing stake -\\nholder expectations, technical requirements, logical decompositions, and design solutions.\\nFIGURE 4.0-1  illustrates the recursive relationship \\namong the four system design processes. These pro -\\ncesses start with a study team collecting and clarifying the stakeholder expectations, including the mission objectives, constraints, design drivers, operational objectives, and criteria for defining mission success. This set of stakeholder expectations and high-level requirements is used to drive an iterative design loop where a straw man architecture/design, the concept of operations, and derived requirements are devel -\\noped. These three products should be consistent with each other and will require iterations and design deci -\\nsions to achieve this consistency. Once consistency is achieved, analyses allow the project team to validate the proposed design against the stakeholder expecta -\\ntions. A simplified validation asks the questions: Will the system work as expected? Is the system achiev -\\nable within budget and schedule constraints? Does the system provide the functionality and fulfill the operational needs that drove the project’s funding approval? If the answer to any of these questions is no, then changes to the design or stakeholder expec -\\ntations will be required, and the process starts again. This process continues until the system—architec -\\nture, ConOps, and requirements—meets the stake -\\nholder expectations.\\nThe depth of the design effort should be sufficient \\nto allow analytical verification of the design to the requirements. The design should be feasible and cred -\\nible when judged by a knowledgeable independent review team and should have sufficient depth to sup -\\nport cost modeling and operational assessment.\\nOnce the system meets the stakeholder expectations, \\nthe study team baselines the products and prepares for the next phase. Often, intermediate levels of decomposition are validated as part of the process. In 444.0 System Design Processes\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nIterate\\tRequirements\\nProgram\\t\\nAuthorityNeeds,\\t\\nGoals\\tand\\t\\nObjectives\\nConstraints\\nSuccess\\t\\nCriteriaDevelop\\t\\nConOpsStakeholder\\tExpectationsIterate\\tConOps\\nIterate\\t\\nExpectations\\nDesign\\tSolution\\tDefinitionDerived\\tand\\t\\nAllocated\\t\\nRequirements\\n•Functional\\n•Performance\\n•Interface\\n•Operational\\n•Safety\\n•“ilities”Validate\\nReq.\\tSetReq.\\tMeet\\t\\nConOps?Inconsistencies\\t -Iterate\\nGoodYesRequirements\\tDefinition\\nNo\\t-Iterate\\nCompare EvaluateSuccess\\t\\nCriteriaIterate\\nLowest\\t\\nLevel?No\\t–Recursive\\tCycle\\nTo\\tProduct\\t\\nRealization\\t\\nProcessesLogical\\t\\nDecomposition\\nDecomposition\\n•Functional\\tFlow\\n•Temporal\\tFlow\\n•Behavioral\\n•Data\\tFlow\\n•States\\tand\\tModesDevelop\\t\\nArchitectureDevelop\\t\\nDesign\\nConOps\\nFIGURE 4.0 -1  Interrelationships among the System Design Processes\\nSYSTEM DESIGN KEYS\\n• Successfully understanding and defining the mission objectives and the concept of operations are keys \\nto capturing the stakeholder expectations, which will translate into quality requirements and operational efficiencies over the life cycle of the project.\\n• Complete and thorough requirements traceability is a critical factor in successful validation of requirements.\\n• Clear and unambiguous requirements will help avoid misunderstanding when developing the overall system and when making major or minor changes.\\n• Document all decisions made during the development of the original design concept in the technical data package. This will make the original design philosophy and negotiation results available to assess future proposed changes and modifications against.\\n• The validation of a design solution is a continuing recursive and iterative process during which the design solution is evaluated against stakeholder expectations.454.0 System Design Processes\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nthe next level of decomposition, the baselined derived \\n(and allocated) requirements become the set of high-level requirements for the decomposed elements and \\nthe process begins again. These system design pro -\\ncesses are primarily applied in Pre-Phase A and con -\\ntinue through Phase C.\\nThe system design processes during Pre-Phase A \\nfocus on producing a feasible design that will lead \\nto Formulation approval. During Phase A, alterna -\\ntive designs and additional analytical maturity are \\npursued to optimize the design architecture. Phase B results in a preliminary design that satisfies the approval criteria. During Phase C, detailed, build-to \\ndesigns are completed.\\nThis is a simplified description intended to demon -\\nstrate the recursive relationship among the system \\ndesign processes. These processes should be used as guidance and tailored for each study team depending \\non the size of the project and the hierarchical level \\nof the study team. The next sections describe each of the four system design processes and their associated products for a given NASA mission.\\n4.1  Stakeholder Expectations \\nDefinition\\nThe Stakeholder Expectations Definition Process is \\nthe initial process within the SE engine that estab -\\nlishes the foundation from which the system is \\ndesigned and the product is realized. The main pur -\\npose of this process is to identify who the stakehold -\\ners are and how they intend to use the product. This \\nis usually accomplished through use-case scenarios (sometimes referred to as Design Reference Missions (DRMs)) and the ConOps.4.1.1 Process Description\\nFIGURE 4.1-1  provides a typical flow diagram for the \\nStakeholder Expectations Definition Process and identifies typical inputs, outputs, and activities to \\nconsider in defining stakeholder expectations.\\n4.1.1.1  Inputs\\nTypical inputs needed for the Stakeholder Expectations \\nDefinition Process include the following:\\n• Initial Customer Expectations: These are the \\nneeds, goals, objectives, desires, capabilities, and other constraints that are received from the cus-\\ntomer for the product within the product layer. \\nFor the top-tier products (final end item), these are the expectations of the originating customer who requested the product. For an end product within the product layer, these are the expectations of the recipient of the end item when transitioned.\\n• Other Stakeholder Expectations: These are the \\nexpectations of key stakeholders other than the customer. For example, such stakeholders may \\nbe the test team that will be receiving the transi -\\ntioned product (end product and enabling prod -\\nucts) or the trainers that will be instructing the \\noperators or managers that are accountable for the product at this layer.\\n• Customer Flow-down Requirements: These are \\nany requirements that are being flowed down or allocated from a higher level (i.e., parent require -\\nments). They are helpful in establishing the expec-tations of the customer at this layer.\\n4.1.1.2  Process Activities\\n4.1.1.2.1  Identify Stakeholders\\nA “stakeholder” is a group or individual that is affected by or has a stake in the product or project. The key players for a project/product are called the \\nkey stakeholders. One key stakeholder is always the 464.0 System Design Processes\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n“customer.” The customer may vary depending on \\nwhere the systems engineer is working in the PBS. For example, at the topmost level, the customer may be the person or organization that is purchasing the product. For a systems engineer working three or four levels down in the PBS, the customer may be the leader of the team that takes the element and inte -\\ngrates it into a larger assembly. Regardless of where the systems engineer is working within the PBS, it is important to understand what is expected by \\nthe customer.Customer F low-down \\nRequirementsInitial Customer\\nExpectations \\nEnabling Product\\nSupport StrategiesConcept of Operations\\nMeasures of Effectiveness Validated Stakeholder\\nExpectations\\nOther Stakeholder\\nExpectations From Project\\n  Establish list of stakeholders\\nDefine stakeholder expec tations in acceptable\\nstatements \\nValidate that deﬁned expec tation statements \\nreﬂect bidirectional traceabilityElicit stakeholder expectations\\nEstablish operations concept and support\\nstrategies\\nAnalyze expectation statements for measures \\nof effectiveness\\nBaseline stakeholder expect ationsObtain stakeholder commitments to the \\nvalidated set of expectations\\nCapture work products from \\nstakeholder expectations activitiesTo Technical Requirements \\nDefinition and Technical Data \\nManagement ProcessesTo Technical Requirements \\nDefinition and Configuration \\nManagement ProcessesTo Technical \\nRequirements Definition and \\nRequirements and Interface \\nManagement Processes\\nFIGURE 4.1 -1 Stakeh older Expectations Definition Process\\nOther interested parties are those who affect the project by providing broad, overarching constraints within which the customers’ needs should be achieved. These parties may be affected by the result -\\ning p\\nroduct, the manner in which the product is \\nused, or have a responsibility for providing life cycle support services. Examples include Congress, advi -\\nsory planning teams, program managers, maintain -\\ners, and mission partners. It is important that the list of stakeholders be identified early in the process, as well as the primary stakeholders who will have the most significant influence over the project.\\nThe customer and users of the system are usually \\neasy to identify. The other key stakeholders may be more difficult to identify and they may change depending on the type of the project and the phase the project is in. \\nTABLE 4.1-1  provides some 474.0 System Design Processes\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nexamples of stakeholders in the life cycle phase that \\nshould be considered.\\n4.1.1.2.2  Understand Stakeholder Expectations\\nThoroughly understanding the customer and other key stakeholders’ expectations for the project/prod -\\nuct is one of the most important steps in the systems engineering process. It provides the foundation upon which all other systems engineering work depends. It helps ensure that all parties are on the same page and that the product being provided will satisfy the cus -\\ntomer. When the customer, other stakeholders, and the systems engineer mutually agree on the func -\\ntions, characteristics, behaviors, appearance, and performance the product will exhibit, it sets more realistic expectations on the customer’s part and helps prevent significant requirements creep later in the life cycle.\\nThrough interviews/discussions, surveys, marketing \\ngroups, e-mails, a Statement of Work (SOW), an initial set of customer requirements, or some other means, stakeholders specify what is desired as an end state or as an item to be produced and put bounds on the achievement of the goals. These bounds may TABLE 4.1-1  Stak eholder Identification throughout the Life Cycle\\nLife-Cycle Stage Example Stakeholders\\nPre-Phase A NASA Headquarters, NASA Centers, Presidential Directives, NASA advisory committees, the \\nNational Academy of Sciences\\nPhase A Mission Directorate, customer, potential users, engineering disciplines, safety organization\\nPhase B Customer, engineering disciplines, safety, crew, operations, logistics, production facilities, suppliers, principle investigators\\nPhase C Customer, engineering disciplines, safety, crew, operations, logistics, production facilities, suppliers, principle investigators\\nPhase D Customer, engineering disciplines, safety, crew, operations, training, logistics, verification team, Flight Readiness Board members\\nPhase E Customer, system managers, operations, safety, logistics, sustaining team, crew, principle investigators, users\\nPhase F Customer, NASA Headquarters, operators, safety, planetary protection, public\\nencompass expenditures (resources), time to deliver, \\nlife cycle support expectations, performance objec -\\ntives, operational constraints, training goals, or other less obvious quantities such as organizational needs or geopolitical goals. This information is reviewed, summarized, and documented so that all parties can come to an agreement on the expectations.\\nFIGURE 4.1-2  shows the type of information needed \\nwhen defining stakeholder expectations and depicts how the information evolves into a set of high-level requirements. The yellow lines depict validation paths. Examples of the types of information that would be defined during each step are also provided.\\nDefining stakeholder expectations begins with \\nthe mission authority  and strategic objectives  that \\nthe mission is meant to achieve. Mission authority changes depending on the category of the mission. For example, science missions are usually driven by NASA Science Mission Directorate strategic plans, whereas the exploration missions may be driven by a Presidential directive. Understanding the objectives of the mission helps ensure that the project team is working toward a common vision. These goals and 484.0 System Design Processes\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nobjectives form the basis for developing the mission, \\nso they need to be clearly defined and articulated.\\nThe project team should also identify the constraints  \\nthat may apply. A “constraint” is a condition that is to \\nbe met. Sometimes a constraint is dictated by external factors such as orbital mechanics, an existing system that must be utilized (external interface), a regu -\\nlatory restriction, or the state of technology; some -\\ntimes constraints are the result of the overall budget environment. Concepts of operation and constraints also need to be included in defining the stakeholder expectations. These identify how the system should be operated to achieve the mission objectives.\\nNOTE:\\n It is extremely important to involve stake -\\nholders in all phases of a project. Such involvement \\nshould be built in as a self-correcting feedback loop that will significantly enhance the chances of mis -\\nsion success. Involving stakeholders in a project builds confidence in the end product and serves as a validation and acceptance with the target audience.Mission \\nGoals\\nOperational Drivers Measurements Mission Drivers\\nExplorationsMission \\nObjectivesOperational \\nObjectivesSuccess\\nCriteriaDesign\\nDrivers\\n• Agency Strategic\\n  Plans• Announcements   of Opportunity• Road Maps• Directed Missions• Science Objectives• Exploration   Objectives• Technology  Demonstration   Objectives• Technology   Development  Objectives• Programmatic  Objectives• Integration and  Test• Launch• On-Orbit• Transfer• Surface• Science Data  Distribution• Maintenance• Logistics• Etc.• Launch Date• Mission Duration• Orbit• Cost Constraints• Etc.• What  measurements?• How well?\\n• What\\n  explorations?• What goals?\\nFIGURE 4.1 -2 Inf ormation Flow for Stakeholder Expectations\\nIn identifying the full set of expectations, the systems \\nengineer will need to interact with various commu -\\nnities, such as those working in the areas of orbital debris, space asset protection, human systems inte -\\ngration, quality assurance, and reliability. Ensuring that a complete set of expectations is captured will help prevent “surprise” features from arising later in the life cycle. For example, space asset protection may require additional encryption for the forward link commands, additional shielding or filtering for RF systems, use of a different frequency, or other design changes that might be costly to add to a system that has already been developed.\\n4.1.1.2.3  Identify Needs, Goals, and Objectives\\nIn order to define the goals and objectives, it is nec -\\nessary to elicit the needs, wants, desires, capabilities, external interfaces, assumptions, and constraints from the stakeholders. Arriving at an agreed-to set of goals and objectives can be a long and arduous task. Proactive iteration with the stakeholders throughout the systems engineering process is the way that all par -\\nties can come to a true understanding of what should be done and what it takes to do the job. It is important 494.0 System Design Processes\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nto know who the primary stakeholders are and who \\nhas the decision authority to help resolve conflicts.\\nNeeds, Goals, and Objectives (NGOs) provide a \\nmechanism to ensure that everyone (implementer, customer, and other stakeholders) is in agreement at the beginning of a project in terms of defining the problem that needs to be solved and its scope. NGOs are not contractual requirements or designs.\\nNeeds are defined in the answer to the question \\n“What problem are we trying to solve?” Goals address what must be done to meet the needs; i.e., what the customer wants the system to do. Objectives \\nexpand on the goals and provide a means to docu -\\nment specific expectations. (Rationale should be pro -\\nvided where needed to explain why the need, goal, \\nor objective exists, any assumptions made, and any other information useful in understanding or man -\\naging the NGO.)\\nWell-written NGOs provide clear traceability from \\nthe needs, then to the goals, and then to objectives. For example, if a given goal does not support a need, or an objective does not support a goal, it should not \\nbe part of the integrated set of NGOs. This traceabil -\\nity helps ensure that the team is actually providing \\nwhat is needed.\\nThe following definitions (source: Applied Space \\nSystems Engineering  edited by Larson, Kirkpatrick, \\nSellers, Thomas, and Verma) are provided to help the \\nreader interpret the NGOs contained in this product.\\n• Need:  A single statement that drives everything \\nelse. It should relate to the problem that the system is supposed to solve but not be the solution. The \\nneed statement is singular. Trying to satisfy more \\nthan one need requires a trade between the two, which could easily result in failing to meet at least one, and possibly several, stakeholder expectations.• Goals:  An elaboration of the need, which con -\\nstitutes a specific set of expectations for the sys -\\ntem. Goals address the critical issues identified \\nduring the problem assessment. Goals need not \\nbe in a quantitative or measurable form, but they should allow us to assess whether the system has \\nachieved  them.\\n• Objectives: Specific target levels of outputs the \\nsystem must achieve. Each objective should relate to a particular goal. Generally, objectives should \\nmeet four criteria. (1) They should be specific \\nenough to provide clear direction, so developers, customers, and testers will understand them. They should aim at results and reflect what the system needs to do but not outline how to implement the solution. (2) They should be measurable, quantifi -\\nable, and verifiable. The project needs to monitor the system’s success in achieving each objective. (3) They should be aggressive but attainable, challenging but reachable, and targets need to be realistic. Objectives “To Be Determined” (TBD) \\nmay be included until trade studies occur, oper -\\nations concepts solidify, or technology matures. \\nObjectives need to be feasible before require -\\nments are written and systems designed. (4) They should be results-oriented focusing on desired outputs and outcomes, not on the methods used \\nto achieve the target (what, not how). It is import -\\nant to always remember that objectives are not \\nrequirements. Objectives are identified during pre-Phase A development and help with the even -\\ntual formulation of a requirements set, but it is the \\nrequirements themselves that are contractually \\nbinding and will be verified against the “as-built” system design.\\nThese stakeholder expectations are captured and are considered as initial until they can be further refined \\nthrough development of the concept of operations \\nand final agreement by the stakeholders.504.0 System Design Processes\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n4.1.1.2.4   Establish Concept of Operations and Support \\nStrategies\\nAfter the initial stakeholder expectations have \\nbeen established, the development of a Concept of Operations (ConOps) will further ensure that the \\ntechnical team fully understands the expectations \\nand how they may be satisfied by the product, and that understanding has been agreed to by the stake -\\nholders. This may lead to further refinement of the initial set of stakeholder expectations if gaps or ambig -\\nuous statements are discovered. These scenarios and \\nconcepts of how the system will behave provide an \\nimplementation-free understanding of the stakehold -\\ners’ expectations by defining what is expected with -\\nout addressing how (the design) to satisfy the need. It captures required behavioral characteristics and \\nthe manner in which people will interact with the \\nsystem. Support strategies include provisions for fab -\\nrication, test, deployment, operations, sustainment, \\nand disposal.\\nThe ConOps is an important component in captur -\\ning stakeholder expectations and is used in defin -\\ning requirements and the architecture of a project. \\nIt stimulates the development of the requirements \\nand architecture related to the user elements of the system. It serves as the basis for subsequent defini -\\ntion documents such as the operations plan, launch and early orbit plan, and operations handbook, and it provides the foundation for the long-range opera -\\ntional planning activities such as operational facili -\\nties, staffing, and network scheduling.\\nThe ConOps is an important driver in the system \\nrequirements and therefore should be considered early \\nin the system design processes. Thinking through the \\nConOps and use cases often reveals requirements and design functions that might otherwise be overlooked. For example, adding system requirements to allow for communication during a particular phase of a mis -\\nsion may require an additional antenna in a specific location that may not be required during the nominal mission. The ConOps should include scenarios for all \\nsignificant operational situations, including known \\noff-nominal situations. To develop a useful and complete set of scenarios, important malfunctions and degraded-mode operational situations should be considered. The ConOps is also an important aide to characterizing life cycle staffing goals and function \\nallocation between humans and systems. In walking \\nthrough the accomplishment of mission objectives, it should become clear when decisions need to be made as to what the human operators are contributing vs. what the systems are responsible for delivering.\\nThe ConOps should consider all aspects of opera -\\ntions including nominal and off-nominal operations \\nduring integration, test, and launch through dis-posal. Typical information contained in the ConOps includes a description of the major phases; operation \\ntimelines; operational scenarios and/or DRM (see \\nFIGURE 4.1-3  for an example of a DRM); fault man -\\nagement strategies, description of human interaction \\nand required training, end-to-end communications \\nstrategy; command and data architecture; opera -\\ntional facilities; integrated logistic support (resup -\\nply, maintenance, and assembly); staffing levels and \\nrequired skill sets; and critical events. The operational scenarios describe the dynamic view of the systems’ operations and include how the system is perceived \\nto function throughout the various modes and mode \\ntransitions, including interactions with external inter -\\nfaces, response to anticipated hazard and faults, and during failure mitigations. For exploration missions, multiple DRMs make up a ConOps. The design and \\nperformance analysis leading to the requirements \\nshould satisfy all of them.\\nAdditional information on the development of the \\nConOps is discussed in Section 4.1.2.1 of the NASA Expanded Guidance for Systems Engineering docu -\\nment found \\nhttps://nen.nasa.gov/web/se/doc-repository . 514.0 System Design Processes\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nCONCEPT OF OPERATIONS VS. OPERATIONS CONCEPT\\nConcept of Operations\\nDeveloped early in Pre-Phase A by the technical team, describes the overall high-level concept of how the system will be used to meet stakeholder expectations, usually in a time sequenced manner. It describes the system from an operational perspective and helps facilitate an understanding of the system goals. It stimulates the development of the requirements and architecture related to the user elements of the system. \\nIt serves as the basis for subsequent definition documents and provides the foundation for the long-range \\noperational planning activities.\\nOperations Concept\\nA description of how the flight system and the ground system are used together to ensure that the concept of operation is reasonable. This might include how mission data of interest, such as engineering or scientific data, are captured, returned to Earth, processed, made available to users, and archived for future reference. It is typically developed by the operational team. (See NPR 7120.5.)\\n100 km\\nLow Lunar Orbit\\nLow Earth\\nOrbitAscen t Stage\\n Expended\\nLunarSurfaceAccess\\nModule(LSAM) Crew Exploration VehicleEarth Departure Stage Expended\\nDirect or Skip Land Entry\\nEarthMoon\\nLSAM Performs Lunar Orbit I njection\\nEarth Departure Stage\\nFIGURE 4.1 -3  Example of a Lunar Sortie DRM Early in the Life Cycle524.0 System Design Processes\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nAppendix S  contains one possible outline for developing \\na ConOps. The specific sections of the ConOps will \\nvary depending on the scope and purpose of the project.\\n4.1.1.2.5   Define Stakeholder Expectations in \\nAcceptable Statements\\nOnce the ConOps has been developed, any gaps or \\nambiguities have been resolved, and understanding between the technical team and stakeholders about \\nwhat is expected/intended for the system/product has \\nbeen achieved, the expectations can be formally doc -\\numented. This often comes in the form of NGOs, mission success criteria, and design drivers. These may be captured in a document, spreadsheet, model, or other form appropriate to the product.\\nThe design drivers  will be strongly dependent upon \\nthe ConOps, including the operational environment, \\norbit, and mission duration requirements. For science missions, the design drivers include, at a minimum, \\nthe mission launch date, duration, and orbit, as well \\nas operational considerations. If alternative orbits are to be considered, a separate concept is needed for each orbit. Exploration missions should consider the destination, duration, operational sequence (and sys-tem configuration changes), crew interactions, main -\\ntenance and repair activities, required training, and in situ exploration activities that allow the explora -\\ntion to succeed.\\n4.1.1.2.6   Analyze Expectations Statements for Measures \\nof Effectiveness\\nThe mission success criteria  define what the mission \\nneeds to accomplish to be successful. This could be \\nin the form of science missions, exploration concept \\nfor human exploration missions, or a technological \\ngoal for technology demonstration missions. The success criteria also define how well the concept measurements or exploration activities should be accomplished. The success criteria capture the stake -\\nholder expectations and, along with programmatic requirements and constraints, are used within the high-level requirements.\\nMeasures of Effectiveness (MOEs) are the measures \\nof success that are designed to correspond to accom -\\nplishment of the system objectives as defined by the stakeholder’s expectations. They are stated from the stakeholder’s point of view and represent criteria that \\nare to be met in order for the stakeholder to consider \\nthe project successful. As such, they can be synon -\\nymous with mission/project success criteria. MOEs are developed when the NGOs or other stakeholder expectation documentation is developed. Additional \\ninformation on MOEs is contained in Section 6.7.2.4 \\nof the NASA Expanded Guidance for SE document at \\nhttps://nen.nasa.gov/web/se/doc-repository .\\n4.1.1.2.7   Validate That Defined Expectation Statements \\nReflect Bidirectional Traceability\\nThe NGOs or other stakeholder expectation doc -\\numentation should also capture the source of the \\nexpectation. Depending on the location within the \\nproduct layer, the expectation may be traced to an \\nNGO or a requirement of a higher layer product, to organizational strategic plans, or other sources. Later functions and requirements will be traced to these NGOs. The use of a requirements management tool or model or other application is particularly useful in \\ncapturing and tracing expectations and requirements.\\n4.1.1.2.8   Obtain Stakeholder Commitments to the \\nValidated Set of Expectations\\nOnce the stakeholder and the technical team are \\nin agreement with the expressed stakeholder expec -\\ntations and the concept of operations, signatures or \\nother forms of commitment are obtained. In order to \\nobtain these commitments, a concept review is typi -\\ncally held on a formal or informal basis depending on the scope and complexity of the system (see \\nSection \\n6.7). The stakeholder expectations (e.g., NGOs), \\nMOEs, and concept of operations are presented, discussed, and refined as necessary to achieve final 534.0 System Design Processes\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nagreement. This agreement shows that both sides \\nhave committed to the development of this product.\\n4.1.1.2.9  Baseline Stakeholder Expectations\\nThe set of stakeholder expectations (e.g., NGOs and MOEs) and the concept of operations that are agreed upon are now baselined. Any further changes \\nwill be required to go through a formal or informal \\n(depending on the nature of the product) approval process involving both the stakeholder and the \\ntechnical  team.\\n4.1.1.2.10  Capture Work Products\\nIn addition to developing, documenting, and base -\\nlining stakeholder expectations, the ConOps and MOEs discussed above and other work products from \\nthis process should be captured. These may include \\nkey decisions made, supporting decision rationale and assumptions, and lessons learned in performing these activities.\\n4.1.1.3  Outputs\\nTypical outputs for capturing stakeholder expecta -\\ntions include the following:\\n• Validated Stakeholder Expectations: These are \\nthe agreed-to set of expectations for this product layer. They are typically captured in the form of \\nneeds, goals, and objectives with constraints and \\nassumptions identified. They may also be in the form of models or other graphical forms.\\n• Concept of Operations:  The ConOps describes \\nhow the system will be operated during the life cycle phases that will meet stakeholder expecta -\\ntions. It describes the system characteristics from an operational perspective and helps facilitate an understanding of the system goals and objectives and other stakeholder expectations. Examples would be the ConOps document, model, or a Design Reference Mission (DRM).• Enabling Product Support Strategies: These \\ninclude any special provisions that might be needed for fabrication, test, deployment, opera -\\ntions sustainment, and disposal of the end prod -\\nuct. They identify what support will be needed and any enabling products that will need to be developed in order to generate the end product.\\n• Measures of Effectiveness: A set of MOEs is \\ndeveloped based on the stakeholder expectations. These are measures that represent expectations \\nthat are critical to the success of the system, and \\nfailure to satisfy these measures will cause the stakeholder to deem the system unacceptable.\\nOther outputs that might be generated:\\n• Human/Systems Function Allocation:  This \\ndescribes the interaction of the hardware and software systems with all personnel and their \\nsupporting infrastructure. In many designs (e.g., \\nhuman space flight) human operators are a critical total-system component and the roles and respon -\\nsibilities of the humans-in-the-system should be clearly understood. This should include all human/system interactions required for a mission \\nincluding assembly, ground operations, logistics, \\nin-flight and ground maintenance, in-flight oper -\\nations, etc.\\n4.1.2   Stakeholder Expectations \\nDefinition Guidance\\nRefer to Section 4.1.2 in the NASA Expanded \\nGuidance for Systems Engineering at https://nen.\\nnasa.gov/web/se/doc-repository  for additional guid-\\nance on:\\n• Concept of Operations (including examples),\\n• protection of space assets, and\\n• identification of stakeholders for each phase.544.0 System Design Processes\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n4.2 Technical Requirements \\nDefinition\\nThe Technical Requirements Definition Process \\ntransforms the stakeholder expectations into a defi -\\nnition of the problem and then into a complete set \\nof validated technical requirements expressed as \\n“shall” statements that can be used for defining a design solution for the Product Breakdown Structure (PBS) and related enabling products. The process of requirements definition is a recursive and iterative one that develops the stakeholders’ requirements, prod -\\nuct requirements, and lower level product/compo -\\nnent requirements. The requirements should enable the description of all inputs, outputs, and required relationships between inputs and outputs, including constraints, and system interactions with operators, \\nmaintainers, and other systems. The requirements \\ndocuments organize and communicate requirements to the customer and other stakeholders and the tech -\\nnical community.\\nNOTE: \\nIt is important to note that the team \\nmust not rely solely on the requirements received to \\ndesign and build the system. Communication and iteration with the relevant stakeholders are essential to ensure a mutual understanding of each require -\\nment. Otherwise, the designers run the risk of misunderstanding and implementing an unwanted solution to a different interpretation of the require -\\nments. This iterative stakeholder communication is a critically important part of project validation. Always confirm that the right products and results are being developed.\\nTechnical requirements definition activities apply to \\nthe definition of all technical requirements from the program, project, and system levels down to the low -\\nest level product/component requirements document.4.2.1  Process Description\\nFIGURE 4.2-1 provides a typical flow diagram for the \\nTechnical Requirements Definition Process and iden -\\ntifies typical inputs, outputs, and activities to con -\\nsider in addressing technical requirements definition.\\n4.2.1.1  Inputs\\nTypical inputs needed for the requirements process include the following:\\n• Baselined Stakeholder Expectations:  This is the \\nagreed-to set of stakeholder expectations (e.g., needs, goals, objectives, assumptions, constraints, \\nexternal interfaces) for the product(s) of this prod -\\nuct layer.\\n• Baselined Concept of Operations: This describes \\nhow the system will be operated during the life \\ncycle phases to meet stakeholder expectations. It \\ndescribes the system characteristics from an oper -\\national perspective and helps facilitate an under -\\nstanding of the system goals, objectives, and \\nconstraints. It includes scenarios, use cases, and/or Design Reference Missions (DRMs) as appropriate for the project. It may be in the form of a docu -\\nment, graphics, videos, models, and/or simulations.\\n• Baselined Enabling Support Strategies: These \\ndescribe the enabling products that were identi -\\nfied in the Stakeholder Expectations Definition \\nProcess as needed to develop, test, produce, \\noperate, or dispose of the end product. They also include descriptions of how the end product will be supported throughout the life cycle.\\n• Measures of Effectiveness:  These MOEs were \\nidentified during the Stakeholder Expectations Definition Process as measures that the stake -\\nholders deemed necessary to meet in order for the project to be considered a success (i.e., to meet success criteria).554.0 System Design Processes\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nOther inputs that might be useful in determining the \\ntechnical requirements:\\n• Human/Systems Function Allocation:  This \\ndescribes the interaction of the hardware and software systems with all personnel and their sup -\\nporting infrastructure. When human operators are a critical total-system component, the roles and responsibilities of the humans-in-the-system should be clearly understood. This should include all human/system interactions required for a mis -\\nsion including assembly, ground operations, logis -\\ntics, in-flight and ground maintenance, in-flight operations, etc.Technical Performance\\nMeasures Baselined Stakeholder \\nExpectationsValidated Technical\\nRequirements \\nBaselined Concept of\\nOperations \\nMeasures of\\nEffectiveness Baselined Enabling\\nSupport Strategies From Stakeholder\\nExpectations Definition\\nand  Configuration \\nManagement P rocesses  \\nMeasures of\\nPerformance To Logical Decomposition\\nand Technical Data\\nManagement Processes  \\nTo Technical\\nAssessment Process To Logical Decomposition\\nand Requirements\\n and  Interface Management\\nProcesses    \\nDefine perfo rmance\\nrequi remen ts for each\\ndefined functional and\\nbehavioral expectation  \\nperformance measures   Define design and\\nproduc t constraints \\nValidate technical\\nrequirements \\nEstablish technical\\nrequirements baseline  Define functional and \\nbehavioral expectation in\\ntechnical terms Analyze scope of problem\\nDefine technical require-\\nments in acceptable \\n“shall” statements \\nDefine measures of\\nDefine technicalperformance for each\\nmeasure of effectiveness\\nCapture work products from technical \\nrequirements definition activities\\nFIGURE 4.2 -1 Tec hnical Requirements Definition Process\\n4.2.1.2  Process Activities\\n4.2.1.2.1   Define Constraints, Functional and \\nBehavioral Expectations\\nThe top-level requirements and expectations are ini -\\ntially assessed to understand the technical problem \\nto be solved (scope of the problem) and establish the design boundary. This boundary is typically estab -\\nlished by performing the following activities:\\n• Defining constraints that the design needs to adhere to or that limit how the system will be used. The constraints typically cannot be changed based on trade-off analyses.\\n• Identifying those elements that are already under design control and cannot be changed. This helps 564.0 System Design Processes\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nestablish those areas where further trades will be \\nmade to narrow potential design solutions.\\n• Identifying external and enabling systems with which the system should interact and establishing physical and functional interfaces (e.g., mechani -\\ncal, electrical, thermal, human, etc.).\\n• Defining functional and behavioral expectations for the range of anticipated uses of the system as identified in the ConOps. The ConOps describes how the system will be operated and the possible use-case scenarios.\\n4.2.1.2.2  Define Requirements\\nA complete set of project requirements includes those that are decomposed and allocated down to design elements through the PBS and those that cut across product boundaries. Requirements allocated to the PBS can be functional requirements (what functions need to be performed), performance requirements (how well these functions should be performed), and interface requirements (product to product interaction requirements). Crosscutting require -\\nments include environmental, safety, human factors, and those that originate from the “-ilities” and from Design and Construction (D&C) standards. \\nFIGURE \\n4.2-2 is a general overview on the flow of require -\\nments, what they are called, and who is responsible (owns) for approving waivers.\\n• Functional requirements define what \\nfunctions need to be performed to accomplish the objectives.\\n• Performance requirements define how well the system needs to perform the functions.\\nWith an overall understanding of the constraints, \\nphysical/functional interfaces, and functional/behav -\\nioral expectations, the requirements can be further defined by establishing performance and other tech -\\nnical criteria. The expected performance is expressed as a quantitative measure to indicate how well each product function needs to be accomplished.\\nEXAMPLE OF FUNCTIONAL AND PERFORMANCE REQUIREMENTS\\nInitial Function Statement\\nThe Thrust Vector Controller (TVC) shall provide vehicle control about the pitch and yaw axes.\\nThis statement describes a high-level function that the TVC must perform. The technical team needs to \\ntransform this statement into a set of design-to functional and performance requirements.\\nFunctional Requirements with Associated Performance Requirements\\n• The TVC shall gimbal the engine a maximum of 9 degrees, ± 0.1 degree.\\n• The TVC shall gimbal the engine at a maximum rate of 5 degrees/second ± 0.3 degrees/second.\\n• The TVC shall provide a force of 40,000 pounds, ± 500 pounds.\\n• The TVC shall have a frequency response of 20 Hz, ± 0.1 Hz.574.0 System Design Processes\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nNOTE: Requirements can be generated from \\nnon-obvious stakeholders and may not directly \\nsupport the current mission and its objectives, but instead provide an opportunity to gain additional benefits or information that can support the Agency or the Nation. Early in the process, the systems engineer can help identify potential areas where the system can be used to collect unique information that is not directly related to the primary mission. Often outside groups are not aware of the system goals and capabilities until it is almost too late in the\\xa0process.\\nTechnical requirements come from a number of \\nsources including functional, performance, interface, environmental, safety, human interfaces, standards and in support of the “’ilities” such as reliability, sus -\\ntainability, producibility and others. Consideration and inclusion of all types of requirements is needed in order to form a complete and consistent set of Program\\t\\nRequirements\\nProject\\t\\nRequirementsMission\\tDirectorate\\nImposed\\tRequirements\\nProgram\\nImposed\\t\\nRequirementsSelf-Imposed\\t\\nDerived\\nRequirements\\nSelf-Imposed\\t\\nDerived\\nRequirements\\nLikewise\\tflow\\tto\\t\\nLower\\tLevel\\t\\nSystems“Programmatic”\\t\\nRequirements\\nTechnical\\t\\nRequirementsOwned\\tby\\t\\nProgram/\\t\\nProject\\nOwned\\tby\\t\\nTechnical\\t\\nAuthorityAll\\nSee\\tnote*Ownership Type Flow\\nEx:\\t\\tAt\\tleast\\tone\\tmajor\\t\\nelement\\tshall\\tbe\\tprovided\\tby\\tthe\\tinternational\\tcommunity.\\nEx:\\t\\tThe\\tspacecraft\\tshall\\tprovide\\ta\\tdirect\\tEarth\\tentry\\t\\ncapability\\tfor\\t11500\\tm/s\\tor\\t\\ngreater.Ex:\\t\\tThe\\tspacecraft\\tshall\\tprovide\\ta\\tdirect\\tEarth\\tentry\\tcapability\\tfor\\t11500\\tm/s\\tor\\tgreater.\\nEx:\\t\\tThe\\tsystem\\tshall\\thave\\ta\\t1.4\\tfactor\\tof\\tsafety\\n*\\tRequirements\\t invoked\\tby\\tOCE,\\tOSMA\\tand\\tOCHMO\\tdirectives,\\ttechnical\\tstandards\\t\\tand\\tCenter\\tinstitutional\\trequirements\\nFIGURE 4.2-2  Flow , Type and Ownership of Requirements\\ntechnical requirements from which the system will \\nbe architected and designed. FIGURE 4.2-3  shows an \\nexample of parent and child requirement flowdown.\\n4.2.1.2.3   Define Requirements in Acceptable \\nStatements\\nFinally, the requirements should be defined in accept -\\nable “shall” statements, which are complete sentences \\nwith a single “shall” per statement. Rationale for the requirement should also be captured to ensure the reason and context of the requirement is understood. The Key Driving Requirements (KDRs) should be identified. These are requirements that can have a large impact on cost or schedule when implemented. A KDR can have any priority or criticality. Knowing the impact that a KDR has on the design allows bet -\\nter management of requirements.\\nSee \\nAppendix C  for guidance and a checklist on how \\nto write good requirements and Appendix E  for val -\\nidating requirements. A well-written requirements 584.0 System Design Processes\\nNASA SYSTEMS ENGINEERING HANDBOOK\\ndocument provides several specific benefits to both \\nthe stakeholders and the technical team as shown in \\nTABLE 4.2-1 .System\\nPerformance\\nRequirementsEnvironmental\\nand Other Design\\nRequirements\\nand Guidelines Institutional\\nConstraints\\nAssumptionsImplementing\\nOrganizationsCustomerProgrammatics:\\n• Cost\\n• Schedule\\n• Constraints\\n• Mission ClassificationMission\\nObjectives \\nSystem\\nFunctional\\nRequirements \\nSubsystem A\\nFunctional and\\nPerformance\\nRequirementsSubsystem\\nCMission\\nRequirements \\nAllocated\\nRequirementsDerived\\nRequirementsSubsystem X\\nFunctional and\\nPerformance\\nRequirements\\nAllocated\\nRequirementsDerived\\nRequirementsSubsystem\\nB\\n...Mission\\nAuthority \\nFIGURE 4.2 -3 The  Flowdown of Requirements\\nI\\nt is useful to capture information about each of the \\nrequirements, called metadata, for future reference and use. Many requirements management tools will request or have options for storing this type of infor -\\nm\\nation. TABLE 4.2-2  provides examples of the types of \\nmetadata that might be useful.\\n4.2.1.2.4  Validate T echnical Requirements\\nAn important part of requirements definition is the validation of the requirements against the stakeholder 594.0 System Design Processes\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nTABLE 4.2-1  Benefits of Well-Written Requirements\\nBenefit Rationale\\nEstablish the basis for \\nagreement between the stakeholders and the developers on what the product is to doThe complete description of the functions to be performed by the product specified in the requirements will assist the potential users in determining if the product specified meets their needs or how the product should be modified to meet their needs. During system design, requirements are allocated to subsystems (e.g., hardware, software, and other major components of the system), people, or processes.\\nReduce the development \\neffort because less rework \\nis required to address poorly written, missing, and misunderstood requirementsThe Technical Requirements Definition Process activities force the relevant \\nstakeholders to rigorously consider all of the requirements before design begins. \\nCareful review of the requirements can reveal omissions, misunderstandings, and inconsistencies early in the development cycle when these problems are easier to correct thereby reducing costly redesign, remanufacture, recoding, and retesting in later life cycle phases.\\nProvide a basis for estimating costs and schedulesThe description of the product to be developed as given in the requirements is a realistic basis for estimating project costs and can be used to evaluate bids or price estimates.\\nProvide a baseline for verification and validationOrganizations can develop their verification and validation plans much more productively from a good requirements document. Both system and subsystem test plans and procedures are generated from the requirements. As part of the \\ndevelopment, the requirements document provides a baseline against which \\ncompliance can be measured. The requirements are also used to provide the stakeholders with a basis for acceptance of the system.\\nFacilitate transfer The requirements make it easier to transfer the product. Stakeholders thus find it easier to transfer the product to other parts of their organization, and developers find it easier to transfer it to new stakeholders or reuse it.\\nServe as a basis for enhancementThe requirements serve as a basis for later enhancement or alteration of the finished product.\\nTABLE 4.2 -2  Requirements Metadata\\nItem Function\\nRequirement ID Provides a unique numbering system for sorting and tracking.\\nRationale Provides additional information to help clarify the intent of the requirements at the time they were written. (See “Rationale” box below on what should be captured.)\\nTraced from Captures the bidirectional traceability between parent requirements and lower level (derived) requirements and the relationships between requirements.\\nOwner Person or group responsible for writing, managing, and/or approving changes to this requirement.\\nVerification method Captures the method of verification (test, inspection, analysis, demonstration) and should be determined as the requirements are developed.\\nVerification lead Person or group assigned responsibility for verifying the requirement.\\nVerification level Specifies the level in the hierarchy at which the requirements will be verified (e.g., system, subsystem, element).604.0 System Design Processes\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nRATIONALE\\nThe rationale should be kept up to date and include the following information:\\n• Reason for the Requirement: Often the reason for the requirement is not obvious, and it may be lost if \\nnot recorded as the requirement is being documented. The reason may point to a constraint or concept of operations. If there is a clear parent requirement or trade study that explains the reason, then it should be referenced.\\n• Document Assumptions:  If a requirement was written assuming the completion of a technology \\ndevelopment program or a successful technology mission, the assumption should be documented.\\n• Document Relationships: The relationships with the product’s expected operations (e.g., expectations about how stakeholders will use a product) should be documented. This may be done with a link to the ConOps.\\n• Document Design Constraints: Constraints imposed by the results from decisions made as the design evolves should be documented. If the requirement states a method of implementation, the rationale should state why the decision was made to limit the solution to this one method of implementation.\\nexpectations, the mission objectives and constraints, \\nthe concept of operations, and the mission success criteria. Validating requirements can be broken into six steps:\\n1. Are the Requirements Written Correctly? \\nIdentify and correct requirements “shall” state -\\nment format errors and editorial errors.\\n2. Are the Requirements Technically Correct? A \\nfew trained reviewers from the technical team identify and remove as many technical errors as possible before having all the relevant stakehold -\\ners review the requirements. The reviewers should check that the requirement statements (a) have bidirectional traceability to the baselined stake -\\nholder expectations; (b) were formed using valid assumptions; and (c) are essential to and consis -\\ntent with designing and realizing the appropriate product solution form that will satisfy the appli -\\ncable product life cycle phase success criteria.3. Do the Requirements Satisfy Stakeholders? \\nAll relevant stakeholder groups identify and remove defects.\\n4. Are the Requirements Feasible? All require -\\nments should make technical sense and be possi -\\nble to achieve.\\n5. Are the Requirements Verifiable? All require -\\nments should be stated in a fashion and with enough information that it will be possible to verify the requirement after the end product is implemented.\\n6. Are the Requirements Redundant or Over-\\nspecified?  All requirements should be unique \\n(not redundant to other requirements) and nec -\\nessary to meet the required functions, perfor -\\nmance, or behaviors.614.0 System Design Processes\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nRequirements validation results are often a decid -\\ning factor in whether to proceed with the next pro -\\ncess of Logical Decomposition or Design Solution \\nDefinition. The project team should be prepared to: \\n(1) demonstrate that the project requirements are complete and understandable; (2) demonstrate that evaluation criteria are consistent with requirements and the operations and logistics concepts; (3) con -\\nfirm that requirements and MOEs are consistent \\nwith stakeholder needs; (4) demonstrate that oper -\\nations and architecture concepts support mission \\nneeds, goals, objectives, assumptions, guidelines, and constraints; and (5) demonstrate that the process for managing change in requirements is established, doc -\\numented in the project information repository, and communicated to stakeholders.\\n4.2.1.2.5  Define MOPs and TPMs\\nMeasures of Performance (MOPs) define the perfor -\\nmance characteristics that the system should exhibit when fielded and operated in its intended environment. \\nMOPs are derived from the MOEs but are stated in \\nmore technical terms from the supplier’s point of view. Typically, multiple MOPs, which are quantitative and measurable, are needed to satisfy a MOE, which can be qualitative. From a verification and acceptance point of view, MOPs reflect the system characteristics \\ndeemed necessary to achieve the MOEs.\\nTechnical Performance Measures (TPMs) are phys -\\nical or functional characteristics of the system asso -\\nciated with or established from the MOPs that are \\ndeemed critical or key to mission success. The TPMs \\nare monitored during implementation by comparing the current actual achievement or best estimate of the parameters with the values that were anticipated for the current time and projected for future dates. They are used to confirm progress and identify deficien -\\ncies that might jeopardize meeting a critical system requirement or put the project at cost or schedule risk.For additional information on MOPs and TPMs, their relationship to each other and MOEs, and examples of each, see Section 6.7.2.6.2 of the NASA Expanded Guidance for SE document at \\nhttps://nen.\\nnasa.gov/web/se/doc-repository .\\n4.2.1.2.6  Establish T echnical Requirement Baseline\\nOnce the technical requirements are identified and validated to be good (clear, correct, complete, and achievable) requirements, and agreement has been \\ngained by the customer and key stakeholders, they \\nare baselined and placed under configuration control. Typically, a System Requirements Review (SRR) is held to allow comments on any needed changes and to gain agreement on the set of requirements so that it may be subsequently baselined. For additional infor -\\nmation on the SRR, see \\nSection 6.7 .\\n4.2.1.2.7  Capture Work Products\\nThe work products generated during the above activ -\\nities should be captured along with key decisions that were made, any supporting decision rationale \\nand assumptions, and lessons learned in performing \\nthese activities.\\n4.2.1.3  Outputs\\n• Validated Technical Requirements:  This is the \\napproved set of requirements that represents a complete description of the problem to be solved \\nand requirements that have been validated and \\napproved by the customer and stakeholders. Examples of documents that capture the require -\\nments are a System Requirements Document (SRD), Project Requirements Document (PRD), Interface Requirements Document (IRD), and a \\nSoftware Requirements Specification (SRS).\\n• Measures of Performance: These are the iden -\\ntified quantitative measures that, when met \\nby the design solution, help ensure that one \\nor more MOEs will be satisfied. There may be 624.0 System Design Processes\\nNASA SYSTEMS ENGINEERING HANDBOOK\\ntwo or more MOPs for each MOE. See Section \\n6.7.2.6.2in the NASA Expanded Guidance for Systems Engineering at \\nhttps://nen.nasa.gov/web/\\nse/doc-repository  for further details.\\n• Technical Performance Measures:  These are the \\nset of performance measures that are monitored and trended by comparing the current actual \\nachievement of the parameters with that expected \\nor required at the time. TPMs are used to confirm progress and identify deficiencies. See Section 6.7.2.6.2 in the NASA Expanded Guidance for Systems Engineering at \\nhttps://nen.nasa.gov/web/\\nse/doc-repository  for further details.\\n4.2.2   Technical Requirements \\nDefinition Guidance\\nRefer to Section 4.2.2 of the NASA Expanded \\nGuidance for SE document at https://nen.nasa.gov/\\nweb/se/doc-repository  for additional information on:\\n• types of requirements,\\n• requirements databases, and\\n• the use of technical standards.\\n4.3 Logical Decomposition\\nLogical decomposition is the process for creating the detailed functional requirements that enable NASA programs and projects to meet the stakeholder expec -\\ntations. This process identifies the “what” that should be achieved by the system at each level to enable a successful project. Logical decomposition utilizes functional analysis to create a system architecture and to decompose top-level (or parent) requirements and allocate them down to the lowest desired levels \\nof the project.\\nThe Logical Decomposition Process is used to:\\n• Improve understanding of the defined techni-\\ncal requirements and the relationships among the requirements (e.g., functional, performance, behavioral, and temporal etc.), and\\n• Decompose the parent requirements into a set of logical decomposition models and their associated sets of derived technical requirements for input to \\nthe Design Solution Definition Process.\\n4.3.1  Process Description\\nFIGURE 4.3-1  provides a typical flow diagram for the \\nLogical Decomposition Process and identifies typical \\ninputs, outputs, and activities to consider in address -\\ning logical decomposition.\\n4.3.1.1  Inputs\\nTypical inputs needed for the Logical Decomposition Process include the following:\\n• Technical Requirements: A validated set of \\nrequirements that represent a description of the problem to be solved, have been established by \\nfunctional and performance analysis, and have \\nbeen approved by the customer and other stake -\\nholders. Examples of documents that capture the requirements are an SRD, PRD, and IRD.\\n• Technical Measures:  An established set of mea -\\nsures based on the expectations and requirements that will be tracked and assessed to determine \\noverall system or product effectiveness and cus -\\ntomer satisfaction. These measures are MOEs, \\nMOPs, and a special subset of these called TPMs. See Section 6.7.2.6.2 in the NASA Expanded Guidance for Systems Engineering at \\nhttps://nen.\\nnasa.gov/web/se/doc-repository  for further details.\\n4.3.1.2  Process Activities\\n4.3.1.2.1   Define One or More Logical Decomposition \\nModels\\nThe key first step in the Logical Decomposition \\nProcess is establishing the system architecture model. The system architecture activity defines the 634.0 System Design Processes\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nunderlying structure and relationships of hardware, \\nsoftware, humans-in-the-loop, support personnel, communications, operations, etc., that provide for the implementation of Agency, mission director -\\nate, program, project, and subsequent levels of the requirements. System architecture activities drive the partitioning of system elements and requirements to lower level functions and requirements to the point that design work can be accomplished. Interfaces and relationships between partitioned subsystems and elements are defined as well.From Technical\\n \\nand Configuration\\nManagemenRequirements Definition\\nRequirements Definitiont Processes  \\nTo Technical Data\\nManagement ProcessDeri ved Technical \\nRequirements\\nLogical Decomposition\\nWork ProductsBaselined Technical \\nRequirements\\nMeasures of \\nPerformance \\nTo Design Solution and \\nRequirements and Interface \\nManagement Processes\\nLogical Decomposition\\nModelsFrom Technical\\nand Technical Data\\nManagement Processes To Design Solution\\nand  Configuration\\nManagement Processes   Define one or more logical\\ndecomposition models \\nAllocate technical requirements to\\nlogical decomposition models to form\\na set of derived technical requirements  \\nResolve derived technical \\nrequirements conflicts  \\nValidate the resulting set of derived\\ntechnical requirements  \\nEstablish the derived technical\\nrequirements baseline\\nCapture work products from logical\\ndecomposition activities \\nFIGURE 4.3 -1 Log ical Decomposition Process\\nO\\nnce the top-level (or parent) functional require -\\nments and constraints have been established, the \\nsystem designer uses functional analysis to begin to formulate a conceptual system architecture. The system architecture can be seen as the strategic orga -\\nnization of the functional elements of the system, laid out to enable the roles, relationships, dependen -\\ncies, and interfaces between elements to be clearly defined and understood. It is strategic in its focus o\\nn the overarching structure of the system and how \\nits elements fit together to contribute to the whole, instead of on the particular workings of the elements themselves. It enables the elements to be developed separately from each other while ensuring that they work together effectively to achieve the top-level (or parent) requirements.\\nMuch like the other elements of functional decom -\\nposition, the development of a good system-level \\narchitecture is a creative, recursive, collaborative, and iterative process that combines an excellent under -\\nstanding of the project’s end objectives and constraints with an equally good knowledge of various potential \\ntechnical means of delivering the end  products.\\nFocusing on the project’s ends, top-level (or parent) requirements, and constraints, the system architect should develop at least one, but preferably multiple, 644.0 System Design Processes\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nconcept architectures capable of achieving program \\nobjectives. Each architecture concept involves spec -\\nification of the functional elements (what the pieces \\ndo), their relationships to each other (interface defi -\\nnition), and the ConOps, i.e., how the various seg -\\nments, subsystems, elements, personnel, units, etc., \\nwill operate as a system when distributed by location and environment from the start of operations to the end of the mission.\\nThe development process for the architectural con -\\ncepts should be recursive and iterative with feedback \\nfrom stakeholders and external reviewers, as well as from subsystem designers and operators, provided as \\noften as possible to increase the likelihood of effec -\\ntively achieving the program’s desired ends while \\nreducing the likelihood of cost and schedule overruns.\\nIn the early stages of development, multiple con -\\ncepts are generated. Cost and schedule constraints \\nwill ultimately limit how long a program or project \\ncan maintain multiple architectural concepts. For all NASA programs, architecture design is completed during the Formulation Phase. For most NASA proj -\\nects (and tightly coupled programs), the baselining \\nof a single architecture happens during Phase A. \\nArchitectural changes at higher levels occasionally occur as decomposition to lower levels produces com -\\nplexity in design, cost, or schedule that necessitates such changes. However, as noted in \\nFIGURE 2.5-1 , the \\nlater in the development process that changes occur, the more expensive they become.\\nAside from the creative minds of the architects, there \\nare multiple tools that can be utilized to develop a system’s architecture. These are primarily modeling and simulation tools, functional analysis tools, archi -\\ntecture frameworks, and trade studies. (For example, \\none way of doing architecture is the Department of \\nDefense (DOD) Architecture Framework (DODAF). A search concept is developed, and analytical models of the architecture, its elements, and their operations are developed with increased fidelity as the project evolves. Functional decomposition, requirements development, and trade studies are subsequently undertaken. Multiple iterations of these activities feed back to the evolving architectural concept as the \\nrequirements flow down and the design matures.\\n4.3.1.2.2   Allocate T echnical Requirements, Resolve \\nConflicts, and Baseline\\nFunctional analysis is the primary method used in \\nsystem architecture development and functional requirement decomposition. It is the systematic pro -\\ncess of identifying, describing, and relating the func -\\ntions a system should perform to fulfill its goals and objectives. Functional analysis identifies and links system functions, trade studies, interface character -\\nistics, and rationales to requirements. It is usually based on the ConOps for the system of interest.\\nThree key steps in performing functional analysis are:\\n1. Translate top-level requirements into functions \\nthat should be performed to accomplish the requirements.\\n2. Decompose and allocate the functions to lower levels of the product breakdown structure.\\n3. Identify and describe functional and subsystem interfaces.\\nThe process involves analyzing each system require -\\nment to identify all of the functions that need to be performed to meet the requirement. Each function identified is described in terms of inputs, outputs, failure modes, consequence of failure, and inter -\\nface requirements. The process is repeated from \\nthe top down so that sub-functions are recognized \\nas part of larger functional areas. Functions are arranged in a logical sequence so that any specified operational usage of the system can be traced in an \\nend-to-end  path.654.0 System Design Processes\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nThe process is recursive and iterative and continues \\nuntil all desired levels of the architecture/system have been analyzed, defined, and baselined. There will \\nalmost certainly be alternative ways to decompose \\nfunctions. For example, there may be several ways to communicate with the crew: Radio Frequency (RF), laser, Internet, etc. Therefore, the outcome is highly dependent on the creativity, skills, and experience of the engineers doing the analysis. As the analysis \\nproceeds to lower levels of the architecture and sys -\\ntem, and the system is better understood, the systems \\nengineer should keep an open mind and a willingness to go back and change previously established archi -\\ntecture and system requirements. These changes will \\nthen have to be decomposed down through the archi -\\ntecture and sub-functions again with the recursive \\nprocess continuing until the system is fully defined with all of the requirements understood and known to be viable, verifiable, and internally consistent. Only at that point should the system architecture and \\nrequirements be baselined.\\n4.3.1.2.3  Capture Work Products\\nThe other work products generated during the \\nLogical Decomposition Process should be captured along with key decisions made, supporting decision \\nrationale and assumptions, and lessons learned in \\nperforming the activities.\\n4.3.1.3  Outputs\\nTypical outputs of the Logical Decomposition Process include the following:\\n• Logical Decomposition Models:  These models \\ndefine the relationship of the requirements and functions and their behaviors. They include the \\nsystem architecture models that define the under -\\nlying structure and relationship of the elements \\nof the system (e.g., hardware, software, humans-in-the-loop, support personnel, communications, operations, etc.) and the basis for the partitioning of requirements into lower levels to the point that design work can be accomplished.\\n• Derived Technical Requirements:  These are \\nrequirements that arise from the definitions of the selected architecture that were not explicitly \\nstated in the baselined requirements that served as \\nan input to this process. Both the baselined and derived requirements are allocated to the system architecture and functions.\\n• Logical Decomposition Work Products: These \\nare the other products generated by the activities of this process.\\n4.3.2   Logical Decomposition \\nGuidance\\nRefer to Section 4.3.2 and Appendix F in the NASA \\nExpanded Guidance for Systems Engineering at \\nhttps://nen.nasa.gov/web/se/doc-repository  for addi -\\ntional guidance on:\\n• Product Breakdown Structures and\\n• Functional Analysis Techniques.\\n4.4 Design Solution Definition\\nThe Design Solution Definition Process is used to translate the high-level requirements derived from the stakeholder expectations and the outputs of the \\nLogical Decomposition Process into a design solu -\\ntion. This involves transforming the defined logical \\ndecomposition models and their associated sets of derived technical requirements into alternative solu -\\ntions. These alternative solutions are then analyzed through detailed trade studies that result in the \\nselection of a preferred alternative. This preferred \\nalternative is then fully defined into a final design solution that satisfies the technical requirements. This design solution definition is used to generate the end product specifications that are used to produce 664.0 System Design Processes\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nthe product and to conduct product verification. This \\nprocess may be further refined depending on whether there are additional subsystems of the end product that need to be defined.\\n4.4.1  Process Description\\nFIGURE 4.4-1  provides a typical flow diagram for the \\nDesign Solution Definition Process and identifies typical inputs, outputs, and activities to consider in addressing design solution definition.\\n4.4.1.1 Inputs\\nThere are several fundamental inputs needed to initi -\\nate the Design Solution Definition Process:\\n * To Implementation ProcessBaselined Logical\\nDecomposition\\nModels System-Speci d\\nRequirements To Requirements  and\\nInterface Management Processes \\nInitial Su\\n SpecificationsbsystemTo Stakeholder Expectations Definition\\nand Requirements  and Interface\\nManagement Processes\\nProduct Validation\\nPlan To Product Va lida tion Process\\nLogistics and Operate-\\nTo ProceduresTo Technical Data Management ProcessEnabling Product\\nRequirementsTo Stakeholder Expectations Definition\\nor Product Implementation and\\nRequirements and Interface \\nManagement Processes   From Logical \\nDecomposition  and\\nConfiguration Management\\nProcesses\\nInitiate development\\nof enabling products No Yes\\nInitiate development \\nof next lower level\\nproducts Define alternative design solutions\\nAnalyze ea ch alternative design solution\\nSelect best d esign solution alternative\\nGenerate full design description of the\\nselected solution\\nVerify\\nCapture work products from design solution \\ndefinition activities the fully defin ed design solution\\nBaseline design solution specified requirements\\nand  design descriptions  \\nNo\\n*Need\\nlower level\\nproduct?  Yes\\n*Enabling\\nproduct\\nexists? End Product–Specified\\nRequirements \\nProduct Verification\\nPlan To Product Verification ProcessBaselined Derived\\nTechnical\\nRequirements\\nFIGURE 4.4 -1 De sign Solution Definition Process• Technical Requirements:  These are the customer \\nand stakeholder needs that have been translated into a complete set of validated requirements for the system, including all interface requirements.\\n• Logical Decomposition Models: Requirements \\nare analyzed and decomposed by one or more different methods (e.g., function, time, behavior, data flow, states, modes, system architecture, etc.) in order to gain a more comprehensive under -\\nstanding of their interaction and behaviors. (See the definition of a model in \\nAppendix B .)674.0 System Design Processes\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n4.4.1.2 Process Activities\\n4.4.1.2.1 Define Alternative Design Solutions\\nThe realization of a system over its life cycle involves \\na succession of decisions among alternative courses of action. If the alternatives are precisely defined and thoroughly understood to be well differentiated in the cost-effectiveness space, then the systems engi -\\nneer can make choices among them with confidence.\\nTo obtain assessments that are crisp enough to facili -\\ntate good decisions, it is often necessary to delve more \\ndeeply into the space of possible designs than has yet been done, as illustrated in \\nFIGURE 4.4-2 . It should \\nbe realized, however, that this illustration represents neither the project life cycle, which encompasses the system development process from inception through disposal, nor the product development pro -\\ncess by which the system design is developed and implemented.Recognize \\nneed/\\nopportunityIdentify and \\nquantify goals\\nCreate conceptsIdentify and \\nquantify goals\\nIdentify and \\nquantify goals\\nCreate concepts\\nCreate concepts\\nCreate conceptsIdentify and \\nquantify goals\\nDo trade studies\\nDo trade studies\\nDo trade studiesSelect\\ndesign\\nSelect\\ndesign\\nSelect\\ndesignSelect\\ndesignDo trade studiesIncreaseresolution\\nPerform\\nmission                Implement decisionsIncreaseresolutionIncreaseresolution\\nFIGURE 4.4 -2 The D octrine of Successive Refinement\\nE\\nach “create concepts” step in FIGURE 4.4-2  involves a \\nrecursive and iterative design loop driven by the set of stakeholder expectations where a straw man architec -\\nture/design, the associated ConOps, and the derived requirements are developed and programmatic con -\\nstraints such as cost and schedule are considered. These three products should be consistent with each other and will require iterations and design decisions to achieve this consistency. This recursive and itera -\\ntive design loop is illustrated in\\n FIGURE 4.0-1 .\\nEach “create concepts” step in FIGURE 4.4-2  also \\ninvolves an assessment of potential capabilities offered by the continually changing state of technology and potential pitfalls captured through experience-based review of prior program/project lessons learned data. It is imperative that there be a continual interaction between the technology development process, cross -\\ncutting processes such as human systems integration, and the design process to ensure that the design reflects the realities of the available technology and that overreliance on immature technology is avoided. Additionally, the state of any technology that is con -\\ns\\nidered enabling should be properly monitored, and \\ncare should be taken when assessing the impact of this technology on the concept performance. This interaction is facilitated through a periodic assess-ment of the design with respect to the maturity of the technology required to implement the design. (See Section 4.4.2.1 in the NASA Expanded Guidance for Systems Engineering at \\nhttps://nen.nasa.gov/web/\\nse/doc-repository  for a more detailed discussion of \\ntechnology assessment.) These technology elements usually exist at a lower level in the PBS. Although the process of design concept development by the integration of lower level elements is a part of the systems engineering process, there is always a danger that the top-down process cannot keep up with the bottom-up process. Therefore, system architecture issues need to be resolved early so that the system can be modeled with sufficient realism to do reliable \\ntrade  studies.\\nAs the system is realized, its particulars become clearer—but also harder to change. See the rising 684.0 System Design Processes\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n“Cost to Change Design Direction” in FIGURE 2.5 -1. \\nThe purpose of systems engineering is to make sure \\nthat the Design Solution Definition Process happens \\nin a way that leads to the most functional, safe, and \\ncost-effective final system while working within any given schedule boundaries. The basic idea is that before those decisions that are hard to undo are made, the alternatives should be carefully and itera -\\ntively assessed, particularly with respect both to the \\nmaturity of the required technology and to stake -\\nholder expectations for efficient, effective operations.\\n4.4.1.2.2  Create Alternative Design Concepts\\nOnce it is understood what the system is to accom -\\nplish, it is possible to devise a variety of ways that \\nthose goals can be met. Sometimes, that comes about \\nas a consequence of considering alternative func -\\ntional allocations and integrating available subsystem \\ndesign options, all of which can have technologies at varying degrees of maturity. Ideally, as wide a range of plausible alternatives as is consistent with the design organization’s charter should be defined, keeping in \\nmind the current stage in the process of successive \\nrefinement. When the bottom-up process is operat -\\ning, a problem for the systems engineer is that the designers tend to become fond of the designs they create, so they lose their objectivity; the systems engi -\\nneer should stay an “outsider” so that there is more objectivity. This is particularly true in the assessment of the technological maturity of the subsystems and components required for implementation. There is a tendency on the part of technology developers and project management to overestimate the maturity \\nand applicability of a technology that is required to \\nimplement a design. This is especially true of “heri -\\ntage” equipment. The result is that critical aspects of systems engineering are often overlooked.\\nThe creation of alternative design solutions involves \\nassessment of potential capabilities offered by the continually changing state of technology. A continual interaction between the technology development process and the design process ensures that the design reflects the realities of the available technology. This interaction is facilitated through periodic assessment of the design with respect to the maturity of the tech -\\nnology required to implement the design.\\nAfter identifying the technology gaps existing in \\na given design concept, it is frequently necessary to undertake technology development in order to ascertain viability. Given that resources will always \\nbe limited, it is necessary to pursue only the most \\npromising technologies that are required to enable a given concept.\\nIf requirements are defined without fully under -\\nstanding the resources required to accomplish needed \\ntechnology developments, then the program/project is at risk. Technology assessment should be done iter -\\natively until requirements and available resources are aligned within an acceptable risk posture. Technology development plays a far greater role in the life cycle \\nof a program/project than has been traditionally con -\\nsidered, and it is the role of the systems engineer to \\ndevelop an understanding of the extent of program/project impacts—maximizing benefits and minimiz -\\ning adverse effects. Traditionally, from a program/\\nproject perspective, technology development has \\nbeen associated with the development and incorpo -\\nration of any “new” technology necessary to meet requirements. However, a frequently overlooked area is that associated with the modification of “heritage” systems incorporated into different architectures and \\noperating in different environments from the ones for \\nwhich they were designed. If the required modifica -\\ntions and/or operating environments fall outside the realm of experience, then these too should be consid -\\nered technology development.\\nTo understand whether or not technology develop -\\nment is required—and to subsequently quantify the 694.0 System Design Processes\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nassociated cost, schedule, and risk—it is necessary \\nto systematically assess the maturity of each system, subsystem, or component in terms of the architecture \\nand operational environment. It is then necessary to \\nassess what is required in the way of development to advance the maturity to a point where it can success -\\nfully be incorporated within cost, schedule, and per -\\nformance constraints. A process for accomplishing this assessment is described in \\nAppendix G . Because \\ntechnology development has the potential for such significant impacts on a program/project, technology \\nassessment needs to play a role throughout the design \\nand development process from concept development through Preliminary Design Review (PDR). Lessons learned from a technology development point of view should then be captured in the final phase of \\nthe program.\\nOn the first turn of the successive refinement in \\nFIGURE 4.4-2 , the subject is often general approaches \\nor strategies, sometimes architectural concepts. On the next, it is likely to be functional design, then \\ndetailed design, and so on. The reason for avoiding \\na premature focus on a single design is to permit dis -\\ncovery of the truly best design. Part of the systems engineer’s job is to ensure that the design concepts to be compared take into account all interface require -\\nments. Characteristic questions include: “Did you \\ninclude the cabling?” or “Did you consider how the \\nmaintainers can repair the system?” When possible, each design concept should be described in terms of controllable design parameters so that each rep -\\nresents as wide a class of designs as is reasonable. In \\ndoing so, the systems engineer should keep in mind \\nthat the potentials for change may include organi -\\nzational structure, personnel constraints, schedules, procedures, and any of the other things that make up a system. When possible, constraints should also be described by parameters.4.4.1.2.3  Analyze Each Alternative Design Solution\\nThe technical team analyzes how well each of the design alternatives meets the system objectives (tech -\\nnology gaps, effectiveness, technical achievability, \\nperformance, cost, schedule, and risk, both quanti -\\nfied and otherwise). This assessment is accomplished \\nthrough the use of trade studies. The purpose of the trade study process is to ensure that the system archi -\\ntecture, intended operations (i.e., the ConOps) and design decisions move toward the best solution that \\ncan be achieved with the available resources. The \\nbasic steps in that process are:\\n• Devise some alternative means to meet the func -\\ntional requirements. In the early phases of the project life cycle, this means focusing on system \\narchitectures; in later phases, emphasis is given to \\nsystem designs.\\n• Evaluate these alternatives in terms of the MOPs and system life cycle cost. Mathematical models are useful in this step not only for forcing rec -\\nognition of the relationships among the outcome variables, but also for helping to determine what the MOPs should be quantitatively.\\n• Rank the alternatives according to appropriate selection criteria.\\n• Drop less promising alternatives and proceed to the next level of resolution, if needed.\\nThe trade study process should be done openly and inclusively. While quantitative techniques and rules are used, subjectivity also plays a significant role. To make the process work effectively, participants should have open minds, and individuals with dif -\\nferent skills—systems engineers, design engineers, \\ncrosscutting specialty discipline and domain engi -\\nneers, program analysts, system end users, deci -\\nsion scientists, maintainers, operators, and project 704.0 System Design Processes\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nmanagers—should cooperate. The right quantita -\\ntive methods and selection criteria should be used. \\nTrade study assumptions, models, and results should \\nbe documented as part of the project archives. The \\nparticipants should remain focused on the functional requirements, including those for enabling products. For an in-depth discussion of the trade study process, see \\nSection 6.8 . The ability to perform these studies is \\nenhanced by the development of system models that relate the design parameters to those assessments, but \\nit does not depend upon them.\\nThe technical team should consider a broad range \\nof concepts when developing the system model. The model should define the roles of crew, operators, maintainers, logistics, hardware, and software in \\nthe system. It should identify the critical technolo -\\ngies required to implement the mission and should \\nconsider the entire life cycle from fabrication to disposal. Evaluation criteria for selecting concepts should be established. Cost is always a limiting fac -\\ntor. However, other criteria, such as time to develop and certify a unit, risk, and reliability, also are crit -\\nical. This stage cannot be accomplished without addressing the roles of operators and maintainers. These contribute significantly to life cycle costs and to the system reliability. Reliability analysis should be \\nperformed based upon estimates of component fail -\\nure rates for hardware and an understanding of the \\nconsequences of these failures. If probabilistic risk assessment models are applied, it may be necessary to include occurrence rates or probabilities for software \\nfaults or human error events. These models should \\ninclude hazard analyses and controls implemented through fault management. Assessments of the matu -\\nrity of the required technology should be done and a technology development plan developed.\\nControlled modification and development of design \\nconcepts, together with such system models, often permits the use of formal optimization techniques to find regions of the design space that warrant fur -\\nther investigation.\\nWhether system models are used or not, the design \\nconcepts are developed, modified, reassessed, and \\ncompared against competing alternatives in a closed-\\nloop process that seeks the best choices for further development. System and subsystem sizes are often determined during the trade studies. The end result is the determination of bounds on the relative cost-ef -\\nfectiveness of the design alternatives, measured in terms of the quantified system goals. (Only bounds, rather than final values, are possible because deter -\\nmination of the final details of the design is inten -\\ntionally deferred.) Increasing detail associated with the continually improving resolution reduces the \\nspread between upper and lower bounds as the pro -\\ncess proceeds.\\n4.4.1.2.4  Select the Best Design Solution Alternative\\nThe technical team selects the best design solution \\nfrom among the alternative design concepts, taking into account subjective factors that the team was \\nunable to quantify, such as robustness, as well as \\nestimates of how well the alternatives meet the quan -\\ntitative requirements; the maturity of the available technology; and any effectiveness, cost, schedule, risk, or other constraints.\\nThe Decision Analysis Process, as described in \\nSection 6.8 , should be used to make an evaluation of \\nthe alternative design concepts and to recommend \\nthe “best” design solution.\\nWhen it is possible, it is usually well worth the trou -\\nble to develop a mathematical expression, called an \\n“objective function,” that expresses the values of com -\\nbinations of possible outcomes as a single measure of cost-effectiveness, as illustrated in \\nFIGURE 4.4-3 , even \\nif both cost and effectiveness should be described by more than one measure.714.0 System Design Processes\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nBCA\\nLife-Cycle Cost, Expressed in Constant DollarsSome Aspect of Effectiveness,\\nExpressed in Quantitative Units\\nFIGURE 4.4 -3  A Quantitative Objective Function, Dependent \\non Life Cycle Cost and All Aspects of Effectiveness\\nNote: The different shaded areas indicate different levels \\nof uncertainty  .  Dashed lines represent constant values of \\nobjective function (cost-effectiveness)  . H igher values of cost-\\neffectiveness are achieved by moving toward upper left  .  A, \\nB, and C are design concepts with different risk patterns  .\\nThe objective function (or “cost function”) assigns a \\nreal number to candidate solutions or “feasible solu -\\ntions” in the alternative space or “search space.” A \\nfeasible solution that minimizes (or maximizes, if \\nthat is the goal) the objective function is called an “optimal solution.” When achievement of the goals can be quantitatively expressed by such an objec -\\ntive function, designs can be compared in terms of their value. Risks associated with design concepts \\ncan cause these evaluations to be somewhat nebulous \\nbecause they are uncertain and are best described by probability distributions.\\nIn \\nFIGURE 4.4-3 , the risks are relatively high for \\ndesign concept A. There is little risk in either effec -\\ntiveness or cost for concept B, while the risk of an \\nexpensive failure is high for concept C, as is shown \\nby the cloud of probability near the x axis with a high \\ncost and essentially no effectiveness. Schedule factors may affect the effectiveness and cost values and the risk distributions.\\nThe mission success criteria for systems differ sig -\\nnificantly. In some cases, effectiveness goals may be \\nmuch more important than all others. Other projects may demand low costs, have an immutable sched -\\nule, or require minimization of some kinds of risks. Rarely (if ever) is it possible to produce a combined quantitative measure that relates all of the important \\nfactors, even if it is expressed as a vector with sev -\\neral components. Even when that can be done, it is \\nessential that the underlying actors and relationships be thoroughly revealed to and understood by the sys -\\ntems engineer. The systems engineer should weigh \\nthe importance of the unquantifiable factors along \\nwith the quantitative  data.\\nTechnical reviews of the data and analyses, includ -\\ning technology maturity assessments, are an import -\\nant part of the decision support packages prepared for the technical team. The decisions that are made \\nare generally entered into the configuration manage -\\nment system as changes to (or elaborations of) the system baseline. The supporting trade studies are archived for future use. An essential feature of the systems engineering process is that trade studies are \\nperformed before decisions are made. They can then \\nbe baselined with much more confidence.\\n4.4.1.2.5  Increase the Resolution of the Design\\nThe successive refinement process of FIGURE 4.4-2 \\nillustrates a continuing refinement of the system design. At each level of decomposition, the baselined derived (and allocated) requirements become the set \\nof high-level requirements for the decomposed ele -\\nments, and the process begins again. One might ask, \\n“When do we stop refining the design?” The answer is that the design effort proceeds to a depth that is sufficient to meet several needs: the design should penetrate sufficiently to allow analytical validation 724.0 System Design Processes\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nof the design to the requirements and ConOps; \\nit should also have sufficient depth to support cost and operations modeling and to convince a review \\nteam of a feasible design with performance, cost, and \\nrisk margins.\\nThe systems engineering engine is applied again and \\nagain as the system is developed. As the system is real -\\nized, the issues addressed evolve and the particulars \\nof the activity change. Most of the major system deci -\\nsions (goals, architecture, acceptable life cycle cost, \\netc.) are made during the early phases of the project, so the successive refinements do not correspond pre -\\ncisely to the phases of the system life cycle. Much of the system architecture can be seen even at the out -\\nset, so the successive refinements do not correspond exactly to development of the architectural hierarchy either. Rather, they correspond to the successively greater resolution by which the system is defined.\\nIt is reasonable to expect the system to be defined \\nwith better resolution as time passes. This tendency is formalized at some point (in Phase B) by defin -\\ning a baseline system definition. Usually, the goals, objectives, and constraints are baselined as the requirements portion of the baseline. The entire base -\\nline is then placed under configuration control in an attempt to ensure that any subsequent changes are indeed justified and affordable.\\nAt this point in the systems engineering process, there \\nis a logical branch point. For those issues for which \\nthe process of successive refinement has proceeded far enough, the next step is to implement the decisions at that level of resolution. For those issues that are still insufficiently resolved, the next step is to refine the development further.\\n4.4.1.2.6  Fully Describe the Design Solution\\nOnce the preferred design alternative has been selected and the proper level of refinement has been completed, then the design is fully defined into a final design solution that will satisfy the technical requirements and ConOps. The design solution defi -\\nnition will be used to generate the end product speci -\\nfications that will be used to produce the product and to conduct product verification. This process may be further refined depending on whether there are addi -\\ntional subsystems of the end product that need to \\nbe defined.\\nThe scope and content of the full design description should be appropriate for the product life cycle phase, the phase success criteria, and the product position in \\nthe PBS (system structure). Depending on these fac -\\ntors, the form of the design solution definition could \\nbe simply a simulation model or a paper study report. The technical data package evolves from phase to phase, starting with conceptual sketches or models and ending with complete drawings, parts list, and \\nother details needed for product implementation or \\nproduct integration. Typical output definitions from the Design Solution Definition Process are shown in \\nFIGURE 4.4-1  and are described in Section 4.4.1.3 .\\n4.4.1.2.7 Verify the Design Solution\\nOnce an acceptable design solution has been selected from among the various alternative designs and doc -\\numented in a technical data package, the design \\nsolution should next be verified against the system \\nrequirements and constraints. A method to achieve this verification is by means of a peer review to evaluate the resulting design solution definition. Guidelines for conducting a peer review are discussed in Section 6.7.2.4.5.\\nIn addition, peer reviews play a significant role as a \\ndetailed technical component of higher level tech -\\nnical and programmatic reviews. For example, the peer review of a component battery design can go \\ninto much more technical detail on the battery than \\nthe integrated power subsystem review. Peer reviews 734.0 System Design Processes\\nNASA SYSTEMS ENGINEERING HANDBOOK\\ncan cover the components of a subsystem down to \\nthe level appropriate for verifying the design against the requirements. Concerns raised at the peer \\nreview might have implications on the power sub -\\nsystem design and verification and therefore should \\nbe reported at the next higher level review of the power subsystem.\\nThe verification should show that the design solution \\ndefinition:\\n• Is realizable within the constraints imposed on \\nthe technical effort;\\n• Has specified requirements that are stated in acceptable statements and have bidirectional traceability with the technical requirements and \\nstakeholder expectations; and\\n• Has decisions and assumptions made in forming \\nthe solution consistent with its set of technical requirements and identified system product and \\nservice constraints.\\nThis design solution verification is in contrast \\nto the verification of the end product described in the end product verification plan which is part of the technical data package. That verifi -\\ncation occurs in a later life cycle phase and is a result of the Product Verification Process (see  \\nSection 5.3 ) applied to the realization of the design \\nsolution as an end product.\\n4.4.1.2.8  Validate the Design Solution\\nThe validation of the design solution is a recursive and iterative process as shown in \\nFIGURE 4.0-1 . \\nEach alternative design concept is validated against the set of stakeholder expectations. The stakeholder \\nexpectations drive the iterative design loop in which \\na straw man architecture/design, the ConOps, and the derived requirements are developed. These three products should be consistent with each other and will require iterations and design decisions to achieve this consistency. Once consistency is achieved, func -\\ntional analyses allow the study team to validate the design against the stakeholder expectations. A sim -\\nplified validation asks the questions: Does the system work as expected? How does the system respond to failures, faults, and anomalies? Is the system afford -\\nable? If the answer to any of these questions is no, then changes to the design or stakeholder expecta -\\ntions will be required, and the process is started over \\nagain. This process continues until the system—\\narchitecture, ConOps, and requirements—meets the stakeholder expectations.\\nThis design solution validation is in contrast to \\nthe validation of the end product described in the \\nend-product validation plan, which is part of the technical data package. That validation occurs in a later life cycle phase and is a result of the Product Validation Process (see \\nSection 5.4 ) applied to the \\nrealization of the design solution as an end product.\\n4.4.1.2.9  Identify Enabling Products\\nEnabling products are the life cycle support prod -\\nucts and services (e.g., production, test, deployment, training, maintenance, and disposal) that facili -\\ntate the progression and use of the operational end product through its life cycle. Since the end prod -\\nuct and its enabling products are interdependent, they are viewed as a system. Project responsibility thus extends to responsibility for acquiring services from the relevant enabling products in each life cycle \\nphase. When a suitable enabling product does not \\nalready exist, the project that is responsible for the end product can also be responsible for creating and using the enabling product.\\nTherefore, an important activity in the Design \\nSolution Definition Process is the identification of the enabling products and personnel that will be required 744.0 System Design Processes\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nduring the life cycle of the selected design solution \\nand then initiating the acquisition or development of those enabling products and personnel. Need dates \\nfor the enabling products should be realistically \\nidentified on the project schedules, incorporating appropriate schedule slack. Then firm commitments in the form of contracts, agreements, and/or opera -\\ntional plans should be put in place to ensure that the enabling products will be available when needed to \\nsupport the product life cycle phase activities. The \\nenabling product requirements are documented as part of the technical data package for the Design Solution Definition Process.\\nAn environmental test chamber is an example of an \\nenabling product whose use would be acquired at an appropriate time during the test phase of a space flight system.\\nSpecial test fixtures or special mechanical handling \\ndevices are examples of enabling products that would \\nhave to be created by the project. Because of long development times as well as oversubscribed facil -\\nities, it is important to identify enabling products and secure the commitments for them as early in the \\ndesign phase as possible.\\n4.4.1.2.10  Baseline the Design Solution\\nAs shown earlier in FIGURE 4.0-1 , once the selected \\nsystem design solution meets the stakeholder expec -\\ntations, the study team baselines the products and prepares for the next life cycle phase. Because of the \\nrecursive nature of successive refinement, intermedi -\\nate levels of decomposition are often validated and baselined as part of the process. In the next level of decomposition, the baselined requirements become the set of high-level requirements for the decomposed \\nelements, and the process begins again.\\nBaselining a particular design solution enables the \\ntechnical team to focus on one design out of all the alternative design concepts. This is a critical point in the design process. It puts a stake in the ground and gets everyone on the design team focused on the same concept. When dealing with complex sys-\\ntems, it is difficult for team members to design their \\nportion of the system if the system design is a mov -\\ning target. The baselined design is documented and placed under configuration control. This includes the system requirements, specifications, and configura -\\ntion descriptions.\\nWhile baselining a design is beneficial to the design \\nprocess, there is a danger if it is exercised too early in the Design Solution Definition Process. The early exploration of alternative designs should be free and \\nopen to a wide range of ideas, concepts, and imple -\\nmentations. Baselining too early takes the inventive \\nnature out of the concept exploration. Therefore, baselining should be one of the last steps in the Design Solution Definition Process.\\n4.4.1.3  Outputs\\nOutputs of the Design Solution Definition Process are the specifications and plans that are passed on to the product realization processes. They contain the \\ndesign-to, build-to, train-to, and code-to documen -\\ntation that complies with the approved baseline for \\nthe system.\\nAs mentioned earlier, the scope and content of the \\nfull design description should be appropriate for the \\nproduct life cycle phase, the phase success criteria, \\nand the product position in the PBS.\\nOutputs of the Design Solution Definition Process \\ninclude the following:\\n• The System Specification: The system spec -\\nification contains the functional baseline for the system that is the result of the Design \\nSolution Definition Process. The system design 754.0 System Design Processes\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nspecification provides sufficient guidance, con -\\nstraints, and system requirements for the design \\nengineers to begin developing the design.\\n• The System External Interface Specifications: \\nThe system external interface specifications describe the functional baseline for the behavior and characteristics of all physical interfaces that the \\nsystem has with the external world. These include \\nall structural, thermal, electrical, and signal inter -\\nfaces, as well as the human-system interfaces.\\n• The End-Product Specifications: The end-prod -\\nuct specifications contain the detailed build-to and code-to requirements for the end product. \\nThey are detailed, exact statements of design par -\\nticulars, such as statements prescribing materials, \\ndimensions, and quality of work to build, install, or manufacture the end product.\\n• The End-Product Interface Specifications: The \\nend-product interface specifications contain the detailed build-to and code-to requirements for \\nthe behavior and characteristics of all logical and \\nphysical interfaces that the end product has with external elements, including the human-system interfaces.\\n• Initial Subsystem Specifications:  The end-prod -\\nuct subsystem initial specifications provide detailed information on subsystems if they are required.\\n• Enabling Product Requirements:  The require -\\nments for associated supporting enabling products provide details of all enabling products. Enabling \\nproducts are the life cycle support products, infra -\\nstructures, personnel, logistics, and services that \\nfacilitate the progression and use of the opera -\\ntional end product through its life cycle. They are viewed as part of the system since the end product and its enabling products are interdependent.\\n• Product Verification Plan:  The end-product ver -\\nification plan (generated through the Technical Planning Process) provides the content and \\ndepth of detail necessary to provide full visibil -\\nity of all verification activities for the end prod -\\nuct. Depending on the scope of the end product, \\nthe plan encompasses qualification, acceptance, prelaunch, operational, and disposal verification activities for flight hardware and software.\\n• Product Validation Plan: The end-product val -\\nidation plan (generated through the Technical Planning Process) provides the content and depth \\nof detail necessary to provide full visibility of \\nall activities to validate the end product against the baselined stakeholder expectations. The plan identifies the type of validation, the validation procedures, and the validation environment that are appropriate to confirm that the realized end \\nproduct conforms to stakeholder expectations.\\n• Logistics and Operate-to Procedures: The appli -\\ncable logistics and operate-to procedures for the \\nsystem describe such things as handling, trans -\\nportation, maintenance, long-term storage, and operational considerations for the particular design solution.\\nOther outputs may include:\\n• Human Systems Integration Plan:  The system \\nHSI Plan should be updated to indicate the numbers, skills, and development (i.e., training) \\nrequired for humans throughout the full life cycle \\ndeployment and operations of the system.764.0 System Design Processes\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n4.4.2   Design Solution Definition \\nGuidance\\nRefer to Section 4.4.2 in the NASA Expanded \\nGuidance for Systems Engineering at https://nen.nasa.\\ngov/web/se/doc-repository  for additional guidance on:\\n• technology assessment,\\n• human capability assessment, and\\n• integrating engineering specialties into the SE process.77\\nNASA SYSTEMS ENGINEERING HANDBOOK5.0Product Realization\\nThis chapter describes the activities in the prod -\\nuct realization processes listed in FIGURE 2.1-1 . \\nThe chapter is separated into sections corresponding \\nto steps 5 through 9 listed in FIGURE 2.1-1 . The pro -\\ncesses within each step are discussed in terms of the inputs, the activities, and the outputs. Additional guidance is provided using examples that are relevant to NASA projects.\\nIn the product realization side of the SE engine, five \\ninterdependent processes result in systems that meet the design specifications and stakeholder expecta -\\ntions. These products are produced, acquired, reused, or coded; integrated into higher level assemblies; ver -\\nified against design specifications; validated against stakeholder expectations; and transitioned to the next level of the system. As has been mentioned in previous sections, products can be models and simu -\\nlations, paper studies or proposals, or hardware and software. The type and level of product depends on the phase of the life cycle and the product’s specific objectives. But whatever the product, all should effec -\\ntively use the processes to ensure the system meets the intended operational concept.\\nThis effort starts with the technical team taking the \\noutput from the system design processes and using the appropriate crosscutting functions, such as data and configuration management, and technical assess -\\nments to make, buy, or reuse subsystems. Once these subsystems are realized, they should be integrated to the appropriate level as designated by the appropri -\\nate interface requirements. These products are then verified through the Technical Assessment Process to ensure that they are consistent with the technical data package and that “the product was built right.” Once consistency is achieved, the technical team validates the products against the stakeholder expectations to ensure that “the right product was built.” Upon successful completion of validation, the products are transitioned to the next level of the system. \\nFIGURE \\n5.0-1 illustrates these processes.\\nThis is an iterative and recursive process. Early in the life cycle, paper products, models, and simulations are run through the five realization processes. As the system matures and progresses through the life cycle, hardware and software products are run through these processes. It is important to detect as many errors and failures as possible at the lowest level of integration and early in the life cycle so that changes can be made through the design processes with min -\\nimum impact to the project.785.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nThe next sections describe each of the five product \\nrealization processes and their associated products for a given NASA mission.DESIGN REALIZATIONPRODUCT\\nTRANSITION\\nPROCESSEVALUATION PROCESSES\\n• Acquire\\n• Make/Code\\n• ReuseProduct\\nImplementationProduct\\nIntegrationProduct\\nVerificationProduct\\nValidationProduct\\nTransition\\n• Assembly\\n• Functional\\nEvaluation• Functional\\n• Environmental\\n• Operational Test-ing in Integration\\n& Test Environment• Operational Testing in MissionEnvironment• Delivery to Next\\nHigher Level in PBS\\n• Delivery to OperationalSystem\\nFIGURE 5.0-1 Product Realization\\nPRODUCT REALIZATION KEYS\\n• Define and execute production activities.\\n• Genera\\nte and manage requirements for off-the-shelf hardware/software products as for all other products.\\n• Unders\\ntand the differences between verification testing and validation testing.\\n• Consid\\ner all customer, stakeholder, technical, programmatic, and safety requirements when evaluating \\nthe input necessary to achieve a successful product transition.\\n• Analyz\\ne for any potential incompatibilities with interfaces as early as possible.\\n• Comple\\ntely understand and analyze all test data for trends and anomalies.\\n• Unders\\ntand the limitations of the testing and any assumptions that are made.\\n• Ensure t\\nhat a reused product meets the verification and validation required for the relevant system in \\nwhich it is to be used, as opposed to relying on the original verification and validation it met for the \\nsystem of its original use. Then ensure that it meets the same verification and validation as a purchased product or a built product. The “pedigree” of a reused product in its original application should not be relied upon in a different system, subsystem, or application.\\nfrom the bottom of the product hierarchy up towards \\n5.1 Product Implementation\\nProduct implementation is the first process encoun -\\ntered in the SE engine that begins the movement th\\ne Product Transition Process. This is where the \\nplans, designs, analysis, requirements development, \\nand drawings are realized into actual products.\\nProduct implementation is used to generate a speci -\\nfied product of a project or activity through buying, \\nmaking/coding, or reusing previously developed 795.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nhardware, software, models, or studies to generate a \\nproduct appropriate for the phase of the life cycle. The product should satisfy the design solution and its specified requirements.\\nThe Product Implementation Process is the key activ -\\nity that moves the project from plans and designs \\ninto realized products. Depending on the project and life cycle phase within the project, the product may be hardware, software, a model, simulations, mock-ups, study reports, or other tangible results. These products may be realized through their purchase from commercial or other vendors, through partial or complete reuse of products from other projects or activities, or they may be generated from scratch. The decision as to which of these realization strategies or combination of strategies will be used for the prod -\\nucts of this project will have been made early in the life cycle using the Decision Analysis Process.5.1.1 Process Description\\nFIGURE 5.1-1  provides a typical flow diagram for \\nthe Product Implementation Process and identifies typical inputs, outputs, and activities to consider in addressing product implementation.\\n5.1.1.1 Inputs\\nInputs to the Product Implementation Process depend primarily on the decision about whether the end product will be purchased, developed from scratch, or formed by reusing part or all of products from other projects. Typical inputs are shown in \\nFIGURE 5.1-1 .End Product\\nDocuments a nd\\nManuals From Configuration\\nManagement ProcessDesired End\\nProduct To Product\\nVerification Process\\nEnd Product Design\\nSpecifications and\\nConfiguration\\nDocumentation \\nProduct\\nImplementation–\\nEnabling Products To Technical Data\\nManagement Process\\nProduct\\nImplementation\\nWork Products From existing\\nresou rces or Pr oduct\\nTransition ProcessRequired Raw\\nMaterials From existing\\nresour ces or\\nexternal sources \\nMake the specified end product\\nCapture work products from product\\nimplementation activitiesPrepare to conduct implementation\\nIf implemented by \\nbuying:\\nParticipate in purchase\\nof specified end product\\nPrepare appropriate\\nproduct support documentation If implemented by\\nreuse:\\nParticipate in acquiring\\nthe reuse end product If implemented by making:\\nEvaluate readiness of\\nproduct implementation–\\nenabling products \\nFIGURE 5.1-1  Prod uct Implementation Process\\n• Inputs If Purchasing the End Product: If the \\ndecision was made to purchase part or all of the products for this project, the end product design specifications are obtained from the configura -\\ntion management system as well as other appli -\\ncable documents.805.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n• Inputs If Making/Coding the End Product:  For \\nend products that will be made/coded by the \\ntechnical team, the inputs will be the config -\\nuration-controlled design specifications, man -\\nufacturing plans, manufacturing processes, manufacturing procedures, and raw materials as provided to or purchased by the project.\\n• Inputs Needed If Reusing an End Product: For \\nend products that will reuse part or all of prod -\\nucts generated by other projects, the inputs may \\nbe the documentation associated with the product \\nas well as the product itself. Care should be taken to ensure that these products will indeed meet the specifications and environments for this project. These would have been factors involved in the Decision Analysis Process to determine the make/\\nbuy/reuse decision.\\n• Enabling Products: These would be any enabling \\nproducts necessary to make, code, purchase, or \\nreuse the product (e.g., drilling fixtures, produc -\\ntion facilities, production lines, software devel -\\nopment facilities, software test facilities, system integration and test facilities).\\n5.1.1.2  Process Activities\\nImplementing the product can take one of three forms:\\n1. Purchase/buy\\n2. Make/code\\n3. Reuse\\nThese three forms will be discussed in the following \\nsubsections. FIGURE 5.1-1  shows what kind of inputs, \\noutputs, and activities are performed during product \\nimplementation regardless of where in the product \\nhierarchy or life cycle it is. These activities include pre -\\nparing to conduct the implementation, purchasing/making/reusing the product, and capturing the prod -\\nuct implementation work product. In some cases, \\nimplementing a product may have aspects of more than one of these forms (such as a build-to-print). In those cases, the appropriate aspects of the applicable \\nforms are used.\\n5.1.1.2.1 Prepare to Conduct Implementation\\nPreparing to conduct the product implementation \\nis a key first step regardless of what form of imple -\\nmentation has been selected. For complex projects, \\nimplementation strategy and detailed planning or \\nprocedures need to be developed and documented. For less complex projects, the implementation strat -\\negy and planning need to be discussed, approved, and documented as appropriate for the complexity of the project.\\nThe documentation, specifications, and other inputs \\nalso need to be reviewed to ensure they are ready and at an appropriate level of detail to adequately complete the type of implementation form being employed and \\nfor the product life cycle phase. For example, if the \\n“make” implementation form is being employed, the design specifications need to be reviewed to ensure they are at a design-to level that allows the product to be developed. If the product is to be bought as a pure Commercial Off-the-Shelf (COTS) item, the \\nspecifications need to be checked to make sure they \\nadequately describe the vendor characteristics to nar -\\nrow to a single make/model of their product line.\\nFinally, the availability and skills of personnel needed \\nto conduct the implementation as well as the avail -\\nability of any necessary raw materials, enabling prod -\\nucts, or special services should also be reviewed. Any \\nspecial training necessary for the personnel to per -\\nform their tasks needs to be performed by this time. This is a key part of the Acceptance Data Package.815.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n5.1.1.2.2 Purchase, Make, or Reuse the Product\\nPurchase the Product\\nIn the first case, the end product is to be purchased \\nfrom a commercial or other vendor. Design/purchase specifications will have been generated during require -\\nments development and provided as inputs. The tech -\\nnical team needs to review these specifications and ensure they are in a form adequate for the contract or purchase order. This may include the generation of contracts, Statements of Work (SOWs), requests for proposals, purchase orders, or other purchasing \\nmechanisms. For major end products purchased from \\na vendor, the responsibilities of the Government and contractor team should be documented in the SEMP and Integration Plan. This will define, for example, whether NASA expects the vendor to provide a fully \\nverified and validated product or whether the NASA \\ntechnical team will be performing those duties. The team needs to work with the acquisition team to ensure the accuracy of the contract SOW or purchase order and to ensure that adequate documentation, certificates of compliance, or other specific needs are \\nrequested from the vendor.\\nFor contracted purchases, as proposals come back \\nfrom the vendors, the technical team should work with the contracting officer and participate in the \\nreview of the technical information and in the selec-\\ntion of the vendor that best meets the design require -\\nments for acceptable cost and schedule.\\nAs the purchased products arrive, the technical team \\nshould assist in the inspection of the delivered prod -\\nuct and its accompanying documentation. The team should ensure that the requested product was indeed the one delivered, and that all necessary documenta -\\ntion, such as source code, operator manuals, certifi -\\ncates of compliance, safety information, or drawings \\nhave been received.The NASA technical team should also ensure that \\nany enabling products necessary to provide test, operations, maintenance, and disposal support for the product are also ready or provided as defined in the contract.\\nDepending on the strategy and roles/responsibilities \\nof the vendor, a determination/analysis of the ven -\\ndor’s verification and validation compliance may need to be reviewed. This may be done informally or formally as appropriate for the complexity of the \\nproduct. For products that were verified and validated \\nby the vendor, after ensuring that all work products from this phase have been captured, the product may be ready to enter the Product Transition Process to be delivered to the next higher level or to its final end \\nuser. For products that the technical team will verify \\nand validate, the product will be ready for verification after ensuring that all work products for this phase have been captured.\\nMake/Code the Product\\nIf the strategy is to make or code the product, the technical team should first ensure that the enabling products are ready. This may include ensuring all \\npiece parts are available, drawings are complete and \\nadequate, software design is complete and reviewed, machines to cut the material are available, interface specifications are approved, operators are trained and available, manufacturing and/or coding procedures/processes are ready, software personnel are trained \\nand available to generate code, test fixtures are devel -\\noped and ready to hold products while being gener -\\nated, and software test cases are available and ready to \\nbegin model generation.\\nThe product is then made or coded in accordance \\nwith the specified requirements, configuration docu -\\nmentation, and applicable standards. Software devel -\\nopment must be consistent with NPR 7150.2, NASA Software Engineering Requirements. Throughout 825.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nthis process, the technical team should work with the \\nquality organization to review, inspect, and discuss progress and status within the team and with higher \\nlevels of management as appropriate. Progress should \\nbe documented within the technical schedules. Peer reviews, audits, unit testing, code inspections, simu -\\nlation checkout, and other techniques may be used to ensure the made or coded product is ready for the verification process. Some production and coding \\ncan also be separately contracted. This is sometimes \\npursued as a cost control feature providing motiva -\\ntion for the design contractor to keep the operations costs low and not roll costs into the operations phase of a long-term contract. This is also valuable when \\nthe design contractor is not well suited for long-term \\ncontinuing production operations. Small projects and activities often use small manufacturing shops to fabricate the system or major portions and small software companies to code their software. In these cases, the production and software engineers may \\nspecify some portion of the hardware production or \\nsoftware coding and request the remaining portions, including as-built documentation, from the man -\\nufacturing or software provider. The specified por -\\ntions are contained as part of the contract statement \\nof work in these cases. The level of process control \\nand information provided to or from the vendor is dependent on the criticality of the systems obtained. As production proceeds and components are pro -\\nduced, there is a need to establish a method (Material Review Boards (MRBs) are typically used for large \\nprojects) to review any nonconformance to specifica -\\ntions and disposition whether the components can be \\naccepted, reworked, or scrapped and remade.\\nReuse\\nIf the strategy is to reuse a product that already exists, extreme care should be taken to ensure that the product is truly applicable to this project and for \\nthe intended uses and the environment in which it \\nwill be used. This should have been a major factor used in the decision strategy to make/buy/reuse. If \\nthe new environment is more extreme, requalifica -\\ntion is needed for the component or system. Design \\nfactors of safety, margins, and other required design and construction standards should also be assessed. If the program/project requires higher factor of safety \\nor margins, the component may not be usable or a \\nwaiver may have to be approved.\\nThe documentation available (e.g., as-built documen -\\ntation, user’s guides, operations manuals, discrep -\\nancy reports, waivers and deviations) from the reuse \\nproduct should be reviewed by the technical team so \\nthat they can become completely familiar with the product and ensure it will meet the requirements in the intended environment. Any supporting manuals, drawings, or other documentation available should \\nalso be gathered.\\nThe availability of any supporting or enabling prod -\\nucts or infrastructure needed to complete the fabrica -\\ntion, coding, testing, analysis, verification, validation, \\nor shipping of the product needs to be determined. \\nSupporting products may be found in product man -\\nufacturing plans, processes, and procedures. If any of \\nthese products or services are lacking, they will need to be developed or arranged for before progressing to the next phase.\\nSpecial arrangements may need to be made or forms \\nsuch as nondisclosure agreements may need to be acquired before the reuse product can be received.\\nA reused product often needs to undergo the same \\nverification and validation as a purchased product \\nor a built product. Relying on prior verification and validation should only be considered if the product’s verification and validation documentation meets or exceeds the verification, validation, and documen -\\ntation requirements of the current project and the documentation demonstrates that the product was 835.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nverified and validated against equivalent require -\\nments (including environments) and expectations. \\nThe savings gained from reuse is not necessarily from \\nreduced acceptance-level testing of the flight prod -\\nucts, but possibly elimination of the need to fully \\nrequalify the item (if all elements are the same, includ -\\ning the environment and operation), elimination of the need to specify all of the internal requirements such as printed circuit board specifications or mate -\\nrial requirements, reduced internal data products, or the confidence that the item will pass acceptance test and will not require rework.\\n5.1.1.2.3  Capture Work Products\\nRegardless of what implementation form was selected, all work products from the make/buy/reuse process should be captured, including as-built design draw -\\nings, design documentation, design models, code list -\\nings, model descriptions, procedures used, operator manuals, maintenance manuals, or other documen -\\ntation as appropriate.\\n5.1.1.3  Outputs\\n• End Product for Verification:  Unless the vendor \\nperforms verification, the made/coded, purchased, or reused end product in a form appropriate for \\nthe life cycle phase is provided for the verification \\nprocess. The form of the end product is a function of the life cycle phase and the placement within the system structure (the form of the end product could be hardware, software, model, prototype, first article for test, or single operational article or \\nmultiple production articles).\\n• End Product Documents and Manuals: \\nAppropriate documentation is also delivered \\nwith the end product to the verification process and to the technical data management process. \\nDocumentation may include applicable as-built \\ndesign drawings; close out photos; operation, user, maintenance, or training manuals; applicable baseline documents (configuration information such as as-built specifications or stakeholder expectations); certificates of compliance; or other vendor documentation.\\n• Product Implementation Work Products: Any \\nadditional work products providing reports, records, lesson learned, assumptions, updated CM \\nproducts, and other outcomes of these activities.\\nThe process is complete when the following activities \\nhave been accomplished:\\n• End products are fabricated, purchased, or reuse modules are acquired.\\n• End products are reviewed, checked, and ready for verification.\\n• Procedures, decisions, assumptions, anomalies, corrective actions, lessons learned, etc., resulting from the make/buy/reuse are recorded.\\n5.1.2   Product Implementation \\nGuidance\\nRefer to Section 5.1.2 in the NASA Expanded \\nGuidance for Systems Engineering at https://nen.\\nnasa.gov/web/se/doc-repository  for additional guid-\\nance on:\\n• buying off-the-shelf products and\\n• the need to consider the heritage of products.\\n5.2 Product Integration\\nProduct integration is a key activity of the systems engineer. Product integration is the engineering of the subsystem interactions and their interactions \\nwith the system environments (both natural and \\ninduced). Also in this process, lower-level products are assembled into higher-level products and checked 845.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nto make sure that the integrated product functions \\nproperly and that there are no adverse emergent behaviors. This integration begins during concept \\ndefinition and continues throughout the system life \\ncycle. Integration involves several activities focused on the interactions of the subsystems and environ -\\nments. These include system analysis to define and understand the interactions, development testing including qualification testing, and integration with \\nexternal systems (e.g., launch operations centers, \\nspace vehicles, mission operations centers, flight con -\\ntrol centers, and aircraft) and objects (i.e., planetary bodies or structures). To accomplish this integration, the systems engineer is active in integrating the dif -\\nferent discipline and design teams to ensure system and environmental interactions are being properly balanced by the differing design teams. The result of a well-integrated and balanced system is an elegant design and operation.\\nIntegration begins with concept development, ensur -\\ning that the system concept has all necessary func -\\ntions and major elements and that the induced and \\nnatural environment domains in which the system is expected to operate are all identified. Integration \\ncontinues during requirements development, ensur -\\ning that all system and environmental requirements \\nare compatible and that the system has a proper bal -\\nance of functional utility to produce a robust and efficient system. Interfaces are defined in this phase and are the pathway of system interactions. Interfaces \\ninclude mechanical (i.e., structure, loads), fluids, \\nthermal, electrical, data, logical (i.e., algorithms and software), and human. These interfaces may include support for assembly, maintenance, and testing func -\\ntions in addition to the system main performance \\nfunctions. The interactions that occur through all of \\nthese interfaces can be subtle and complex, leading to both intended and unintended consequences. All of these interactions need to be engineered to produce an elegant and balanced system.Integration during the design phase continues the engineering of these interactions and requires con -\\nstant analysis and management of the subsystem \\nfunctions and the subsystem interactions between \\nthemselves and with their environments. Analysis of the system interactions and managing the balance of the system is the central function of the systems engineer during the design process. The system needs to create and maintain a balance between the sub -\\nsystems, optimizing the system performance over any one subsystem to achieve an elegant and efficient design. The design phase often involves development testing at the component, assembly, or system level. This is a key source of data on system interactions, \\nand the developmental test program should be struc -\\ntured to include subsystem interactions, human-in-\\nthe-loop evaluations, and environmental interaction test data as appropriate.\\nIntegration continues during the operations phase, \\nbringing together the system hardware, software, and \\nhuman operators to perform the mission. The inter -\\nactions between these three integrated natures of the system need to be managed throughout development and into operations for mission success. The systems \\nengineer, program manager, and the operations team \\n(including the flight crew from crewed missions) need to work together to perform this management. The systems engineer is not only cognizant of these oper -\\nations team interactions, but is also involved in the design responses and updates to changes in mission \\nparameters and unintended consequences (through \\nfault management).\\nFinally, integration or de-integration occurs during \\nsystem closeout (i.e., decommissioning and disposal). \\nThe system capabilities to support de-integration \\nand/or disposal need to be engineered into the sys -\\ntem from the concept definition phase. The closeout phase involves the safe disposal of flight assets con -\\nsistent with U.S. policy and law and international 855.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\ntreaties. This disposal can involve the safe reentry and \\nrecovery or impact in the ocean, impact on the moon, or solar trajectory. This can also involve the disassem -\\nbly or repurposing of terrestrial equipment used in manufacturing, assembly, launch, and flight opera -\\ntions. Dispositioning of recovered flight assets also occurs during this phase. Capture of system data and archiving for use in future analysis also occurs. In all of these activities, the systems engineer is involved \\nin ensuring a smooth and logical disassembly of the \\nsystem and associated program assets.\\nThe Product Integration Process applies not only \\nto hardware and software systems but also to ser -\\nvice-oriented solutions, requirements, specifications, plans, and concepts. The ultimate purpose of prod -\\nuct integration is to ensure that the system elements function as a whole.\\nProduct integration involves many activities that need \\nto be planned early in the program or project in order \\nto effectively and timely accomplish the integration. Some integration activities (such as system tests) can require many years of work and costs that need to be identified and approved through the budget cycles. \\nAn integration plan should be developed and doc -\\numented to capture this planning. Small projects \\nand activities may be able to include this as part of their SEMP. Some activities may have their integra -\\ntion plans captured under the integration plan of the sponsoring flight program or R&T program. Larger \\nprograms and projects need to have a separate inte -\\ngration plan to clearly lay out the complex analysis \\nand tests that need to occur. An example outline for a separate integration plan is provided in \\nAppendix H .\\nDuring project closeout, a separate closeout plan \\nshould be produced describing the decommission -\\ning and disposal of program assets. (For example, \\nsee National Space Transportation System (NSTS) 60576, Space Shuttle Program, Transition Management Plan). For smaller projects and activities, particularly \\nwith short life cycles (i.e., short mission durations), the closeout plans may be contained in the SEMP.\\n5.2.1  Process Description\\nFIGURE 5.2-1  provides a typical flow diagram for the \\nProduct Integration Process and identifies typical inputs, outputs, and activities to consider in address -\\ning product integration. The activities of the Product Integration Process are truncated to indicate the action and object of the action.\\n5.2.1.1  Inputs\\n• Lower-level products to be integrated: These are \\nthe products developed in the previous lower-level tier in the product hierarchy. These products will \\nbe integrated/assembled to generate the product \\nfor this product layer.\\n• End product design specifications and config -\\nuration documentation: These are the specifi -\\ncations, Interface Control Documents (ICDs), drawings, integration plan, procedures, or other \\ndocumentation or models needed to perform the \\nintegration including documentation for each of the lower-level products to be integrated.\\n• Product integration-enabling products:  These \\nwould include any enabling products, such as holding fixtures, necessary to successfully inte -\\ngrate the lower-level products to create the end product for this product layer.\\n5.2.1.2  Process Activities\\nThis subsection addresses the approach to the imple -\\nmentation of the Product Integration Process, includ -\\ning the activities required to support the process. The \\nbasic tasks that need to be established involve the \\nmanagement of internal and external interactions of the various levels of products and operator tasks to support product integration and are as follows:865.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nProduc t Documents\\nand Manuals To Product\\nVerification ProcessLower Level\\nProduc ts to Be\\nIntegrated  \\nEnd Product Design\\nSpecifications and\\nConfiguration\\nDocumentation  \\nProduct Integration–\\nEnabling ProductsTo Technical Data\\nManagement ProcessFrom Pro duct\\nTransition Process\\nFrom Configuration\\nManagement Process\\nFrom existing\\nresou rces  or Pro duct\\nTransition ProcessIntegrated Product\\nProduct Integration\\nWork Products  Confirm that received products\\nhave been  validated \\nPrepare the integration environment\\nfor assembly and integration\\nAssemble and integrate the\\nreceived products into the desired\\nend productObtain lower level products for\\nassembly and integration \\nCapture work products from \\nproduct integration activitiesPrepare to conduct product\\nintegration\\nPrepare appropriate product\\nsupport documentation \\nFIGURE 5.2-1  Product Integration Process\\n5.2.1.2.1  Prepare to Conduct Product Integration\\nPrepare to conduct product integration by (1) review -\\ning the product integration strategy/plan (see Section \\n6.1.2.4.4), generating detailed planning for the inte -\\ngration, and developing integration sequences and procedures; and (2) determining whether the prod -\\nuct configuration documentation is adequate to con -\\nduct the type of product integration applicable for the product life cycle phase, location of the product in the system structure, and management phase suc -\\ncess criteria.\\nAn integration strategy is developed and documented \\nin an integration plan. This plan, as well as support -\\ning documentation, identifies the optimal sequence of receipt, assembly, and activation of the various components that make up the system. This strategy should use technical, cost, and schedule factors to ensure an assembly, activation, and loading sequence that minimizes cost and assembly difficulties. The larger or more complex the system or the more deli -\\ncate the element, the more critical the proper sequence becomes, as small changes can cause large impacts on project results.\\nThe optimal sequence of assembly is built from the \\nbottom up as components become sub-elements, elements, and subsystems, each of which should be checked prior to fitting it into the next higher assem -\\nbly. The sequence will encompass any effort needed 875.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nto establish and equip the assembly facilities; e.g., \\nraised floor, hoists, jigs, test equipment, input/out -\\nput, and power connections. Once established, the \\nsequence should be periodically reviewed to ensure \\nthat variations in production and delivery schedules have not had an adverse impact on the sequence or compromised the factors on which earlier decisions were made.\\n5.2.1.2.2   Obtain Lower-Level Products for Assembly \\nand Integration\\nEach of the lower-level products that is needed for \\nassembly and integration is obtained from the tran -\\nsitioning lower-level product owners or a storage \\nfacility as appropriate. Received products should be \\ninspected to ensure no damages occurred during the transitioning process.\\n5.2.1.2.3   Confirm That Received Products Have Been \\nValidated\\nConfirm that the received products that are to be \\nassembled and integrated have been validated to demonstrate that the individual products satisfy the \\nagreed-to set of stakeholder expectations, including \\ninterface requirements. This validation can be con -\\nducted by the receiving organization or by the pro -\\nviding organization if fully documented or witnessed by the receiving representative.\\n5.2.1.2.4   Prepare the Integration Environment for \\nAssembly and Integration\\nPrepare the integration environment in which assem -\\nbly and integration will take place, including evaluat -\\ning the readiness of the product integration-enabling \\nproducts and the assigned workforce. These enabling \\nproducts may include facilities, equipment jigs, tool -\\ning, and assembly/production lines. The integration environment includes test equipment, simulators, models, storage areas, and recording devices.5.2.1.2.5   Assemble and Integrate the Received Products \\ninto the Desired End Product\\nAssemble and integrate the received products into \\nthe desired end product in accordance with the spec -\\nified requirements, configuration documentation, \\ninterface requirements, applicable standards, and \\nintegration sequencing and procedures. This activ -\\nity includes managing, evaluating, and controlling physical, functional, and data interfaces among the products being integrated.\\nFunctional testing of the assembled or integrated \\nunit is conducted to ensure that assembly is ready to enter verification testing and ready to be integrated into the next level. Typically, all or key representative functions are checked to ensure that the assembled \\nsystem is functioning as expected. Formal product \\nverification and validation will be performed in the next process.\\n5.2.1.2.6   Prepare Appropriate Product Support  \\nDocumentation\\nPrepare appropriate product support documentation, such as special procedures for performing product verification and product validation. Drawings or \\naccurate models of the assembled system are devel -\\noped and confirmed to be representative of the \\nassembled system.\\n5.2.1.2.7  Capture Product Integration Work Products\\nCapture work products and related information generated while performing the Product Integration Process activities. These work products include system \\nmodels, system analysis data and assessment reports, \\nderived requirements, the procedures that were used in the assembly, decisions made and supporting ratio -\\nnale, assumptions that were made, identified anoma -\\nlies and associated corrective actions, lessons learned in performing the assembly, and updated product \\nconfiguration and support documentation.885.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n5.2.1.3  Outputs\\nThe following are typical outputs from this process \\nand destinations for the products from this process:\\n• Integrated product(s)  with all system interactions \\nidentified and properly balanced.\\n• Documentation and manuals , including system \\nanalysis models, data, and reports supporting flight-readiness rationale and available for future \\nanalysis during the operation of the system in the \\nmission-execution phase.\\n• Work products , including reports, records, and \\nnon-deliverable outcomes of product integra -\\ntion activities (to support the Technical Data \\nManagement Process); integration strategy doc -\\nument; assembly/check area drawings; system/\\ncomponent documentation sequences and ratio -\\nnale for selected assemblies; interface manage -\\nment documentation; personnel requirements; special handling requirements; system documen -\\ntation; shipping schedules; test equipment and drivers’ requirements; emulator requirements; and identification of limitations for both hard -\\nware and software.\\n5.2.2  Product Integration Guidance\\nRefer to Section 5.2.2 in the NASA Expanded Guidance for Systems Engineering at \\nhttps://nen.nasa.\\ngov/web/se/doc-repository  for additional guidance on:\\n• product integration strategies,\\n• the relationship to product implementation,\\n• product integration support,\\n• product integration of the design solution,\\n• system analysis, and\\n• interface system integration.5.3 Product Verification\\nThe Product Verification Process is the first of the verification and validation processes conducted on an end product. As used in the context of the systems \\nengineering common technical processes, a product \\nis one provided by either the Product Implementation Process or the Product Integration Process in a form suitable for meeting applicable life cycle phase success criteria. Realization is the act of implementing, inte -\\ngrating, verifying, validating, and transitioning the \\nend product for use at the next level up of the system \\nstructure or to the customer. At this point, the end product can be referred to as a “realized product” or “realized end product.”\\nProduct verification proves that an end product \\n(whether built, coded, bought, or reused) for any element within the system structure conforms to its requirements or specifications. Such specifications and other design description documentation estab -\\nlish the configuration baseline of that product, which \\nmay have to be modified at a later time. Without a \\nverified baseline and appropriate configuration con -\\ntrols, such later modifications could be costly or cause major performance problems.\\nFrom a process perspective, product verification and \\nvalidation may be similar in nature, but the objec -\\ntives are fundamentally different. A customer is inter -\\nested in whether the end product provided will do what the customer intended within the environment of use. Examination of this condition is validation. \\nSimply put, the Product Verification Process answers \\nthe critical question, “Was the end product realized right?” The Product Validation Process addresses the equally critical question, “Was the right end prod -\\nuct realized?” When cost effective and warranted by \\nanalysis, the expense of validation testing alone can \\nbe mitigated by combining tests to perform verifica -\\ntion and validation simultaneously.895.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nThe outcome of the Product Verification Process is con -\\nfirmation that the end product, whether achieved by \\nimplementation or integration, conforms to its speci -\\nfied requirements, i.e., verification of the end product. This subsection discusses the process activities, inputs, outcomes, and potential product deficiencies.DIFFERENCES BETWEEN VERIFICATION AND VALIDATION TESTING\\nTesting is a detailed evaluation method of both verification and validation\\nVerification Testing:  Verification testing relates back to the approved requirements set (such as an SRD) \\nand can be performed at different stages in the product life cycle. Verification tests are the official “for \\nthe record” testing performed on a system or element to show that it meets its allocated requirements or specifications including physical and functional interfaces. Verification tests use instrumentation and measurements and are generally accomplished by engineers, technicians, or operator-maintainer test personnel in a controlled environment to facilitate failure analysis.\\nValidation Testing:  Validation relates back to the ConOps document. Validation testing is conducted \\nunder realistic conditions (or simulated conditions) on any end product to determine the effectiveness and \\nsuitability of the product for use in mission operations by typical users and to evaluate the results of such tests. It ensures that the system is operating as expected when placed in a realistic environment.\\n5.3.1  Process Description\\nFIGURE 5.3-1 , taken from NPR 7123.1, provides a typ -\\nical flow diagram for the Product Verification Process \\nand identifies typical inputs, outputs, and activities to consider in addressing product verification.\\n5.3.1.1  Inputs\\nKey inputs to the process are:\\n• The product to be verified: This product \\nwill have been transitioned from either the Product Implementation Process or the Product Integration Process. The product will likely have been through at least a functional test to ensure it was assembled correctly. Any supporting doc -\\num\\nentation should be supplied with the product.\\n• Verification plan: This plan will have been devel -\\noped under the Technical Planning Process and baselined before entering this verification.\\n• Specified requirements baseline:  These are the \\nrequirements that have been identified to be verified for this product. Acceptance criteria should have been identified for each requirement to be verified.\\n• Enabling products:  Any other products needed \\nto perform the Product Verification Process. This may include test fixtures and support equipment.\\nAdditional work products such as the ConOps, mis-sion needs and goals, interface control drawings, test -\\ning standards and policies, and Agency standards and policies may also be needed to put verification activ -\\nities into context.905.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nDIFFERENCES BETWEEN VERIFICATION, QUALIFICATION, \\nACCEPTANCE AND CERTIFICATION\\nVerification: Verification is a formal process, using the method of test, analysis, inspection or \\ndemonstration, to confirm that a system and its associated hardware and software components satisfy all specified requirements. The Verification program is performed once regardless of how many flight units may be generated (as long as the design doesn’t change).\\nQualification: Qualification activities are performed to ensure that the flight unit design will meet functional \\nand performance requirements in anticipated environmental conditions. A subset of the verification program is performed at the extremes of the environmental envelope and will ensure the design will operate properly with the expected margins. Qualification is performed once regardless of how many flight units may be generated (as long as the design doesn’t change).\\nAcceptance : smaller subset of the verification program is selected as criteria for the acceptance program. \\nThe selected Acceptance activities are performed on each of the flight units as they are manufactured and \\nreadied for flight/use. An Acceptance Data Package is prepared for each of the flight units and shipped with the unit. The acceptance test/analysis criteria are selected to show that the manufacturing/workmanship of the unit conforms to the design that was previously verified/qualified. Acceptance testing is performed for each flight unit produced.\\nCertification: Certification is the audit process by which the body of evidence that results from the \\nverification activities and other activities are provided to the appropriate certifying authority to indicate the design is certified for flight/use. The Certification activity is performed once regardless of how many flight units may be generated.\\n5.3.1.2  Process Activities\\nThere are five major activities in the Product \\nVerification Process: (1) prepare to conduct prod -\\nuct verification; (2) perform verification; (3) analyze verification results; (4) preparing a product verifica -\\ntion report; and (5) capture work products generated during the verification activities.\\nProduct Verification is often performed by the devel -\\noper that produced the end product with participa -\\ntion of the end user and customer. Quality Assurance (QA) personnel are also critical in the verification \\npla\\nnning and execution activities.\\nA verification approach should be adapted (tailored) to the project it supports. The project manager and systems engineer should work with the verification lead engineer to develop a verification approach and plan the activities. Many factors need to be consid -\\nered in developing this approach and the subsequent verification program. These factors include:915.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n• Project type, especially for flight projects. \\nVerification activities and timing depend on the following:\\n »The type of flight article involved (e.g., an experiment, payload, or launch vehicle).\\n »For missions required to follow NPR 7120.5, NASA Space Flight Program and Project Management Requirements, NASA \\npayload classification (NPR 8705.4, Risk \\nClassification for NASA Payloads) guidelines are intended to serve as a starting point for establishing the formality of verification approaches that can be adapted to the needs of a specific project based on the “A–D” \\npayload classification. Further flexibility is \\nimparted to projects following NPR 7120.8, NASA Research and Technology Program and Project Management Requirements.Product Verification\\nResults \\nProduct Verification\\nReport  From Pro duct\\nImplementation or\\nProduct Integra tion  Process\\nEnd P roduct\\nto Be Verified\\nProduct Verification\\nPlan Specified Requirements\\nBaseline  \\nProduct Verification\\nEnabling ProductsTo Technical\\nAssessment ProcessVerified End Product To Product\\nValidation Process\\nTo Technical Data\\nManagement ProcessFrom Configura ation\\nManagement Process\\nFrom existing\\nresources or Pro duct\\nTransition ProcessProduct Verification\\nWork P roducts  Perform the product\\nverification\\nAnalyze the outcomes of\\nthe product verification\\nCapture work products from\\nproduct verification activitiesPrepare a product\\nverification reportPrepare to conduct product\\nverification\\nFIGURE 5.3-1  P roduct Verification Process\\n »Project cost and schedule implications. Verification activities can be significant driv -\\ners of a project’s cost and schedule, and these \\nimplications should be considered early in the \\ndevelopment of the verification plan. Trade studies should be performed early in the life cycle to support decisions about verification methods and types and the selection of facil -\\nity capabilities and locations. For example, a \\ntrade study might be made to decide between \\nperforming a test at a centralized facility or at several decentralized locations.\\n »Risk management should be considered in the development of the verification approach. Qualitative risk assessments and quantita -\\ntive risk analyses (e.g., a Failure Mode and Effects Analysis (FMEA)) often identify new concerns that can be mitigated by additional verifications, thus increasing the extent of 925.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nverification activities. Other risk assessments \\ncontribute to trade studies that determine \\nthe preferred methods of verification to be \\nused and when those methods should be \\nperformed. For example, a trade might be \\nmade between performing a model test versus \\ndetermining model characteristics by a less \\ncostly but less revealing analysis. The project \\nmanager/systems engineer should determine \\nwhat risks are acceptable in terms of the proj -\\nect’s cost and schedule.\\n• Availability of verification facilities/sites and \\ntransportation assets to move an article from one \\nlocation to another (when needed). This requires \\ncoordination with the Integrated Logistics \\nSupport (ILS) engineer.\\n• Availability of appropriately trained users for \\ninteraction with systems having human interfaces.\\n• Acquisition strategy; i.e., in-house development or \\nsystem contract. A NASA field Center can often \\nshape a contractor’s verification process through \\nthe project’s SOW.\\n• Degree of design heritage and hardware/software reuse.\\n5.3.1.2.1 Product Verification Preparation\\nIn preparation for verification, the verification \\nplan and the specified requirements are collected, \\nreviewed, and confirmed. The product to be verified \\nis obtained (output from the Product Implementation \\nProcess or the Product Integration Process) along \\nwith any enabling products, such as those repre -\\nsenting external interfacing products and support \\nresources (including personnel) that are necessary for \\nverification. Procedures capturing detailed step-by-\\nstep activities and based on the verification type and \\nmethods are finalized and approved. Development of \\nprocedures typically begins during the design phase of the project life cycle and matures as the design is \\nmatured. The verification environment is considered as \\npart of procedure development. Operational scenarios \\nare assessed to explore all possible verification activities \\nto be performed. The final element is preparation of the \\nverification environment; e.g., facilities, equipment, \\ntools, measuring devices, and climatic conditions.\\nWhen operator or other user interaction is involved, \\nit is important to ensure that humans are properly \\nrepresented in the verification activities. This includes \\nphysical size, skills, knowledge, training, clothing, \\nspecial gear, and tools. Note: Testing that includes \\nrepresentatives of the human in the system is often \\nreferred to as “human-in-the-loop” testing.\\nNOTE: Depending on the nature of the verifica -\\ntion effort and the life cycle phase the program is in, \\nsome type of review to assess readiness for verifi -\\ncation (as well as validation later) is typically held. In \\nearlier phases of the life cycle, these Test Readiness \\nReviews (TRRs) may be held informally; in later \\nphases of the life cycle, this review may become a \\nmore formal event. TRRs and other technical reviews \\nare an activity of the Technical Assessment Process.\\nOn most projects, a number of TRRs with tailored \\nentrance/success criteria are held to assess the \\nreadiness and availability of test ranges, test facili -\\nties, trained testers, instrumentation, integration labs, \\nsupport equipment, and other enabling products.\\nPeer reviews are additional reviews that may be \\nconducted formally or informally to ensure readiness \\nfor verification (as well as the results of the verifi -\\ncation process). Guidelines for conducting a peer \\nreview are discussed in Section 6.7.2.4.5.\\nTABLE 5.3-1 provides an example of the type of infor -\\nmation that may be included in a verification proce -\\ndure and a verification report.935.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nMETHODS OF VERIFICATION\\nAnalysis:  The use of mathematical modeling and analytical techniques to predict the suitability of a \\ndesign to stakeholder expectations based on calculated data or data derived from lower system structure \\nend product verifications. Analysis is generally used when a prototype; engineering model; or fabricated, assembled, and integrated product is not available. Analysis includes the use of modeling and simulation as analytical tools. A model is a mathematical representation of reality. A simulation is the manipulation of a model. Analysis can include verification by similarity of a heritage product.\\nDemonstration:  Showing that the use of an end product achieves the individual specified requirement. \\nIt is generally a basic confirmation of performance capability, differentiated from testing by the lack of \\ndetailed data gathering. Demonstrations can involve the use of physical models or mock-ups; for example, a requirement that all controls shall be reachable by the pilot could be verified by having a pilot perform flight-related tasks in a cockpit mock-up or simulator. A demonstration could also be the actual operation of the end product by highly qualified personnel, such as test pilots, who perform a one-time event that demonstrates a capability to operate at extreme limits of system performance, an operation not normally expected from a representative operational pilot.\\nInspection:  The visual examination of a realized end product. Inspection is generally used to verify physical \\ndesign features or specific manufacturer identification. For example, if there is a requirement that the safety \\narming pin has a red flag with the words “Remove Before Flight” stenciled on the flag in black letters, a visual inspection of the arming pin flag can be used to determine if this requirement was met. Inspection can include inspection of drawings, documents, or other records.\\nTest: The use of an end product to obtain detailed data needed to verify performance or provide sufficient \\ninformation to verify performance through further analysis. Testing can be conducted on final end products, breadboards, brassboards, or prototypes. Testing produces data at discrete points for each specified requirement under controlled conditions and is the most resource-intensive verification technique. As the saying goes, “Test as you fly, and fly as you test.” (See Section 5.3.2.5 in the NASA Expanded Guidance for Systems Engineering at \\nhttps://nen.nasa.gov/web/se/doc-repository )\\nOutcomes of verification preparation include the \\nfollowing:\\n• The verification plan, approved procedures, and an appropriate baseline set of specified require -\\nments and supporting configuration documenta -\\ntion is available and on hand;• Articles/models to be verified and verification-en -\\nabling products are on hand, assembled, and integrated with the verification environment according to verification plans and schedules;\\n• The resources (funding, facilities, and people including appropriately skilled operators) needed 945.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nTABLE 5.3-1 Example information in Verification Procedures and Reports\\nVerification Procedure Verification Report\\nNomenclature and identification of the test article or \\nmaterial;Verification objectives and the degree to which they were \\nmet;\\nIdentification of test configuration and any differences \\nfrom flight operational configuration;Description of verification activity including deviations \\nfrom nominal results (discrepancies);\\nIdentification of objectives and criteria established for the \\nverification by the applicable requirements specification;Test configuration and differences from the flight \\noperational configuration;\\nCharacteristics and design criteria to be inspected, \\ndemonstrated, or tested, including values with tolerances \\nfor acceptance or rejection;Specific result of each activity and each procedure, \\nincluding the location or link to verification data/artifacts;\\nDescription, in sequence, of steps, operations, and \\nobservations to be taken;Specific result of each analysis including those associated \\nwith test-data analysis;\\nIdentification of computer software required; Test performance data tables, graphs, illustrations, and \\npictures;\\nIdentification of measuring, test, and recording equipment \\nto be used, specifying range, accuracy, and type;Summary of nonconformance/discrepancy reports, \\nincluding dispositions with approved corrective actions \\nand planned retest activity if available;\\nProvision for recording equipment calibration or software \\nversion data;Conclusions and recommendations relative to the \\nsuccess of verification activity;\\nCredentials showing that required computer test \\nprograms/support equipment and software have been \\nverified prior to use with flight operational hardware;Status of Government-Supplied Equipment (GSE) and \\nother enabling support equipment as affected by test;\\nAny special instructions for operating data recording \\nequipment or other automated test equipment as \\napplicable;Copy of the as-run procedure (may include redlines); and\\nLayouts, schematics, or diagrams showing identification, \\nlocation, and interconnection of test equipment, test \\narticles, and measuring points and any other associated \\ndesign or configuration work products;Authentication of test results and authorization of \\nacceptability.\\nIdentification of hazardous situations or operations;\\nPrecautions and safety instructions to ensure safety of \\npersonnel and prevent degradation of test articles and \\nmeasuring equipment;\\nEnvironmental and/or other conditions to be maintained \\nwith tolerances;\\nConstraints on inspection or testing;\\nProvision or instructions for the recording of verification \\nresults and other artifacts;\\nSpecial instructions for instances of nonconformance and \\nanomalous occurrences or results; and\\nSpecifications for facility, equipment maintenance, \\nhousekeeping, quality inspection, and safety and handling \\nrequirements before, during, and after the total verification \\nactivity.955.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nto conduct the verification are available according \\nto the verification plans and schedules; and\\n• The verification environment is evaluated for ade -\\nquacy, completeness, readiness, and integration.\\n5.3.1.2.2 Perform Product Verification\\nThe actual act of verifying the end product is per -\\nformed as spelled out in the plans and procedures, \\nand conformance is established with each specified \\nproduct requirement. The verification lead should \\nensure that the procedures were followed and per -\\nformed as planned, the verification-enabling prod -\\nucts and instrumentation were calibrated correctly, \\nand the data were collected and recorded for required \\nverification measures.\\nA verification program may include verifications at \\nseveral layers in the product hierarchy. Some verifica -\\ntions need to be performed at the lowest component \\nlevel if the ability to verify a requirement at a higher \\nassembly is not possible. Likewise, there may be veri -\\nfications at assemblies, sub-systems and system levels. \\nIf practicable, a final set of testing with as much of \\nthe end-to-end configuration as possible is important.\\nThe purpose of end-to-end testing is to demonstrate \\ninterface compatibility and desired total function -\\nality among different elements of a mission system, \\nbetween systems (the system of interest and external \\nenabling systems), and within a system as a whole. \\nIt can involve real or representative input and oper -\\national scenarios. End-to-end tests performed on \\nthe integrated ground and flight assets include all \\nelements of the flight article (payload or vehicle), \\nits control, stimulation, communications, and data \\nprocessing to demonstrate that the entire integrated \\nmission system is operating in a manner to fulfill all \\nmission requirements and objectives. End-to-end tests \\nmay be performed as part of investigative engineering \\ntests, verification testing, or validation testing. These are some of the most important tests for the systems \\nengineers to participate in or to lead. They review \\nthe overall compatibility of the various systems and \\ndemonstrate compliance with system-level require -\\nments and whether the system behaves as expected \\nby the stakeholders.\\nEnd-to-end testing includes executing complete \\nthreads or operational scenarios across multi -\\nple configuration items, ensuring that all mission \\nrequirements are verified and validated. Operational \\nscenarios are used extensively to ensure that the \\nmission system (or collections of systems) will suc -\\ncessfully execute mission requirements. Operational \\nscenarios are a step-by-step description of how the \\nsystem should operate and interact with its users and \\nits external interfaces (e.g., other systems). Scenarios \\nshould be described in a manner that allows engi -\\nneers to walk through them and gain an understand -\\ning of how all the various parts of the system should \\nfunction and interact as well as verify that the system \\nwill satisfy the user’s goals and expectations (MOEs). \\nOperational scenarios should be described for all \\noperational modes, mission phases (e.g., installation, \\nstartup, typical examples of normal and contingency \\noperations, shutdown, and maintenance), and critical \\nsequences of activities for all classes of users identi -\\nfied. Each scenario should include events, actions, \\nstimuli, information, and interactions as appropri -\\nate to provide a comprehensive understanding of the \\noperational aspects of the system.\\nFIGURE 5.3-2  presents an example of an end-to-end \\ndata flow for a scientific satellite mission. Each arrow \\nin the diagram represents one or more data or control \\nflows between two hardware, software, subsystem, or \\nsystem configuration items. End-to-end testing veri -\\nfies that the data flows throughout the multisystem \\nenvironment are correct, that the system provides the \\nrequired functionality, and that the outputs at the \\neventual end points correspond to expected results. 965.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nEXTERNAL\\nSTIMULI\\nInstrument\\nSet B X-rays\\nInfraredVisible\\nUltravi olet\\nMicrowaveInstrument\\nSet A\\nSpacecraft\\nExecution\\nCommunityMission\\nPlanning\\nData\\nCaptureSoftware\\nLoadsCommand\\nGenerationPlanning\\nArchivalTransmission\\nAnalysisUplink Process\\nDownlink ProcessEXTERNAL\\nSYSTEMSGROUND SYSTEM FLIGHT SYSTEM\\nFIGURE 5.3-2  Exa mple of End-to-End Data Flow for a Scientific Satellite Mission\\nSince the test environment is as close an approxi -\\nmation as possible to the operational environment, \\nsystem performance requirements testing is also included. This figure is not intended to show the full extent of end-to-end testing. Each system shown would need to be broken down into a further level of granularity for completeness.\\nEnd-to-end testing is an integral part of the verifi -\\ncation and validation of the total (mission) system. \\nIt is a set of activities that can be employed during selected hardware, software, and system phases throughout the life cycle using developmental forms and external simulators. However, final end-to-end testing should be done on the flight articles in the flight configuration if possible and prior to deploy -\\nment and launch. In comparison with configura -\\ntion item testing, end-to-end testing addresses each configuration item (end product) only down to the level designated by the verification plan (generally, the segment r element) and focuses on external inter -\\nfaces, which can be either hardware, software, or human-based. Internal interfaces (e.g., software sub -\\nro\\nutine calls, analog-to-digital conversion) of a desig -\\nnated configuration item are not within the scope of end-to-end testing.\\nWhen a “discrepancy” is observed (i.e., any variance, \\nlack of agreement, or contradiction with the required or expected outcome, configuration, or result), verifi -\\ncation activities should stop and a discrepancy report should be generated. The activities and events leading up to the discrepancy should be analyzed to deter -\\nmine if a nonconforming product exists or there is an issue with the verification procedure or conduct. The Decision Analysis Process should be used to make decisions with respect to needed changes in the veri -\\nfication plans, environment, and/or procedures.\\nOutcomes of performing product verification include \\nthe following:\\n• A verified product is established with supporting confirmation that the product (in the appropriate 975.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nform for the life cycle phase) complies with its \\nspecified requirements, and if it does not, a non -\\nconformance report delineating the variance is \\navailable.\\n• A determination is made as to whether the appro -\\npriate results were collected and evaluated to show \\ncompletion of verification objectives throughout \\ntheir performance envelope.\\n• A determination is made that the verification \\nproduct was appropriately integrated with the \\nenabling products and verification environment.\\n5.3.1.2.3  Analyze Product Verification Results and \\nReport\\nAs the verification activities are completed, the \\nresults are collected and analyzed. The data are ana -\\nlyzed for quality, integrity, correctness, consistency, \\nand validity. Any verification discrepancies (anom -\\nalies, variations, and out-of-compliance conditions) \\nare identified and reviewed to determine if there is a \\nnonconforming product not resulting from poor ver -\\nification conduct, procedure, or conditions. If possi -\\nble, this analysis is performed while the test/analysis \\nconfiguration is still intact. This allows a quick turn -\\naround in case the data indicates that a correction to \\nthe test or analysis run needs to be performed again.\\nDiscrepancies and nonconforming products should \\nbe recorded and reported for follow-up action and \\nclosure. Verification results should be recorded in \\na requirements compliance or verification matrix \\nor other method developed during the Technical \\nRequirements Definition Process to trace compliance \\nfor each product requirement. Waivers needed as a \\nresult of verification to request relief from or modify \\na requirement are identified.NOTE:  Nonconformance and discrepancy \\nreports may be directly linked with the Technical \\nRisk Management Process. Depending on the \\nnature of the nonconformance, approval through \\nsuch bodies as a Material Review Board or \\nConfiguration Control Board (which typically \\nincludes risk management participation) may \\nbe\\xa0required.\\nSystem design and product realization process activ -\\nities may be required to resolve product nonconfor -\\nmance. If the mitigation of the nonconformance \\nresults in a change to the product, the verification \\nmay need to be planned and performed again.\\nOutcomes of analyzing the verification results include \\nthe following:\\n• Product nonconformance (not compliant with \\nproduct requirement) is identified.\\n• Appropriate replanning, redefinition of require -\\nments, redesign, implementation/integration, mod -\\nification, and reverification have been accomplished \\nfor resolution of the nonconforming product.\\n• Appropriate facility modifications, procedure \\ncorrections, enabling product modification, and \\nreverification have been performed for non-prod -\\nuct-related discrepancies.\\n• Waivers for nonconforming products are accepted.\\n• Discrepancy and nonconformance reports including \\ncorrective actions have been generated as needed.\\n• The verification report is completed.985.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nRe-engineering\\nBased on analysis of verification results, it could be \\nnecessary to re-realize the end product used for ver -\\nification or to re-engineer the end products assem -\\nbled and integrated into the product being verified, \\nbased on where and what type of nonconformance \\nwas found.\\nRe-engineering could require the reapplication of the \\nsystem design processes (Stakeholder Expectations \\nDefinition Process, Technical Requirements \\nDefinition Process, Logical Decomposition Process, \\nand Design Solution Definition Process).\\n5.3.1.2.4 Capture Product Verification Work Products\\nVerification work products (inputs to the Technical \\nData Management Process) take many forms and \\ninvolve many sources of information. The capture \\nand recording of verification results and related data \\nis a very important, but often underemphasized, step \\nin the Product Verification Process.\\nVerification results, peer review reports, anomalies, \\nand any corrective action(s) taken should be captured, \\nas should all relevant results from the application of \\nthe Product Verification Process (related decisions, \\nrationale for the decisions made, assumptions, and \\nlessons learned).\\nOutcomes of capturing verification work products \\ninclude the following:\\n• Verification of work products is recorded, e.g., \\nmethod of verification, procedures, environ -\\nments, outcomes, decisions, assumptions, correc -\\ntive actions, and lessons learned.\\n• Variations, anomalies, and out-of-compliance \\nconditions have been identified and documented, \\nincluding the actions taken to resolve them.• Proof that the realized end product did or did not \\nsatisfy the specified requirements is documented.\\n• The verification report is developed, including:\\n »recorded test/verification results/data;\\n »version of the set of specified requirements \\nused;\\n »version of the product verified;\\n »version or standard for tools, data, and equip -\\nment used;\\n »results of each verification including pass or \\nfail declarations; and\\n »discrepancies.\\n5.3.1.3 Outputs\\nKey outputs from the process are:\\n• Verified product ready for validation: After the \\nproduct is verified, it will next pass through the \\nProduct Validation Process.\\n• Product verification results: Results from executed \\nprocedures are passed to technical assessment.\\n• Product verification report(s): A report shows the \\nresults of the verification activities. It includes the \\nrequirement that was to be verified and its bidi -\\nrectional traceability, the verification method \\nused, and reference to any special equipment, \\nconditions, or procedures used. It also includes \\nthe results of the verification, any anomalies, \\nvariations or out-of-compliance results noted and \\nassociated corrective actions taken.\\n• Product verification work products:  These \\ninclude discrepancy and nonconformance reports \\nwith identified correction actions; updates \\nto requirements compliance documentation; \\nchanges needed to the procedures, equipment or 995.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nenvironment; configuration drawings; calibra -\\ntions; operator certifications; and other records.\\nCriteria for completing verification of the product \\ninclude: (1) documented objective evidence of com -\\npliance with requirements or waiver and (2) closure of all discrepancy and nonconformance reports.\\n5.3.2  Product Verification Guidance\\nRefer to Section 5.3.2 in the NASA Expanded Guidance for Systems Engineering at \\nhttps://nen.nasa.\\ngov/web/se/doc-repository  for additional guidance on:\\n• the verification approach,\\n• verification in the life cycle,\\n• verification procedures,\\n• verification reports\\n• end-to-end testing,\\n• use of modeling and simulations, and\\n• hardware-in-the-loop testing.\\n5.4 Product Validation\\nThe Product Validation Process is the second of the verification and validation processes conducted on an implemented or integrated end product. While veri -\\nfication proves whether “the product was done right,” validation proves whether “the right product was done.” In other words, verification provides objective evidence that every “shall” statement in the require -\\nments document or specification was met, whereas validation is performed for the benefit of the custom -\\ners and users to ensure that the system functions in the expected manner when placed in the intended environment. This is achieved by examining the products of the system at every level of the product structure and comparing them to the stakeholder \\nexpectations for that level. A well-structured valida -\\ntion process can save cost and schedule while meeting \\nthe stakeholder expectations.System validation confirms that the integrated real -\\nized end products conform to stakeholder expec -\\ntations as captured in the MOEs, MOPs, and ConOps. Validation also ensures that any anomalies \\ndiscovered are appropriately resolved prior to prod -\\nuct delivery. This section discusses the process activ -\\nities, methods of validation, inputs and outputs, and \\npotential deficiencies.\\nSee \\nSection 2.4  for a discussion about the distinctions \\nbetween Product Verification and Product Validation.\\n5.4.1  Process Description\\nFIGURE 5.4-1 , taken from NPR 7123.1, provides a typ -\\nical flow diagram for the Product Validation Process \\nand identifies typical inputs, outputs, and activities \\nto consider in addressing product validation.\\n5.4.1.1  Inputs\\nKey inputs to the process are:\\n• End product to be validated: This is the end prod -\\nuct that is to be validated and which has success -\\nfully passed through the verification process.\\n• Validation plan: This plan would have been devel -\\noped under the Technical Planning Process and \\nbaselined prior to entering this process. This plan \\nmay be a separate document or a section within \\nthe Verification and Validation Plan.\\n• Baselined stakeholder expectations: These would \\nhave been developed for the product at this level during the Stakeholder Expectations Definition \\nProcess. It includes the needs, goals, and objec-\\ntives as well as the baselined and updated concept of operations and MOEs.\\n• Any enabling products: These are any special \\nequipment, facilities, test fixtures, applications, 1005.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nor other items needed to perform the Product \\nValidation Process.\\n5.4.1.2  Process Activities\\nThe Product Validation Process demonstrates that the end product satisfies its stakeholder (customer and other interested party) expectations (MOEs) within the intended operational environments, with validation performed by anticipated operators and/or users whenever possible. The method of validation is a function of the life cycle phase and the position of the end product within the system structure.\\nThere are five major steps in the validation process: \\n(1) preparing to conduct validation, (2) conduct planned validation (perform validation), (3) analyze validation results, (4) prepare a validation report, and (5) capture the validation work products.\\nFrom Pro duct\\nVerification Process To Product\\nTransition Process\\nProduct Validation\\nResults \\nProduct Validation\\nReport  End Product\\nto Be Validated \\nProduct Validation\\nPlan Stakeholder Expectation\\nBaseline  \\nProduct Validation–\\nEnabling ProductsTo Technical\\nAssessment Process\\nFrom Design Solution \\n and  Technical\\nPlanning ProcessesValidated End Product \\nTo Technical Data\\nManagement ProcessFrom Configuration\\nManagement Process\\nFrom existing\\nresou rces or Pro duct\\nTransition ProcessProduct V alidation\\nWork Products  Perform the product\\nvalidation\\nAnalyze the outcomes of\\nthe product validation\\nCaptur e the work  products\\nfrom product validation activities Prepare a product\\nvalidation report Prepare to conduct product\\nvalidation\\nFIGURE 5.4-1  Pr oduct Validation ProcessThe objectives of the Product Validation Process are:\\n• To confirm that the end product fulfills its intended use when operated in its intended environment:\\n »Validation is performed for each implemented or integrated and verified end product from the lowest end product in a system structure branch up to the top level end product (the system).\\n »Evidence is generated as necessary to con -\\nfirm that products at each layer of the sys -\\ntem structure meet the capability and other operational expectations of the customer/user/operator and other interested parties for that product.1015.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n• To ensure the human has been properly integrated \\ninto the system:\\n »The user interface meets human engineering criteria.\\n »Operators and maintainers have the required skills and abilities.\\n »Instructions are provided and training pro -\\ngrams are in place.\\n »The working environment supports crew health and safety.• To ensure that any problems discovered are appropriately resolved prior to delivery of the end product (if validation is done by the supplier of the product) or prior to integration with other products into a higher level assembled product (if validation is done by the receiver of the product).\\nMETHODS OF VALIDATION\\nAnalysis: The use of mathematical modeling and analytical techniques to predict the suitability of a \\ndesign to stakeholder expectations based on calculated data or data derived from lower system structure end product verifications. Analysis is generally used when a prototype; engineering model; or fabricated, assembled, and integrated product is not available. Analysis includes the use of modeling and simulation as analytical tools. A model is a mathematical representation of reality. A simulation is the manipulation of a\\xa0model.\\nDemonstration: Showing that the use of an end product achieves the stakeholder expectations as defined \\nin the NGOs and the ConOps. It is generally a basic confirmation of behavioral capability, differentiated from testing by the lack of detailed data gathering. Demonstrations can involve the use of physical models or mock-ups; for example, an expectation that controls are readable by the pilot in low light conditions could be validated by having a pilot perform flight-related tasks in a cockpit mock-up or simulator under those\\xa0conditions.\\nInspection: The visual examination of a realized end product. Inspection is generally used to validate the \\npresence of a physical design features or specific manufacturer identification. For example, if there is an expectation that the safety arming pin has a red flag with the words “Remove Before Flight” stenciled on the flag in black letters, a visual inspection of the arming pin flag can be used to determine if this expectation has been met.\\nTest: The use of an end product to obtain detailed data needed to determine a behavior, or provide \\nsufficient information to determine a behavior through further analysis. Testing can be conducted on final end products, breadboards, brassboards, or prototypes. Testing produces information at discrete points for each specified expectation under controlled conditions and is the most resource-intensive validation\\xa0technique.5.4.1.2.1  Product Validation Preparation\\nTo prepare for performing product validation, the \\nappropriate set of expectations, including MOEs and MOPs, against which the validation is to be made 1025.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nshould be obtained. In addition to the V&V Plan, \\nother documentation such as the ConOps and HSI \\nPlan may be useful. The product to be validated (out -\\nput from implementation, or integration and verifi -\\ncation), as well as the appropriate enabling products \\nand support resources (requirements identified and \\nacquisition initiated by design solution activities) \\nwith which validation will be conducted should be \\ncollected. Enabling products includes those repre -\\nsenting external interfacing products and special \\ntest equipment. Support resources include person -\\nnel necessary to support validation and operators. \\nProcedures, capturing detailed step-by-step activities \\nand based on the validation type and methods are \\nfinalized and approved. Development of procedures \\ntypically begins during the design phase of the proj -\\nect life cycle and matures as the design is matured. \\nThe validation environment is considered as part of \\nprocedure development. Operational scenarios are \\nassessed to explore all possible validation activities to \\nbe performed. The final element is preparation of the \\nvalidation environment; e.g., facilities, equipment, \\nsoftware, and climatic conditions.\\nWhen operator or other user interaction is involved, \\nit is important to ensure that humans are properly \\nrepresented in the validation activities. This includes \\nphysical size, skills, knowledge, training, clothing, \\nspecial gear, and tools. When possible, actual end \\nusers/operators should be used and other stakeholders \\nshould participate or observe activities as appropriate \\nand practical.\\nOutcomes of validation preparation include the \\nfollowing:\\n• The validation plan, approved procedures, sup -\\nporting configuration documentation, and an \\nappropriate baseline set of stakeholder expecta -\\ntions are available and on hand;• Enabling products are integrated within the valida -\\ntion environment according to plans and schedules;\\n• Users/operators and other resources are available \\naccording to validation plans and schedules; and\\n• The validation environment is evaluated for ade -\\nquacy, completeness, readiness, and integration.\\n5.4.1.2.2 Perform Product Validation\\nThe act of validating the end product is performed \\nas spelled out in the validation plans and procedures, \\nand the conformance established to each specified \\nstakeholder expectation (MOEs and ConOps) shows \\nthat the validation objectives were met. Validation \\ndiffers from qualification testing. Validation testing is \\nfocused on the expected environments and operations \\nof the system where as qualification testing includes \\nthe worst case loads and environmental requirements \\nwithin which the system is expected to perform or \\nsurvive. The verification lead should ensure that the \\nprocedures were followed and performed as planned, \\nthe validation-enabling products and instrumenta -\\ntion were calibrated correctly, and the data were col -\\nlected and recorded for required validation measures.\\nWhen a discrepancy is observed, the validation should \\nbe stopped and a discrepancy report generated. The \\nactivities and events leading up to the discrepancy \\nshould be analyzed to determine if a nonconforming \\nproduct exists or there is an issue with the verifica -\\ntion procedure, conduct, or conditions. If there are \\nno product issues, the validation is replanned as nec -\\nessary, the environment preparation anomalies are \\ncorrected, and the validation is conducted again with \\nimproved or correct procedures and resources. The \\nDecision Analysis Process should be used to make \\ndecisions with respect to needed changes to the vali -\\ndation plans, environment, and/or conduct.1035.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nOutcomes of performing validation include the \\nfollowing:\\n• A validated product is established with support -\\ning confirmation that the appropriate results were \\ncollected and evaluated to show completion of \\nvalidation objectives.\\n• A determination is made as to whether the fabri -\\ncated/manufactured or assembled and integrated \\nproducts (including software or firmware builds \\nand human element allocations) comply with \\ntheir respective stakeholder expectations.\\n• A determination is made that the validated prod -\\nuct was appropriately integrated with the valida -\\ntion environment and the selected stakeholder \\nexpectations set was properly validated.\\n• A determination is made that the product being \\nvalidated functions together with interfacing \\nproducts throughout their operational envelopes.\\n5.4.1.2.3 Analyze Product Validation Results\\nOnce the validation activities have been completed, \\nthe results are collected and the data are analyzed \\nto confirm that the end product provided will sup -\\nply the customer’s needed capabilities within the \\nintended environments of use, validation procedures \\nwere followed, and enabling products and supporting \\nresources functioned correctly. The data are also ana -\\nlyzed for quality, integrity, correctness, consistency, \\nand validity, and any unsuitable products or product \\nattributes are identified and reported.\\nIt is important to compare the actual validation \\nresults to the expected results. If discrepancies are \\nfound, it needs to be determined if they are a result \\nof the test configuration or analysis assumptions or \\nwhether they are a true characteristic or behavior of the end product. If it is found to be a result of the test \\nconfiguration, the configuration should be corrected \\nand the validation repeated. If it is found to be a result \\nof the end product being validated, discussions with \\nthe customer should be held and any required sys -\\ntem design and product realization process activities \\nshould be conducted to resolve deficiencies. The defi -\\nciencies along with recommended corrective actions \\nand resolution results should be recorded, and valida -\\ntion should be repeated, as required.\\nOutcomes of analyzing validation results include the \\nfollowing:\\n• Product anomalies, variations, deficiencies, non -\\nconformance and/or issues are identified.\\n• Assurances that appropriate replanning, redefi -\\nnition of requirements, design, and revalidation \\nhave been accomplished for resolution of anoma -\\nlies, variations, deficiencies or out-of-compliance \\nconditions (for problems not caused by poor vali -\\ndation conduct).\\n• Discrepancy and corrective action reports are gen -\\nerated as needed.\\n• The validation report is completed.\\nRe-engineering\\nBased on the results of the Product Validation Process, \\nit could become necessary to re-engineer a deficient \\nend product. Care should be taken that correcting a \\ndeficiency or set of deficiencies does not generate a \\nnew issue with a part or performance that had previ -\\nously operated satisfactorily. Regression testing, a for -\\nmal process of rerunning previously used acceptance \\ntests (primarily used for software), is one method to \\nensure a change does not affect function or perfor -\\nmance that was previously accepted.1045.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nValidation Deficiencies\\nValidation outcomes can be unsatisfactory for several \\nreasons. One reason is poor conduct of the validation \\n(e.g., enabling products and supporting resources \\nmissing or not functioning correctly, untrained \\noperators, procedures not followed, equipment not \\ncalibrated, or improper validation environmental \\nconditions) and failure to control other variables not \\ninvolved in validating a set of stakeholder expecta -\\ntions. A second reason could be a shortfall in the ver -\\nification process of the end product. This could create \\nthe need for:\\n• Re-engineering end products lower in the system \\nstructure that make up the end product that was \\nfound to be deficient (i.e., that failed to satisfy val -\\nidation requirements); and/or\\n• Re-performing any needed verification and vali -\\ndation processes.\\nOther reasons for validation deficiencies (particularly \\nwhen M&S are involved) may be incorrect and/or \\ninappropriate initial or boundary conditions; poor \\nformulation of the modeled equations or behaviors; \\nthe impact of approximations within the modeled \\nequations or behaviors; failure to provide the required \\ngeometric and physics fidelities needed for credible \\nsimulations for the intended purpose; and/or poor \\nspatial, temporal, and perhaps, statistical resolution \\nof physical phenomena used in M&S.\\nNOTE: Care should be exercised to ensure that \\nthe corrective actions identified to remove valida -\\ntion deficiencies do not conflict with the baselined \\nstakeholder expectations without first coordinating \\nsuch changes with the appropriate stakeholders.Of course, the ultimate reason for performing vali -\\ndation is to determine if the design itself is the right \\ndesign for meeting stakeholder expectations. After \\nany and all validation test deficiencies are ruled \\nout, the true value of validation is to identify design \\nchanges needed to ensure the program/product’s \\nmission. Validation should be performed as early and \\nas iteratively as possible in the SE process since the \\nearlier re-engineering needs are discovered, the less \\nexpensive they are to resolve.\\nPass Verification but Fail Validation?\\nSometimes systems successfully complete verification \\nbut then are unsuccessful in some critical phase of \\nthe validation process, delaying development and \\ncausing extensive rework and possible compromises \\nwith the stakeholder. Developing a solid ConOps in \\nearly phases of the project (and refining it through the \\nrequirements development and design phases) is crit -\\nical to preventing unsuccessful validation. Similarly, \\ndeveloping clear expectations for user community \\ninvolvement in the HSI Plan is critical to success -\\nful validation. Frequent and iterative communica -\\ntions with stakeholders helps to identify operational \\nscenarios and key needs that should be understood \\nwhen designing and implementing the end product. \\nShould the product fail validation, redesign may be a \\nnecessary reality. Review of the understood require -\\nments set, the existing design, operational scenarios, \\nuser population numbers and skills, training, and \\nsupport material may be necessary, as well as nego -\\ntiations and compromises with the customer, other \\nstakeholders, and/or end users to determine what, if \\nanything, can be done to correct or resolve the situ -\\nation. This can add time and cost to the overall proj -\\nect or, in some cases, cause the project to fail or be \\ncanceled. However, recall from FIGURE 2.5-1  that the \\nearlier design issues are discovered, the less costly the \\ncorrective action.1055.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n5.4.1.2.4  Prepare Report and Capture Product \\nValidation Work Products\\nValidation work products (inputs to the Technical \\nData Management Process) take many forms and \\ninvolve many sources of information. The capture \\nand recording of validation-related data is a very \\nimportant, but often underemphasized, step in the \\nProduct Validation Process.\\nValidation results, deficiencies identified, and correc -\\ntive actions taken should be captured, as should all \\nrelevant results from the application of the Product \\nValidation Process (related decisions, rationale for \\ndecisions made, assumptions, and lessons learned).\\nOutcomes of capturing validation work products \\ninclude the following:\\n• Work products and related information generated \\nwhile doing Product Validation Process activities \\nand tasks are recorded; i.e., method of validation \\nconducted, the form of the end product used for \\nvalidation, validation procedures used, validation \\nenvironments, outcomes, decisions, assumptions, \\ncorrective actions, lessons learned, etc. (often cap -\\ntured in a matrix or other tool—see Appendix E ).\\n• Deficiencies (e.g., variations and anomalies and \\nout-of-compliance conditions) are identified and \\ndocumented, including the actions taken to resolve.\\n• Proof is provided that the end product is in con -\\nformance with the stakeholder expectation set \\nused in the validation.\\n• Validation report including:\\n »Recorded validation results/data;\\n »Version of the set of stakeholder expectations \\nused;\\n »Version and form of the end product \\nvalidated; »Version or standard for tools and equipment \\nused, together with applicable calibration \\ndata;\\n »Outcome of each validation including pass or \\nfail declarations; and\\n »Discrepancy between expected and actual \\nresults.\\nNOTE: For systems where only a single deliv -\\nerable item is developed, the Product Validation \\nProcess normally completes acceptance testing \\nof the system. However, for systems with several \\nproduction units, it is important to understand \\nthat continuing verification and validation is not an \\nappropriate approach to use for the items following \\nthe first deliverable. Instead, acceptance testing \\nis the preferred means to ensure that subsequent \\ndeliverables meet stakeholder expectations.\\n5.4.1.3 Outputs\\nKey outputs of validation are:\\n• Validated end product:  This is the end product \\nthat has successfully passed validation and is \\nready to be transitioned to the next product layer \\nor to the customer.\\n• Product validation results:  These are the raw \\nresults of performing the validations.\\n• Product validation report: This report provides \\nthe evidence of product conformance with the \\nstakeholder expectations that were identified as \\nbeing validated for the product at this layer. It \\nincludes any nonconformance, anomalies, or \\nother corrective actions that were taken.\\n• Work products:  These include procedures, \\nrequired personnel training, certifications, 1065.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nconfiguration drawings, and other records gener -\\nated during the validation activities.\\nSuccess criteria for this process include: (1) objective \\nevidence of performance and the results of each sys -\\ntem-of-interest validation activity are documented, \\nand (2) the validation process should not be consid -\\nered or designated as complete until all issues and \\nactions are resolved.\\n5.4.2  Product Validation Guidance\\nRefer to Section 5.4.2 in the NASA Expanded \\nGuidance for Systems Engineering at https://nen.nasa.\\ngov/web/se/doc-repository  for additional guidance on:\\n• use of modeling and simulation,\\n• software validation, and\\n• taking credit for validation.\\n5.5 Product Transition\\nThe Product Transition Process is used to transition a \\nverified and validated end product that has been gen -\\nerated by product implementation or product inte -\\ngration to the customer at the next level in the system \\nstructure for integration into an end product or, for \\nthe top-level end product, transitioned to the intended \\nend user. The form of the product transitioned will be \\na function of the product life cycle phase success cri -\\nteria and the location within the system structure of \\nthe WBS model in which the end product exists. The \\nsystems engineer involvement in this process includes \\nensuring the product being transitioned has been \\nproperly tested and verified/validated prior to being \\nshipped to the next level stakeholder/customer.\\nProduct transition occurs during all phases of the \\nlife cycle. During the early phases, the technical \\nteam’s products are documents, models, studies, and \\nreports. As the project moves through the life cycle, these paper or soft products are transformed through \\nimplementation and integration processes into hard -\\nware and software solutions to meet the stakeholder \\nexpectations. They are repeated with different degrees \\nof rigor throughout the life cycle. The Product \\nTransition Process includes product transitions from \\none level of the system architecture upward. The \\nProduct Transition Process is the last of the product \\nrealization processes, and it is a bridge from one level \\nof the system to the next higher level.\\nThe Product Transition Process is the key to bridge \\nfrom one activity, subsystem, or element to the overall \\nengineered system. As the system development nears \\ncompletion, the Product Transition Process is again \\napplied for the end product, but with much more \\nrigor since now the transition objective is delivery of \\nthe system-level end product to the actual end user. \\nDepending on the kind or category of system devel -\\noped, this may involve a Center or the Agency and \\nimpact thousands of individuals storing, handling, \\nand transporting multiple end products; preparing \\nuser sites; training operators and maintenance per -\\nsonnel; and installing and sustaining, as applicable. \\nExamples are transitioning the external tank, solid \\nrocket boosters, and orbiter to Kennedy Space Center \\n(KSC) for integration and flight. Another example is \\nthe transition of a software subsystem for integration \\ninto a combined hardware/software system.\\n5.5.1  Process Description\\nFIGURE 5.5-1  provides a typical flow diagram for \\nthe Product Transition Process and identifies typical \\ninputs, outputs, and activities to consider in address -\\ning product transition.\\n5.5.1.1 Inputs\\nInputs to the Product Transition Process depend pri -\\nmarily on the transition requirements, the product \\nthat is being transitioned, the form of the product 1075.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\ntransition that is taking place, and the location to \\nwhich the product is transitioning. Typical inputs are shown in \\nFIGURE 5.5-1  and described below.To Product\\nImplementation,\\nIntegration , Verification,\\nValidation , and \\nTransition Processes    Product  Transition\\nWork ProductsEnd Product to Be\\nTransitioned \\nDocumentation to\\nAccomp any the\\nDeli vered End Product\\nProduct Transition–\\nEnabling ProductsTo Technical Data\\nManagement ProcessFrom Technical Data\\nManagement Process  Deli vered  End Pro duct\\nWith Applicable\\nDocumentation\\nRealized Enabling\\nProducts  Prepare the end product for transition \\nPrepare sites, as required, where the\\nend product will be stored, assembled,\\nintegrated, installed, used, and/or\\nmaintainedEvaluate the end product, personnel,\\nand enabling product readiness for\\nproduct transition \\nCapture work products from product\\ntransition activities  Prepare to conduct product\\ntransition\\nTransition the end product to t he \\ncustomer with required documentation\\nbased on the type of transition requiredTo end user or Pro duct\\nIntegration Process \\n(recursive loop)\\nFIGURE 5.5-1  Pr oduct Transition Process\\n• The end product or products to be transitioned \\n(from the Product Validation Process):  The prod -\\nuct to be transitioned can take several forms. It \\ncan be a subsystem component, system assembly, or top-level end product. It can be hardware, ana -\\nlytical models, or software. It can be newly built, purchased, or reused. A product can transition from a lower system product to a higher one by being integrated with other transitioned products. This process may be repeated until the final end product is achieved. Each succeeding transition requires unique input considerations when pre -\\nparing the validated product for transition to the next level.Early phase products can take the form of infor -\\nm\\nation or data generated from basic or applied \\nresearch using analytical or physical models and are often in paper or electronic form. In fact, the end product for many NASA research projects or science activities is a report, paper, model, or even an oral presentation. In a sense, the dissem -\\nination of information gathered through NASA research and development is an important form of product transition.\\n• Documentation including manuals, procedures, \\nand processes that accompany the end product (from the Technical Data Management\\n Process):  \\nThe documentation required for the Product \\nTransition Process depends on the specific end product; its current location within the system 1085.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nstructure; and the requirements identified in \\nvarious agreements, plans, or requirements doc -\\numents. Typically, a product has a unique iden -\\ntification (i.e., serial or version number) and may \\nhave a pedigree (documentation) that specifies its \\nheritage and current state. Pertinent information \\nmay be controlled using a configuration control \\nprocess or work order system as well as design \\ndrawings and test reports. Documentation often \\nincludes proof of verification and validation con -\\nformance. A COTS product would typically con -\\ntain a manufacturer’s specification or fact sheet. \\nDocumentation may include operations manuals, \\ninstallation instructions, and other information.\\nThe documentation level of detail is dependent \\nupon where the product is within the product hier -\\narchy and the life cycle. Early in the life cycle, this \\ndocumentation may be conceptual or preliminary \\nin nature. Later in the life cycle, the documentation \\nmay be detailed design documents, user manuals, \\ndrawings, or other work products. Documentation \\nthat is gathered during the input process for the \\ntransition phase may require editing, assembling, \\nor repackaging to ensure it is in the required condi -\\ntion for acceptance by the customer.\\nSpecial consideration should be given to safety, \\nincluding clearly identifiable tags and markings \\nthat identify the use of hazardous materials, special \\nhandling instructions, and storage requirements.\\n• Product transition-enabling products, includ -\\ning packaging materials; containers; handling \\nequipment; and storage, receiving, and ship -\\nping facilities (from existing resources or the \\nProduct Transition Process for enabling product \\nrealization): Product transition-enabling prod -\\nucts may be required to facilitate the implementa -\\ntion, integration, evaluation, transition, training, \\noperations, support, and/or retirement of the transition product at its next higher level or for \\nthe transition of the final end product. Some or all \\nof the enabling products may be defined in tran -\\nsition-related agreements, system requirements \\ndocuments, or project plans. In some cases, prod -\\nuct transition-enabling products are developed \\nduring the realization of the product itself or may \\nbe required to be developed during the transition \\nstage.\\nAs a product is developed, special containers, \\nholders, or other devices may also be developed to \\naid in the storing and transporting of the product \\nthrough development and realization. These may \\nbe temporary accommodations that do not sat -\\nisfy all the transition requirements, but allow the \\nproduct to be initiated into the transition process. \\nIn such cases, the temporary accommodations \\nwill have to be modified or new accommodations \\nwill need to be designed and built or procured to \\nmeet specific transportation, handling, storage, \\nand shipping requirements.\\nSensitive or hazardous products may require spe -\\ncial enabling products such as monitoring equip -\\nment, security features, inspection devices, safety \\ndevices, and personnel training to ensure ade -\\nquate safety and environmental requirements are \\nachieved and maintained.\\n5.5.1.2 Process Activities\\nTransitioning the product can take one of two forms:\\n• The delivery of lower system end products to \\nhigher ones for integration into another end prod -\\nuct; or\\n• The delivery of the final end product to the cus -\\ntomer or user that will use it in its operational \\nenvironment.1095.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nIn the first case, the end product is one of perhaps \\nseveral other pieces that will ultimately be integrated \\ntogether to form the item. In the second case, the \\nend product is for final delivery to the customer. For \\nexample, the end product might be one of several \\ncircuit cards that will be integrated together to form \\nthe final unit that is delivered. Or that unit might \\nalso be one of several units that have to be integrated \\ntogether to form the final product.\\nThe form of the product transitioned is not only a \\nfunction of the location of that product within the \\nsystem product hierarchy, but also a function of the \\nlife cycle phase. Early life cycle phase products may be \\nin the form of paper, electronic files, physical models, \\nor technology demonstration prototypes. Later phase \\nproducts may be preproduction prototypes (engineer -\\ning models), the final study report, or the flight units.\\nFIGURE 5.5-1  shows what kind of inputs, outputs, \\nand activities are performed during product transi -\\ntion regardless of where in the product hierarchy or \\nlife cycle the product is. These activities include pre -\\nparing to conduct the transition; making sure the end \\nproduct, all personnel, and any enabling products are \\nready for transitioning; preparing the site; and per -\\nforming the transition including capturing and doc -\\numenting all work products.\\nHow these activities are performed and what form the \\ndocumentation takes depends on where the end items \\nare in the product hierarchy and the life cycle phase.\\n5.5.1.2.1 Prepare to Conduct Transition\\nThe first task is to identify which of the two forms \\nof transition is needed: (1) the delivery of lower sys -\\ntem end products to higher ones for integration into \\nanother end product; or (2) the delivery of the final \\nend product to the customer or user that will use the \\nend product in its operational environment. The form \\nof the product being transitioned affects transition planning and the kind of packaging, handling, stor -\\nage, and transportation that is required. The cus -\\ntomer and other stakeholder expectations, as well \\nas the specific design solution, may indicate special \\ntransition procedures or enabling product needs for \\npackaging, storage, handling, shipping/transporting, \\nsite preparation, installation, and/or sustainability. \\nThese requirements need to be reviewed during the \\npreparation stage.\\nOther tasks in preparing to transition a product \\ninvolve making sure the end product, personnel, and \\nany enabling products are ready for that transition. \\nThis includes the availability of the documentation \\nor models that will be sent with the end product, \\nincluding proof of verification and validation confor -\\nmance. The appropriateness of detail for that docu -\\nmentation depends upon where the product is within \\nthe product hierarchy and the life cycle. Early in the \\nlife cycle, this documentation may be preliminary \\nin nature. Later in the life cycle, the documentation \\nmay be detailed design documents, user manuals, \\ndrawings, or other work products. Procedures neces -\\nsary for conducting the transition should be reviewed \\nand approved by this time.\\nFinally, the availability and skills of personnel needed \\nto conduct the transition as well as the availability \\nof any necessary packaging materials/containers, \\nhandling equipment, storage facilities, and shipping/\\ntransporter services should also be reviewed. Any \\nspecial training necessary for the personnel to per -\\nform their tasks needs to be performed by this time.\\n5.5.1.2.2 Prepare the Site to Receive the Product\\nFor either of the forms of product transition, the receiv -\\ning site needs to be prepared to receive the product. \\nHere the end product is stored, assembled, integrated, \\ninstalled, used, and/or maintained as appropriate for \\nthe life cycle phase, position of the end product in the \\nsystem structure, and customer agreement.1105.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nA vast number of key complex activities, many of \\nthem outside direct control of the technical team, \\nneed to be synchronized to ensure smooth transition \\nto the end user. If transition activities are not care -\\nfully controlled, there can be impacts on schedule, \\ncost, and safety of the end product.\\nA site survey may need to be performed to deter -\\nmine the issues and needs. This should address the \\nadequacy of existing facilities to accept, store, and \\noperate the new end product and identify any logis -\\ntical-support-enabling products and services required \\nbut not planned for. Additionally, any modifications \\nto existing facilities should be planned well in advance \\nof fielding; therefore, the site survey should be made \\nduring an early phase in the product life cycle. These \\nmay include logistical enabling products and ser -\\nvices to provide support for end-product use, opera -\\ntions, maintenance, and disposal. Training for users, \\noperators, maintainers, and other support personnel \\nmay need to be conducted. National Environmental \\nPolicy Act documentation or approvals may need to \\nbe obtained prior to the receipt of the end product.\\nPrior to shipment or after receipt, the end product \\nmay need to be stored in suitable storage conditions \\nto protect and secure the product and prevent dam -\\nage or the deterioration of it. These conditions should \\nhave been identified early in the design life cycle.\\n5.5.1.2.3 Prepare the Product for Transition\\nWhether transitioning a product to the next room for \\nintegration into the next higher assembly, or for final \\ntransportation across the country to the customer, \\ncare should be taken to ensure the safe transporta -\\ntion of the product. The requirements for packag -\\ning, handling, storage, training, and transportation \\nshould have been identified during system design. \\nPreparing the packaging for protection, security, and \\nprevention of deterioration is critical for products \\nplaced in storage or when it is necessary to transport \\nor ship between and within organizational facilities or between organizations by land, air, and/or water \\nvehicles. Particular emphasis needs to be on protect -\\ning surfaces from physical damage, preventing cor -\\nrosion, eliminating damage to electronic wiring or \\ncabling, shock or stress damage, heat warping or cold \\nfractures, moisture, and other particulate intrusion \\nthat could damage moving parts.\\nThe design requirements should have already \\naddressed the ease of handling or transporting the \\nproduct such as component staking, addition of \\ntransportation hooks, crating, etc. The ease and \\nsafety of packing and unpacking the product should \\nalso have been addressed. Additional measures may \\nalso need to be implemented to show accountability \\nand to securely track the product during transporta -\\ntion. In cases where hazardous materials are involved, \\nspecial labeling or handling needs, including trans -\\nportation routes, need to be in place.\\n5.5.1.2.4 Transition the Product\\nThe end product is then transitioned (i.e., moved, \\ntransported, or shipped) with required documen -\\ntation to the customer based on the type of tran -\\nsition required, e.g., to the next higher level item \\nin the product hierarchy (often called the Product \\nBreakdown Structure (PBS)) for product integra -\\ntion or to the end user. Documentation may include \\noperations manuals, installation instructions, and \\nother information.\\nThe end product is finally installed into the next \\nhigher assembly or into the customer/user site using \\nthe preapproved installation procedures.\\nConfirm Ready to Support\\nAfter installation, whether into the next higher \\nassembly or into the final customer site, functional \\nand acceptance testing of the end product should be \\nconducted. This ensures no damage from the ship -\\nping/handling process has occurred and that the \\nproduct is ready for support. Any final transitional 1115.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nwork products should be captured as well as docu -\\nmentation of product acceptance.\\n5.5.1.2.5 Capture Product Transition Work Products\\nOther work products generated during the transition \\nprocess are captured and archived as appropriate. \\nThese may include site plans, special handling proce -\\ndures, training, certifications, videos, inspections, or \\nother products from these activities.\\n5.5.1.3 Outputs\\n• Delivered end product with applicable docu -\\nmentation:  This may take one of two forms:\\n1. Delivered end product for integration to next \\nlevel up in system structure: This includes the \\nappropriate documentation. The form of the \\nend product and applicable documentation \\nare a function of the life cycle phase and the \\nplacement within the system structure. (The \\nform of the end product could be hardware, \\nsoftware, model, prototype, first article for \\ntest, or single operational article or multiple \\nproduction articles.) Documentation includes \\napplicable draft installation, operation, user, \\nmaintenance, or training manuals; applicable \\nbaseline documents (configuration baseline, \\nspecifications, and stakeholder expectations); \\nand test results that reflect completion of veri -\\nfication and validation of the end product.\\n2. Delivered operational end product for end \\nusers: The appropriate documentation is \\nto accompany the delivered end product as \\nwell as the operational end product appro -\\npriately packaged. Documentation includes \\napplicable final installation, operation, user, \\nmaintenance, or training manuals; applica -\\nble baseline documents (configuration base -\\nline, specifications, stakeholder expectations); \\nand test results that reflect completion of verification and validation of the end product. \\nIf the end user will perform end product val -\\nidation, sufficient documentation to support \\nend user validation activities is delivered with \\nthe end  product.\\n• Work products from transition activities to tech -\\nnical data management:  Work products could \\ninclude the transition plan, site surveys, measures, \\ntraining modules, procedures, decisions, lessons \\nlearned, corrective actions, etc.\\n• Realized enabling end products to appropri -\\nate life cycle support organization:  Some of the \\nenabling products that were developed during the \\nvarious phases could include fabrication or integra -\\ntion specialized machines; tools; jigs; fabrication \\nprocesses and manuals; integration processes and \\nmanuals; specialized inspection, analysis, demon -\\nstration, or test equipment; tools; test stands; \\nspecialized packaging materials and containers; \\nhandling equipment; storage-site environments; \\nshipping or transportation vehicles or equipment; \\nspecialized courseware; instructional site environ -\\nments; and delivery of the training instruction. \\nFor the later life cycle phases, enabling products \\nthat are to be delivered may include specialized \\nmission control equipment; data collection equip -\\nment; data analysis equipment; operations man -\\nuals; specialized maintenance equipment, tools, \\nmanuals, and spare parts; specialized recovery \\nequipment; disposal equipment; and readying \\nrecovery or disposal site environments.\\nThe process is complete when the following activities \\nhave been accomplished:\\n• For deliveries to the integration path, the end \\nproduct is delivered to intended usage sites in \\na condition suitable for integration with other \\nend products or composites of end products. 1125.0 Product Realization\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nProcedures, decisions, assumptions, anomalies, \\ncorrective actions, lessons learned, etc., resulting \\nfrom transition for integration are recorded.\\n• For delivery to the end user path, the end \\nproducts are installed at the appropriate sites; \\nappropriate acceptance and certification activi -\\nties are completed; training of users, operators, \\nmaintainers, and other necessary personnel is \\ncompleted; and delivery is closed out with appro -\\npriate acceptance documentation.• Any realized enabling end products are also deliv -\\nered as appropriate including procedures, deci -\\nsions, assumptions, anomalies, corrective actions, \\nlessons learned, etc., resulting from transition-en -\\nabling products.\\n5.5.2  Product Transition Guidance\\nRefer to Section 5.5.2 in the NASA Expanded \\nGuidance for Systems Engineering at https://nen.nasa.\\ngov/web/se/doc-repository  for additional guidance on:\\n• additional product transition considerations and\\n• what’s next after product transition to the end user.113\\nNASA SYSTEMS ENGINEERING HANDBOOK6.0Crosscutting Technical \\nManagement\\nThis chapter describes the activities in the techni -\\ncal management processes listed in the systems \\nengineering engine ( FIGURE 2.1-1 ). The processes \\ndescribed in Chapters 4  and 5 are performed through \\nthe design and realization phases of a product. These \\nprocesses can occur throughout the product life cycle, from concept through disposal. They may occur simultaneously with any of the other processes. The chapter is separated into sections corresponding to the technical management processes 10 through 17 listed in \\nFIGURE 2.1-1 . Each technical management \\nprocess is discussed in terms of the inputs, the activi -\\nties, and the outputs. Additional guidance is provided \\nusing examples that are relevant to NASA  projects.\\nThe technical management processes are the bridges between project management and the technical team. In this portion of the engine, eight processes provide the crosscutting functions that allow the design solu -\\ntion to be developed, realized, and to operate. Even though every technical team member might not be directly involved with these eight processes, they are indirectly affected by these key functions. Every member of the technical team relies on technical planning; management of requirements, interfaces, technical risk, configuration, and technical data; technical assessment; and decision analysis to meet the project’s objectives. Without these crosscutting processes, individual members and tasks cannot be integrated into a functioning system that meets the ConOps within cost and schedule. These technical processes also support the project management team in executing project control.\\nThe next sections describe each of the eight technical \\nmanagement processes and their associated products for a given NASA mission.\\n6.1 Technical Planning\\nThe Technical Planning Process, the first of the eight technical management processes contained in the sys-tems engineering engine, establishes a plan for apply -\\ning and managing each of the common technical processes that will be used to drive the development of system products and associated work products. This process also establishes a plan for identifying and defining the technical effort required to satisfy the project objectives and life cycle phase success cri -\\nteria within the cost, schedule, and risk constraints of the project.\\nThis effort starts with the technical team conduct -\\ning extensive planning early in Pre-Phase A. With 1146.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nthis early planning, technical team members will \\nunderstand the roles and responsibilities of each team member, and can establish cost and schedule goals and objectives. From this effort, the Systems Engineering Management Plan (SEMP) and other technical plans are developed and baselined. Once the SEMP and technical plans have been established, they should be synchronized with the project master plans and schedule. In addition, the plans for estab -\\nlishing and executing all technical contracting efforts are identified.CROSSCUTTING TECHNICAL MANAGEMENT KEYS\\n• Thoroughly understand and plan the scope of the technical effort by investing time upfront to develop the \\nte\\nchnical product breakdown structure, the technical schedule and workflow diagrams, and the technical \\nresource requirements and constraints (funding, budget, facilities, and long-lead items) that will be the technical planning infrastructure. The systems engineer also needs to be familiar with the non-technical aspects of the project.\\n•\\n Defi\\nne all interfaces and assign interface authorities and responsibilities to each, both intra-and inter-\\norganizational. This includes understanding potential incompatibilities and defining the transition processes.\\n•\\n Con\\ntrol of the configuration is critical to understanding how changes will impact the system. For example, \\nchanges in design and environment could invalidate previous analysis results.\\n• Con\\nduct milestone reviews to enable a critical and valuable assessment to be performed. These reviews \\nare not to be solely used to meet contractual or scheduling incentives. These reviews have specific entrance criteria and should be conducted when these are met.\\n•\\n Unde\\nrstand any biases, assumptions, and constraints that impact the analysis results.\\n• Place\\n all analysis under configuration control to be able to track the impact of changes and understand \\nwhen the analysis needs to be reevaluated.\\nTh\\nis is a recursive and iterative process. Early in the \\nlife cycle, the technical plans are established and syn -\\nchronized to run the design and realization processes. \\nAs the system matures and progresses through the life cycle, these plans should be updated as neces -\\nsary to reflect the current environment and resources and to control the project’s performance, cost, and schedule. At a minimum, these updates will occur at e\\nvery Key Decision Point (KDP). However, if there \\nis a significant change in the project, such as new stakeholder expectations, resource adjustments, or other constraints, all plans should be analyzed for the impact of these changes on the baselined project.\\n6.1.1 Process Description\\nFIGURE 6.1-1  provides a typical flow diagram for the \\nTechnical Planning Process and identifies typical inputs, outputs, and activities to consider in address -\\ning technical planning.\\n6.1.1.1  Inputs\\nInput to the Technical Planning Process comes from both the project management and technical teams as outputs from the other common technical processes. Initial planning utilizing external inputs from the project to determine the general scope and framework 1156.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nof the technical effort will be based on known tech -\\nnical and programmatic requirements, constraints, \\npolicies, and processes. Throughout the project’s life cycle, the technical team continually incorpo -\\nrates results into the technical planning strategy and documentation and any internal changes based on decisions and assessments generated by the other processes of the SE engine or from requirements and constraints mandated by the project.Project T ort\\nRequirements and P roject\\nResource Constraints\\nTo Technical Data\\nManagement ProcessFrom project \\nFrom Technical Data\\nManagement ProcessTo Technical\\nAssessment ProcessTo project \\nTo applicable technical\\nprocesses\\nTo applicable technical teams\\nFrom Technical Assessment\\nand Technical Risk\\nManagement ProcessesPrepare to conduct technical\\nplanning\\nk \\nSchedule, organize, and cost\\nthe technical work \\nPrepare SEMP and other\\ntechnical plans\\nObtain stakeholder commitment s\\nto technical plans\\nIssue authorized technical\\nwork direc tives\\nCapture work products from\\n technical planning activitiesAgreements, Capability\\nNeeds, Applicable Product-\\nLine Life-Cycle Phase\\nApplicable Policies, \\nProcedures, Standards, \\nand Organizational \\nProcesses\\nPrior Phase or\\nBaseline Plans\\nReplanning NeedsCost Estimates,\\nSchedules, and Resource\\nRequests\\nProduct and Process\\nMeasures\\nSEMP and Other\\nTechnical Plans\\nTechnical Planning\\nWork P roductsTechnical Work Directives\\nFIGURE 6.1 -1 Tec hnical Planning Process\\n• Project Technical Effort Requirements and \\nProject Resource Constraints:  The program/\\nproject plan provides the project’s top-level tech -\\nnical requirements, the available budget allocated \\nto the program/project from the program, and the desired schedule to support overall program needs. Although the budget and schedule allo -\\nca\\nted to the program/project serve as constraints, \\nthe technical team generates a technical cost estimate and schedule based on the actual work required to satisfy the technical requirements. Discrepancies between the allocated budget and schedule and the technical team’s actual cost esti -\\nmate and schedule should be reconciled continu -\\nously throughout the life cycle.\\n• Agreements, Capability Needs, Applicable \\nProduct Life Cycle Phase:  The program/project \\nplan also defines the applicable life cycle phases \\nand milestones, as well as any internal and exter -\\nnal agreements or capability needs required for successful execution. The life cycle phases and 1166.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nprogrammatic milestones provide the general \\nframework for establishing the technical plan -\\nning effort and for generating the detailed tech -\\nnical activities and products required to meet the \\noverall milestones in each of the life cycle phases.\\n• Applicable Policies, Procedures, Standards, \\nand Organizational Processes:  The program/\\nproject plan includes all programmatic policies, \\nprocedures, standards, and organizational pro -\\ncesses that should be adhered to during execution \\nof the technical effort. The technical team should \\ndevelop a technical approach that ensures the pro -\\ngram/project requirements are satisfied and that \\nany technical procedures, processes, and stan -\\ndards to be used in developing the intermediate \\nand final products comply with the policies and \\nprocesses mandated in the program/project plan.\\n• Prior Phase or Baseline Plans:  The latest tech -\\nnical plans (either baselined or from the previous \\nlife cycle phase) from the Data Management or \\nConfiguration Management Processes should be \\nused in updating the technical planning for the \\nupcoming life cycle phase.\\n• Replanning Needs:  Technical planning updates \\nmay be required based on results from technical \\nreviews conducted in the Technical Assessment \\nProcess, issues identified during the Technical \\nRisk Management Process, or from decisions \\nmade during the Decision Analysis Process.\\n6.1.1.2 Process Activities\\nTechnical planning as it relates to systems engineering \\nat NASA is intended to define how the project will be \\norganized, structured, and conducted and to identify, \\ndefine, and plan how the 17 common technical pro -\\ncesses in NPR 7123.1, NASA Systems Engineering \\nProcesses and Requirements will be applied in each \\nlife cycle phase for all levels of the product hierarchy (see Section 6.1.2.1 in the NASA Expanded Guidance \\nfor Systems Engineering at https://nen.nasa.gov/web/\\nse/doc-repository .) within the system structure to \\nmeet product life cycle phase success criteria. A key \\ndocument capturing and updating the details from \\nthe technical planning process is the SEMP.\\nThe SEMP is a subordinate document to the proj -\\nect plan. The project plan defines how the project \\nwill be managed to achieve its goals and objectives \\nwithin defined programmatic constraints. The SEMP \\ndefines for all project participants how the project \\nwill be technically managed within the constraints \\nestablished by the project. The SEMP also commu -\\nnicates how the systems engineering management \\ntechniques will be applied throughout all phases of \\nthe project life cycle.\\nTechnical planning should be tightly integrated \\nwith the Technical Risk Management Process (see \\nSection 6.4 ) and the Technical Assessment Process \\n(see Section 6.7 ) to ensure corrective action for future \\nactivities will be incorporated based on current issues \\nidentified within the project.\\nTechnical planning, as opposed to program or proj -\\nect planning, addresses the scope of the technical \\neffort required to develop the system products. While \\nthe project manager concentrates on managing the \\noverall project life cycle, the technical team, led by \\nthe systems engineer, concentrates on managing the \\ntechnical aspects of the project. The technical team \\nidentifies, defines, and develops plans for performing \\ndecomposition, definition, integration, verification, \\nand validation of the system while orchestrating and \\nincorporating the appropriate concurrent and cross -\\ncutting engineering. Additional planning includes \\ndefining and planning for the appropriate technical \\nreviews, audits, assessments, and status reports and \\ndetermining crosscutting engineering discipline and/\\nor design verification requirements.1176.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nThis section describes how to perform the activities \\ncontained in the Technical Planning Process shown \\nin FIGURE 6.1-1 . The initial technical planning at the \\nbeginning of the project establishes the technical \\nteam members; their roles and responsibilities; and \\nthe tools, processes, and resources that will be uti -\\nlized in executing the technical effort. In addition, \\nthe expected activities that the technical team will \\nperform and the products it will produce are iden -\\ntified, defined, and scheduled. Technical planning \\ncontinues to evolve as actual data from completed \\ntasks are received and details of near-term and future \\nactivities are known.\\n6.1.1.2.1 T echnical Planning Preparation\\nFor technical planning to be conducted properly, the \\nprocesses and procedures that are needed to conduct \\ntechnical planning should be identified, defined, \\nand communicated. As participants are identified, \\ntheir roles and responsibilities and any training and/\\nor certification activities should be clearly defined \\nand communicated.\\nTeam Selection\\nTeams engaged in the early part of the technical \\nplanning process need to identify the required skill \\nmix for technical teams that will develop and pro -\\nduce a product. Typically, a technical team consists \\nof a mix of both subsystem and discipline engineers. \\nConsidering a spacecraft example, subsystem engi -\\nneers normally have cognizance over development of \\na particular subsystem (e.g., mechanical, power, etc.), \\nwhereas discipline engineers normally provide spe -\\ncific analyses (e.g., flight dynamics, radiation, etc.). \\nThe availability of appropriately skilled personnel \\nalso needs to be considered.\\nTo an extent, determining the skill mix required \\nfor developing any particular product is a subjec -\\ntive process. Due to this, the skill mix is normally \\ndetermined in consultation with people experienced in leading design teams for a particular mission or \\ntechnical application. Some of the subjective consid -\\nerations involved include the product and its require -\\nments, the mission class, and the project phase.\\nContinuing with a spacecraft example, most teams \\ntypically share a common core of required skills, such \\nas subsystem engineering for mechanical, thermal, \\npower, etc. However, the particular requirements of \\na spacecraft and mission can cause the skill mix to \\nvary. For example, as opposed to robotic space mis -\\nsions, human-rated systems typically add the need \\nfor human systems discipline engineering and envi -\\nronmental control and life support subsystem engi -\\nneering. As opposed to near Earth space missions, \\ndeep space missions may add the need for safety and \\nplanetary protection discipline engineering specific \\nto contamination of the Earth or remote solar system \\nbodies. And, as opposed to teams designing space -\\ncraft instruments that operate at moderate tempera -\\ntures, teams designing spacecraft instruments that \\noperate at cryogenic temperatures will need cryogen -\\nics subsystem support.\\nMission class and project phase may also influence the \\nrequired team skill mix. For example, with respect to \\nmission class, certain discipline analyses needed for \\nClass A and B missions may not be required for Class \\nD (or lower) missions. And with respect to project \\nphase, some design and analyses may be performed by \\na single general discipline in Pre-Phase A and Phase \\nA, whereas the need to conduct design and analyses \\nin more detail in Phases B and C may indicate the \\nneed for multiple specialized subsystem design and \\ndiscipline engineering skills.\\nAn example skill mix for a Pre-Phase A technical \\nteam tasked to design a cryogenic interferometer \\nspace observatory is shown in TABLE 6.1-1  for pur -\\nposes of illustration. For simplicity, analysis and \\ntechnology development is assumed to be included 1186.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nTABLE 6.1-1 Example Engineering Team Disciplines in Pre-Phase A for Robotic Infrared Observatory\\nSystems Engineering\\nMission Systems Engineer\\nInstrument Systems Engineer\\nSpacecraft Bus, Flight Dynamics, Launch Vehicle Interface, Ground System Interface Subteam\\nFlight Dynamics Analysis\\nMission Operations (includes ConOps, & interfaces with ground station, mission ops center, science ops center)\\nBus Mechanical Subsystem (includes mechanisms)\\nBus Power Subsystem (includes electrical harness)\\nBus Thermal Subsystem\\nBus Propulsion Subsystem\\nBus Attitude Control and Determination Subsystem\\nBus Avionics Subsystem\\nBus Communications Subsystem\\nBus Flight Software Subsystem\\nIntegration and Test (bus, observatory)\\nLaunch Vehicle Integration\\nRadiation Analysis\\nOrbital Debris/End of Mission Planning Analysis\\nSystem Reliability/Fault Tolerance Analysis (includes analysis of instrument)\\nInstrument Subteam\\nMechanical Subsystem\\nMechanisms Subsystem\\nThermal Subsystem\\nCryogenics Subsystem\\nAvionics Subsystem (incl. Electrical Harness)\\nMechanism Drive Electronics Subsystem\\nDetector Subsystem\\nOptics Subsystem\\nControl Subsystem\\nMetrology Subsystem\\nFlight Software Subsystem\\nIntegration and Test\\nStray Light/Radiometry Analysis\\nOther Specialty Disciplines (e.g., Contamination Analysis) as needed1196.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nin the subsystem or discipline shown. For example, \\nthis means “mechanical subsystem” includes both \\nloads and dynamics analysis and mechanical tech -\\nnology development.\\nOnce the processes, people, and roles and responsi -\\nbilities are in place, a planning strategy may be for -\\nmulated for the technical effort. A basic technical \\nplanning strategy should address the following:\\n• The communication strategy within the technical \\nteam and for up and out communications;\\n• Identification and tailoring of NASA procedural \\nrequirements that apply to each level of the PBS \\nstructure;\\n• The level of planning documentation required \\nfor the SEMP and all other technical planning \\ndocuments;\\n• Identifying and collecting input documentation;\\n• The sequence of technical work to be conducted, \\nincluding inputs and outputs;\\n• The deliverable products from the technical work;\\n• How to capture the work products of technical \\nactivities;\\n• How technical risks will be identified and \\nmanaged;\\n• The tools, methods, and training needed to con -\\nduct the technical effort;\\n• The involvement of stakeholders in each facet of \\nthe technical effort;\\n• How the NASA technical team will be involved \\nwith the technical efforts of external contractors;\\n• The entry and success criteria for milestones, such \\nas technical reviews and life cycle phases;\\n• The identification, definition, and control of \\ninternal and external interfaces;\\n• The identification and incorporation of relevant \\nlessons learned into the technical planning;\\n• The team’s approach to capturing lessons learned \\nduring the project and how those lessons will be \\nrecorded;• The approach for technology development and \\nhow the resulting technology will be incorporated \\ninto the project;\\n• The identification and definition of the technical \\nmetrics for measuring and tracking progress to \\nthe realized product;\\n• The criteria for make, buy, or reuse decisions and \\nincorporation criteria for Commercial Off-the-\\nShelf (COTS) software and hardware;\\n• The plan to identify and mitigate off-nominal \\nperformance;\\n• The “how-tos” for contingency planning and \\nreplanning;\\n• The plan for status assessment and reporting;\\n• The approach to decision analysis, including \\nmaterials needed, skills required, and expecta -\\ntions in terms of accuracy; and\\n• The plan for managing the human element in the \\ntechnical activities and product.\\nBy addressing these items and others unique to \\nthe project, the technical team will have a basis for \\nunderstanding and defining the scope of the tech -\\nnical effort, including the deliverable products that \\nthe overall technical effort will produce, the schedule \\nand key milestones for the project that the technical \\nteam should support, and the resources required by \\nthe technical team to perform the work.\\nA key element in defining the technical planning \\neffort is understanding the amount of work associated \\nwith performing the identified activities. Once the \\nscope of the technical effort begins to coalesce, the \\ntechnical team may begin to define specific planning \\nactivities and to estimate the amount of effort and \\nresources required to perform each task. Historically, \\nmany projects have underestimated the resources \\nrequired to perform proper planning activities and \\nhave been forced into a position of continuous cri -\\nsis management in order to keep up with changes in \\nthe project.1206.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nIdentifying Facilities\\nThe planning process also includes identifying the \\nrequired facilities, laboratories, test beds, and instru -\\nmentation needed to build, test, launch, and operate a variety of commercial and Government products. A sample list of the kinds of facilities that might be con -\\nsidered when planning is illustrated in \\nTABLE 6.1-2 .\\n6.1.1.2.2  TABLE 6.1-2 Examples of Types of Facilities to Consider During Planning\\nCommunications and Tracking Labs\\nPower Systems Labs\\nPropulsion Test Stands\\nMechanical/Structures Labs\\nInstrumentation Labs\\nHuman Systems Labs\\nGuidance and Navigation Labs\\nRobotics Labs\\nSoftware Development Environment\\nMeeting Rooms\\nEducation/Outreach CentersModels and Simulation Labs\\nPrototype Development Shops\\nCalibration Labs\\nBiological Labs\\nSpace Materials Curation Labs\\nElectromagnetic Effects Labs\\nMaterials Labs\\nVacuum Chambers\\nMission Control Center\\nTraining Facilities\\nServer FarmsThermal Chambers\\nVibration Labs\\nRadiation Labs\\nAnimal Care Labs\\nFlight Hardware Storage Areas\\nDesign Visualization\\nWiring Shops\\nNDE Labs\\nLogistics Warehouse\\nConference Facilities\\nProject Documentation Centers\\nDefine the T echnical Work\\nThe technical effort should be defined commensurate with the level of detail needed for the life cycle phase. When performing the technical planning, realistic values for cost, schedule, and labor resources should be used. Whether extrapolated from historical databases or from interactive planning sessions with the proj -\\nect and stakeholders, realistic values should be calcu -\\nlated and provided to the project team. Contingency should be included in any estimate and should be based on the complexity and criticality of the effort. Contingency planning should be conducted. The fol -\\nlowing are examples of contingency planning:\\n• Additional, unplanned-for software engineering resources are typically needed during hardware and systems development and testing to aid in troubleshooting errors/anomalies. Frequently, s\\noftware engineers are called upon to help trou -\\nbleshoot problems and pinpoint the source of errors in hardware and systems development and testing (e.g., for writing additional test drivers to debug hardware problems). Additional software staff should be planned into the project contin -\\ngencies to accommodate inevitable component and system debugging and avoid cost and sched -\\nule overruns.\\n• Hardware-In-the-Loop (HWIL) should be accounted for in the technical planning contin -\\ngencies. HWIL testing is typically accomplished as a debugging exercise where the hardware and software are brought together for the first time in the costly environment of HWIL. If upfront work is not done to understand the messages and errors arising during this test, additional time in the HWIL facility may result in significant cost and schedule impacts. Impacts may be mitigated through upfront planning, such as making appro -\\npriate debugging software available to the techni -\\ncal team prior to the test, etc.1216.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n• Similarly, Human-In-The-Loop (HITL) evalua -\\ntions identify contingency operational issues. HITL \\ninvestigations are particularly critical early in the \\ndesign process to expose, identify, and cost-effec -\\ntively correct operational issues—nominal, main -\\ntenance, repair, off-nominal, training, etc.—in \\nthe required human interactions with the planned \\ndesign. HITL testing should also be approached \\nas a debugging exercise where hardware, software, \\nand human elements interact and their perfor -\\nmance is evaluated. If operational design and/or \\nperformance issues are not identified early, the cost \\nof late design changes will be significant.\\n6.1.1.2.3  Schedule, Organize, and Budget the T echnical \\nEffort\\nOnce the technical team has defined the technical \\nwork to be done, efforts can focus on producing a \\nschedule and cost estimate for the technical portion \\nof the project. The technical team should organize \\nthe technical tasks according to the project WBS \\nin a logical sequence of events, taking into consid -\\neration the major project milestones, phasing of \\navailable funding, and timing of the availability of \\nsupporting resources.\\nScheduling\\nProducts described in the WBS are the result of activ -\\nities that take time to complete. These activities have \\ntime precedence relationships among them that may \\nbe used to create a network schedule explicitly defin -\\ning the dependencies of each activity on other activ -\\nities, the availability of resources, and the receipt of \\nreceivables from outside sources. Use of a scheduling \\ntool may facilitate the development and maintenance \\nof the schedule.\\nScheduling is an essential component of planning \\nand managing the activities of a project. The process \\nof creating a network schedule provides a standard method for defining and communicating what needs \\nto be done, how long it will take, and how each ele -\\nment of the project WBS might affect other elements. \\nA complete network schedule may be used to calcu -\\nlate how long it will take to complete a project; which \\nactivities determine that duration (i.e., critical path \\nactivities); and how much spare time (i.e., float) exists \\nfor all the other activities of the project.\\n“Critical path” is the sequence of dependent tasks \\nthat determines the longest duration of time needed \\nto complete the project. These tasks drive the schedule \\nand continually change, so they should be updated. \\nThe critical path may encompass only one task or a \\nseries of interrelated tasks. It is important to identify \\nthe critical path and the resources needed to com -\\nplete the critical tasks along the path if the project is \\nto be completed on time and within its resources. As \\nthe project progresses, the critical path will change as \\nthe critical tasks are completed or as other tasks are \\ndelayed. This evolving critical path with its identified \\ntasks needs to be carefully monitored during the pro -\\ngression of the project.\\nNetwork scheduling systems help managers accu -\\nrately assess the impact of both technical and resource \\nchanges on the cost and schedule of a project. Cost \\nand technical problems often show up first as sched -\\nule problems. Understanding the project’s schedule \\nis a prerequisite for determining an accurate project \\nbudget and for tracking performance and progress. \\nBecause network schedules show how each activity \\naffects other activities, they assist in assessing and \\npredicting the consequences of schedule slips or \\naccelerations of an activity on the entire project.\\nFor additional information on scheduling, refer to \\nNASA/SP-2010-3403, NASA Schedule Management \\nHandbook.1226.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nBudgeting\\nBudgeting and resource planning involve establish -\\ning a reasonable project baseline budget and the capa -\\nbility to analyze changes to that baseline resulting \\nfrom technical and/or schedule changes. The proj -\\nect’s WBS, baseline schedule, and budget should be \\nviewed as mutually dependent, reflecting the techni -\\ncal content, time, and cost of meeting the project’s \\ngoals and objectives. The budgeting process needs to \\ntake into account whether a fixed cost cap or fixed \\ncost profile exists. When no such cap or profile exists, \\na baseline budget is developed from the WBS and \\nnetwork schedule. This specifically involves combin -\\ning the project’s workforce and other resource needs \\nwith the appropriate workforce rates and other finan -\\ncial and programmatic factors to obtain cost element \\nestimates. These elements of cost include\\n• direct labor costs,\\n• overhead costs,\\n• other direct costs (travel, data processing, etc.),\\n• subcontract costs,\\n• material costs,\\n• equipment costs,\\n• general and administrative costs,\\n• cost of money (i.e., interest payments, if \\napplicable),\\n• fee (if applicable), and\\n• contingency (Unallocated Future Expenses (UFE)).\\nFor additional information on cost estimating, refer \\nto the NASA Cost Estimating Handbook and NPR \\n7120.5, NASA Space Flight Program and Project \\nManagement Requirements .\\n6.1.1.2.4  Prepare the SEMP and Other T echnical Plans\\nSystems Engineering Management Plan\\nThe SEMP is the primary, top-level technical man -\\nagement document for the project and is developed \\nearly in the Formulation Phase and updated through -\\nout the project life cycle. The SEMP is driven by the type of project, the phase in the project life cycle, and \\nthe technical development risks and is written specif -\\nically for each project or project element. While the \\nspecific content of the SEMP is tailored to the project, \\nthe recommended content is discussed in Appendix J . \\nIt is important to remember that the main value of \\nthe SEMP is in the work that goes into the planning.\\nThe technical team, working under the overall project \\nplan, develops and updates the SEMP as necessary. \\nThe technical team works with the project manager \\nto review the content and obtain concurrence. This \\nallows for thorough discussion and coordination of \\nhow the proposed technical activities would impact \\nthe programmatic, cost, and schedule aspects of the \\nproject. The SEMP provides the specifics of the tech -\\nnical effort and describes the technical processes that \\nwill be used, how the processes will be applied using \\nappropriate activities, how the project will be orga -\\nnized to accomplish the activities, and the cost and \\nschedule associated with accomplishing the activities.\\nThe physical length of a SEMP is not what is import -\\nant. This will vary from project to project. The plan \\nneeds to be adequate to address the specific techni -\\ncal needs of the project. It is a living  document that \\nis updated as often as necessary to incorporate new \\ninformation as it becomes available and as the proj -\\nect develops through the Implementation Phase. The \\nSEMP should not duplicate other project documents; \\nhowever, the SEMP should reference and summarize \\nthe content of other technical plans.\\nThe systems engineer and project manager should \\nidentify additional required technical plans based on \\nthe project scope and type. If plans are not included \\nin the SEMP, they should be referenced and coordi -\\nnated in the development of the SEMP. Other plans, \\nsuch as system safety, probabilistic risk assessment, \\nand an HSI Plan also need to be planned for and \\ncoordinated with the SEMP. If a technical plan is a 1236.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nstand-alone, it should be referenced in the SEMP. \\nDepending on the size and complexity of the project, these may be separate plans or they may be included \\nwithin the SEMP. Once identified, the plans can be \\ndeveloped, training on these plans established, and the plans implemented. Examples of technical plans in addition to the SEMP are listed in \\nAppendix K .\\nThe SEMP should be developed during pre- \\nformulation. In developing the SEMP, the technical \\napproach to the project’s life cycle is developed. This determines the project’s length and cost. The devel -\\nopment of the programmatic and technical man -\\nagement approaches requires that the key project personnel develop an understanding of the work to \\nbe performed and the relationships among the vari -\\nous parts of that work. Refer to Sections 6.1.2.1 and \\n6.1.1.2 on WBSs and network scheduling, respec-tively. The SEMP then flows into the project plan to ensure the proper allocation of resources including \\ncost, schedule, and personnel.\\nThe SEMP’s development requires contributions \\nfrom knowledgeable programmatic and technical experts from all areas of the project that can signifi -\\ncantly influence the project’s outcome. The involve -\\nment of recognized experts is needed to establish a SEMP that is credible to the project manager and to secure the full commitment of the project team.\\nRole of the SEMP\\nThe SEMP is the rule book that describes to all par -\\nticipants how the project will be technically man -\\naged. The NASA technical team on the project \\nshould have a SEMP to describe how it will con -\\nduct its technical management, and each contractor \\nshould have a SEMP to describe how it will man -\\nage in accordance with both its contract and NASA’s technical management practices. Since the SEMP is unique to a project and contract, it should be \\nupdated for each significant programmatic change or it will become outmoded and unused and the proj -\\nect could slide into an uncontrolled state. The lead \\nNASA field Center should have its SEMP developed before attempting to prepare an initial cost estimate since activities that incur cost, such as technical risk reduction and human element accounting, need to be identified and described beforehand. The contractor \\nshould have its SEMP developed during the proposal \\nprocess (prior to costing and pricing) because the SEMP describes the technical content of the project, the potentially costly risk management activities, and the verification and validation techniques to be used, \\nall of which should be included in the preparation of \\nproject cost estimates. The SEMPs from the support -\\ning Centers should be developed along with the pri -\\nmary project SEMP. The project SEMP is the senior technical management document for the project; all other technical plans should comply with it. The \\nSEMP should be comprehensive and describe how a \\nfully integrated engineering effort will be managed \\nand conducted.\\nVerification Plan\\nThe verification plan is developed as part of the Technical Planning Process and is baselined at PDR. As the design matures throughout the life cycle, the \\nplan is updated and refined as needed. The task of \\npreparing the verification plan includes establishing the method of verification to be performed, depen -\\ndent on the life cycle phase; the position of the prod -\\nuct in the system structure; the form of the product used; and the related costs of verification of individ -\\nual specified requirements. The verification methods include analyses, inspection, demonstration, and test. In some cases, the complete verification of a given requirement might require more than one method. For example, to verify the performance of a product \\nmay require looking at many use cases. This might be \\naccomplished by running a Monte Carlo simulation (analysis) and also running actual tests on a few of the key cases. The verification plan, typically written 1246.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nat a detailed technical level, plays a pivotal role in \\nbottom-up product realization.\\nA phase product can be verified recursively through -\\nout the project life cycle and on a wide variety of \\nproduct forms. For example:\\nTYPES OF HARDWARE\\nBreadboard:  A low fidelity unit that demonstrates function only without considering form or fit in the case \\nof hardware or platform in the case of software. It often uses commercial and/or ad hoc components and is \\nnot intended to provide definitive information regarding operational performance.\\nBrassboard:  A medium fidelity functional unit that typically tries to make use of as much operational \\nhardware/software as possible and begins to address scaling issues associated with the operational \\nsystem. It does not have the engineering pedigree in all aspects, but is structured to be able to operate in simulated operational environments in order to assess performance of critical functions.\\nEngineering Unit:  A high fidelity unit that demonstrates critical aspects of the engineering processes \\ninvolved in the development of the operational unit. Engineering test units are intended to closely resemble \\nthe final product (hardware/software) to the maximum extent possible and are built and tested so as to establish confidence that the design will function in the expected environments. In some cases, the engineering unit will become the final product, assuming proper traceability has been exercised over the components and hardware handling.\\nPrototype Unit: The prototype unit demonstrates form, fit, and function at a scale deemed to be \\nrepresentative of the final product operating in its operational environment. A subscale test article provides fidelity sufficient to permit validation of analytical models capable of predicting the behavior of full-scale systems in an operational environment.\\nQualification Unit: A unit that is the same as the flight unit (form, fit, function, components, etc.) that will be \\nexposed to the extremes of the environmental criteria (thermal, vibration, etc.). The unit will typically not be flown due to these off-nominal stresses.\\nProtoflight Unit:  In projects that will not develop a qualification unit, the flight unit may be designated as a \\nprotoflight unit and a limited version of qualification test ranges will be applied. This unit will be flown.Flight Unit:  The end product that will be flown and will typically undergo acceptance level testing.• simulated (algorithmic models, virtual reality \\nsimulator);\\n• mock-up (plywood, brassboard, breadboard);\\n• concept description (paper report);\\n• engineering unit (fully functional but may not be same form/fit);\\n• prototype (form, fit, and function);1256.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n• design verification test units (form, fit, and func -\\ntion is the same, but they may not have flight parts);\\n• qualification units (identical to flight units but \\nmay be subjected to extreme environments); and\\n• flight units (end product that is flown, including \\nprotoflight units).\\nVerification of the end product—that is, the official \\n“run for the record” verification where the program/\\nproject takes credit for meeting a requirement—is \\nusually performed on a qualification, protoflight, or \\nflight unit to ensure its applicability to the flight sys -\\ntem. However, with discussion and approval from the \\nprogram/project and systems engineering teams, ver -\\nification credit may be taken on lower fidelity units \\nif they can be shown to be sufficiently like the flight \\nunits in the areas to be verified.\\nAny of these types of product forms may be in any of \\nthese states:\\n• produced (built, fabricated, manufactured, or coded);\\n• reused (modified internal non-developmental \\nproducts or OTS product); or\\n• assembled and integrated (a composite of low -\\ner-level products).\\nThe conditions and environment under which the \\nproduct is to be verified should be established and \\nthe verification should be planned based on the asso -\\nciated entrance/exit criteria that are identified. The \\nDecision Analysis Process should be used to help \\nfinalize the planning details.\\nNOTE:  The final, official verification of the end \\nproduct should be on a controlled unit. Typically, \\nattempting to “buy off” a “shall” on a prototype is \\nnot acceptable; it is usually completed on a qualifi -\\ncation, flight, or other more final, controlled unit.Procedures should be prepared to conduct verifica -\\ntion based on the method (e.g., analysis, inspection, \\ndemonstration, or test) planned. These procedures \\nare typically developed during the design phase \\nof the project life cycle and matured as the design \\nis matured. Operational use scenarios are thought \\nthrough in order to explore all possible verification \\nactivities to be performed.\\nNOTE: Verification planning begins early in the \\nproject life cycle during the requirements devel -\\nopment phase. (See Section 4.2 .) The verification \\napproach to use should be included as part of \\nrequirements development to plan for future activi -\\nties, to establish special requirements derived from \\nidentified verification-enabling products, and to \\nensure that the requirements are verifiable. Updates \\nto verification planning continue throughout logical \\ndecomposition and design development, especially \\nas design reviews and simulations shed light on \\nitems under consideration. (See Section 6.1 .)\\nAs appropriate, project risk items are updated based \\non approved verification strategies that cannot dupli -\\ncate fully integrated test systems, configurations, \\nand/or target operating environments. Rationales, \\ntrade space, optimization results, and implications of \\nthe approaches are documented in the new or revised \\nrisk statements as well as references to accommodate \\nfuture design, test, and operational changes to the \\nproject baseline.\\nValidation Plan\\nThe validation plan is one of the work products of the \\nTechnical Planning Process and is generated during \\nthe Design Solution Process to validate the end prod -\\nuct against the baselined stakeholder expectations. \\nThis plan can take many forms. The plan describes \\nthe total Test and Evaluation (T&E) planning from 1266.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\ndevelopment of lower-end through higher-end prod -\\nucts in the system structure and through operational \\nT&E into production and acceptance. It may com -\\nbine the verification and validation plans into a single \\ndocument. (See Appendix I  for a sample Verification \\nand Validation Plan outline.)\\nThe methods of validation include test, demonstra -\\ntion, inspection, and analysis. While the name of \\neach method is the same as the name of the methods \\nfor verification, the purpose and intent as described \\nabove are quite different.\\nPlanning to conduct the product validation is a key \\nfirst step. The method of validation to be used (e.g., \\nanalysis, demonstration, inspection, or test) should \\nbe established based on the form of the realized end \\nproduct, the applicable life cycle phase, cost, sched -\\nule, resources available, and location of the system \\nproduct within the system structure.\\nAn established set or subset of expectations or behav -\\niors to be validated should be identified and the val -\\nidation plan reviewed (an output of the Technical \\nPlanning Process, based on design solution outputs) \\nfor any specific procedures, constraints, success cri -\\nteria, or other validation requirements. The condi -\\ntions and environment under which the product is \\nto be validated should be established and the vali -\\ndation should be planned based on the relevant life \\ncycle phase and associated success criteria identified. \\nThe Decision Analysis Process should be used to help \\nfinalize the planning details.\\nIt is important to review the validation plans with rel -\\nevant stakeholders and to understand the relationship \\nbetween the context of the validation and the con -\\ntext of use (human involvement). As part of the plan -\\nning process, validation-enabling products should be \\nidentified and scheduling and/or acquisition should \\nbe initiated.Procedures should be prepared to conduct valida -\\ntion based on the method planned; e.g., analysis, \\ninspection, demonstration, or test). These proce -\\ndures are typically developed during the design phase \\nof the project life cycle and matured as the design \\nis matured. Operational and use-case scenarios are \\nthought through in order to explore all possible vali -\\ndation activities to be performed.\\nValidation is conducted by the user/operator or by \\nthe developer as determined by NASA Center direc -\\ntives or the contract with the developers. Systems-\\nlevel validation (e.g., customer Test and Evaluation \\n(T&E) and some other types of validation) may be \\nperformed by an acquirer testing organization. For \\nthose portions of validation performed by the devel -\\noper, appropriate agreements should be negotiated \\nto ensure that validation proof-of-documentation is \\ndelivered with the product.\\nRegardless of the source (buy, make, reuse, assemble \\nand integrate) and the position in the system struc -\\nture, all realized end products should be validated \\nto demonstrate/confirm satisfaction of stakeholder \\nexpectations. Variations, anomalies, and out-of-com -\\npliance conditions, where such have been detected, \\nare documented along with the actions taken to \\nresolve the discrepancies. Validation is typically car -\\nried out in the intended operational environment or \\na relevant environment under simulated or actual \\noperational conditions, not necessarily under the \\ntightly controlled conditions usually employed for \\nthe Product Verification Process.\\nValidation of phase products can be performed recur -\\nsively throughout the project life cycle and on a wide \\nvariety of product forms. For example:\\n• simulated (algorithmic models, virtual reality \\nsimulator);\\n• mock-up (plywood, brassboard, breadboard);1276.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n• concept description (paper report);\\n• engineering unit (functional but may not be same \\nform/fit);\\n• prototype (product with form, fit, and function);\\n• design validation test units (form, fit, and func-tion may be the same, but they may not have flight parts);\\n• qualification unit (identical to flight unit but may be subjected to extreme environments); and\\n• flight unit (end product that is flown).\\nAny of these types of product forms may be in any of these states:\\n• produced (built, fabricated, manufactured, or coded);\\n• reused (modified internal non-developmental products or off-the-shelf product); or\\n• assembled and integrated (a composite of lower level products).ENVIRONMENTS\\nRelevant Environment: Not all systems, subsystems, and/or components need to be operated in the \\noperational environment in order to satisfactorily address performance margin requirements or stakeholder expectations. Consequently, the relevant environment is the specific subset of the operational environment that is required to demonstrate critical “at risk” aspects of the final product performance in an operational environment.\\nOperational Environment: The environment in which the final product will be operated. In the case of \\nspace flight hardware/software, it is space. In the case of ground-based or airborne systems that are not directed toward space flight, it is the environments defined by the scope of operations. For software, the environment is defined by the operational platform.\\nNO\\nTE: The final, official validation of the end \\nproduct should be for a controlled unit. Typically, attempting final validation against the ConOps on a prototype is not acceptable: it is usually completed on a q\\nualification, flight, or other more final, con -\\ntrolled unit.\\nNOTE:  In planning for validation, consideration \\nshould be given to the extent to which validation testing will be done. In many instances, off-nominal operational scenarios and nominal operational sce -\\nnarios should be utilized. Off-nominal testing offers insight into a system’s total performance charac -\\nteristics and often assists in identifying the design issues and human-machine interface, training, and procedural changes required to meet the mission goals and objectives. Off-nominal testing as well as nominal testing should be included when planning for validation.\\nFor additional information on technical plans, refer \\nto the following appendices of this document and to Section 6.1.1.2.4 of the NASA Expanded Guidance for Systems Engineering at \\nhttps://nen.nasa.gov/web/\\nse/doc-repository :1286.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n• Appendix H  Integration Plan Outline\\n• Appendix I   Verification and Validation Plan \\nOutline\\n• Appendix J  SEMP Content Outline\\n• Appendix K  Technical Plans\\n• Appendix L   Interface Requirements Document \\nOutline\\n• Appendix M  CM Plan Outline\\n• Appendix R  HSI Plan Content Outline\\n• Appendix S   Concept of Operations Annotated \\nOutline\\n6.1.1.2.5  Obtain Stakeholder Commitments to \\nT echnical Plans\\nStakeholder Roles in Project Planning\\nTo obtain commitments to the technical plans from \\nthe stakeholders, the technical team should ensure \\nthat the appropriate stakeholders, including subject \\ndomain experts, have a method to provide inputs and \\nto review the project planning for implementation of \\nstakeholder interests.\\nDuring the Formulation Phase, the roles of the stake -\\nholders should be defined in the project plan and the \\nSEMP. Review of these plans and the agreements \\nfrom the stakeholders to the content of these plans \\nconstitutes buy-in from the stakeholders to the techni -\\ncal approach. It is essential to identify the stakeholders \\nand get their concurrence on the technical approach.\\nLater in the project life cycle, stakeholders may be \\nresponsible for delivering products to the project. \\nInitial agreements regarding the responsibilities of \\nthe stakeholders are key to ensuring that the proj -\\nect technical team obtains the appropriate deliveries \\nfrom stakeholders.\\nStakeholder Involvement in Defining Requirements\\nThe identification of stakeholders is one of the early \\nsteps in the systems engineering process. As the proj -\\nect progresses, stakeholder expectations are flowed down through the Logical Decomposition Process, \\nand specific stakeholders are identified for all of the \\nprimary and derived requirements. A critical part \\nof the stakeholders’ involvement is in the definition \\nof the technical requirements. As requirements and \\nthe ConOps are developed, the stakeholders will be \\nrequired to agree to these products. Inadequate stake -\\nholder involvement leads to inadequate requirements \\nand a resultant product that does not meet the stake -\\nholder expectations. Status on relevant stakeholder \\ninvolvement should be tracked and corrective action \\ntaken if stakeholders are not participating as planned.\\nStakeholder Agreements\\nThroughout the project life cycle, communication \\nwith the stakeholders and commitments from the \\nstakeholders may be accomplished through the use of \\nagreements. Organizations may use an Internal Task \\nAgreement (ITA), a Memorandum Of Understanding \\n(MOU), or other similar documentation to establish \\nthe relationship between the project and the stake -\\nholder. These agreements are also used to document \\nthe customer and provider responsibilities for defin -\\ning products to be delivered. These agreements should \\nestablish the Measures of Effectiveness (MOEs) or \\nMeasures of Performance (MOPs) that will be used \\nto monitor the progress of activities. Reporting \\nrequirements and schedule requirements should be \\nestablished in these agreements. Preparation of these \\nagreements will ensure that the stakeholders’ roles \\nand responsibilities support the project goals and that \\nthe project has a method to address risks and issues as \\nthey are identified.\\nStakeholder Support for Forums\\nDuring development of the project plan and the \\nSEMP, forums are established to facilitate communi -\\ncation and document decisions during the life cycle of \\nthe project. These forums include meetings, working \\ngroups, decision panels, and control boards. Each of \\nthese forums should establish a charter to define the 1296.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nscope and authority of the forum and identify neces -\\nsary voting or nonvoting participants. Ad hoc mem -\\nbers may be identified when the expertise or input of \\nspecific stakeholders is needed when specific topics \\nare addressed. It is important to ensure that stake -\\nholders have been identified to support the forum.\\n6.1.1.2.6 Issue T echnical Work Directives\\nThe technical team provides technical work directives \\nto Cost Account Managers (CAMs). This enables the \\nCAMs to prepare detailed plans that are mutually \\nconsistent and collectively address all of the work to \\nbe performed. These plans include the detailed sched -\\nules and budgets for cost accounts that are needed for \\ncost management and EVM.\\nIssuing technical work directives is an essential \\nactivity during Phase B of a project when a detailed \\nplanning baseline is required. If this activity is not \\nimplemented, then the CAMs are often left with \\ninsufficient guidance for detailed planning. The \\nschedules and budgets that are needed for EVM will \\nthen be based on assumptions and local interpreta -\\ntions of project-level information. If this is the case, \\nit is highly likely that substantial variances will occur \\nbetween the baseline plan and the work performed. \\nProviding technical work directives to CAMs pro -\\nduces a more organized technical team. This activity \\nmay be repeated when replanning occurs.\\nThis “technical work directives” step produces: (1) \\nplanning directives to CAMs that result in (2) a con -\\nsistent set of cost account plans. Where EVM is called \\nfor, it produces (3) an EVM planning baseline, includ -\\ning a Budgeted Cost of Work Scheduled (BCWS).\\nThis activity is not limited to systems engineering. \\nThis is a normal part of project planning wherever \\nthere is a need for an accurate planning baseline. \\nFor additional information on Technical Work Directives, refer to Section 6.1.1.2.6 in the NASA \\nExpanded Guidance for Systems Engineering at \\nhttps://nen.nasa.gov/web/se/doc-repository .\\n6.1.1.2.7 Capture T echnical Planning Work Products\\nThe work products from the Technical Planning \\nProcess should be managed using either the Technical \\nData Management Process or the Configuration \\nManagement Process as required. Some of the more \\nimportant products of technical planning (i.e., the \\nWBS, the SEMP, and the schedule, etc.) are kept \\nunder configuration control and captured using \\nthe CM process. The Technical Data Management \\nProcess is used to capture trade studies, cost estimates, \\ntechnical analyses, reports, and other important \\ndocuments not under formal configuration control. \\nWork products, such as meeting minutes and corre -\\nspondence (including e-mail) containing decisions or \\nagreements with stakeholders also should be retained \\nand stored in project files for later reference.\\n6.1.1.3 Outputs\\nTypical outputs from technical planning activities are:\\n• Technical work cost estimates, schedules, and \\nresource needs:  e.g., funds, workforce, facili -\\nties, and equipment (to the project) within the \\nproject resources;\\n• Product and process measures:  Those needed \\nto assess progress of the technical effort and \\nthe effectiveness of processes (to the Technical \\nAssessment Process);\\n• SEMP and other technical plans:  Technical plan -\\nning strategy, WBS, SEMP, HSI Plan, V&V Plan, \\nand other technical plans that support implemen -\\ntation of the technical effort (to all processes; \\napplicable plans to technical processes);1306.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n• Technical work directives: e.g., work packages or \\ntask orders with work authorization (to applicable \\ntechnical teams); and\\n• Technical Planning Process work products: \\nIncludes products needed to provide reports, records, and non-deliverable outcomes of process activities (to the Technical Data Management Process).\\nThe resulting technical planning strategy constitutes an outline, or rough draft, of the SEMP. This serves as a starting point for the overall Technical Planning Process after initial preparation is complete. When preparations for technical planning are complete, the technical team should have a cost estimate and sched -\\nule for the technical planning effort. The budget and schedule to support the defined technical planning effort can then be negotiated with the project man -\\nager to resolve any discrepancies between what is needed and what is available. The SEMP baseline needs to be completed. Planning for the update of the SEMP based on programmatic changes needs to be developed and implemented. The SEMP needs to be approved by the appropriate level of authority.\\n6.1.2  Technical Planning Guidance\\nRefer to Section 6.1.2 in the NASA Expanded Guidance for Systems Engineering at \\nhttps://nen.nasa.\\ngov/web/se/doc-repository  for additional guidance on:• Work Breakdown Structure (WBS),\\n• cost definition and modeling, and\\n• lessons learned .\\nAdditional information on the WBS can also be found in NASA/SP-2010-3404, NASA Work Breakdown Structure Handbook  and on costing in the \\nNASA Cost Estimating Handbook.\\nDEFINITIONS\\nTraceability:  A discernible association between two or more logical entities such as requirements, system \\nelements, verifications, or tasks.\\nBidirectional traceability: The ability to trace any given requirement/expectation to its parent requirement/\\nexpectation and to its allocated children requirements/expectations.6.2 Requirements Management\\nRequirements management activities apply to the \\nmanagement of all stakeholder expectations, cus -\\ntomer requirements, and technical product require -\\nments down to the lowest level product component requirements (hereafter referred to as expectations and requirements). This includes physical functional and operational requirements, including those that result from interfaces between the systems in ques -\\ntion and other external entities and environments. The Requirements Management Process is used to:\\n• Identify, control, decompose, and allocate require -\\nments across all levels of the WBS.\\n• Provide bidirectional traceability.1316.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n• Manage the changes to established requirement \\nbaselines over the life cycle of the system products.\\n6.2.1  Process Description\\nFIGURE 6.2-1  provides a typical flow diagram for the \\nRequirements Management Process and identifies typical inputs, outputs, and activities to consider in addressing requirements management.Expectations and \\nRequirements to Be\\nManagedFrom  system design processes \\nFrom Product Verification\\nand Validation ProcessesTo Technical Data\\nManagement ProcessTo Configuration\\nManagement  ProcessPrepare to conduct\\nrequirements management\\nConduct requirements\\nmanagement\\nConduct expectations and \\nrequirements traceability\\nManage ex pectations\\nand requirement changes\\nCapture work products from\\nrequirements management\\nactivitiesRequirements Change\\nRequests\\nTPM Estimation/\\nEvaluation ResultsRequirements\\nDocuments\\nApproved Changes to\\nRequirements Baselines\\nRequirements\\nManagement Work\\nProductsFrom project and Technical\\nAssessment P rocess\\nFrom Technical\\nAssessment P rocess\\nProduct Verification and\\nProduct Validation Results \\nFIGURE 6.2 -1 Re quirements Management Process\\n6.2.1.1  Inputs\\nThere are several fundamental inputs to the Requirements Management Process.\\n• Expectations and requirements to be managed: \\nRequirements and stakeholder expectations are identified during the system design processes, primarily from the Stakeholder Expectations Definition Process and the Technical Requirements Definition Process.• Requirement change requests: The Requirements \\nManagement Process should be prepared to deal with requirement change requests that can be generated at any time during the project life cycle or as a result of reviews and assessments as part of the Technical Assessment Process.\\n• TPM estimation/evaluation results: TPM esti -\\nmation/evaluation results from the Technical Assessment Process provide an early warning of the adequacy of a design in satisfying selected crit -\\nical technical parameter requirements. Variances from expected values of product performance may trigger changes to requirements.\\n• Product verification and validation results: \\nProduct verification and product validation results from the Product Verification and Product Validation Processes are mapped into the 1326.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nrequirements database with the goal of verifying \\nand validating all requirements.\\n6.2.1.2 Process Activities\\n6.2.1.2.1  Prepare to Conduct Requirements \\nManagement\\nPreparing to conduct requirements management \\nincludes gathering the requirements that were defined \\nand baselined during the Requirements Definition \\nProcess. Identification of the sources/owners of each \\nrequirement should be checked for currency. The \\norganization (e.g., change board) and procedures to \\nperform requirements management are established.\\n6.2.1.2.2 Conduct Requirements Management\\nThe Requirements Management Process involves \\nmanaging all changes to expectations and require -\\nments baselines over the life of the product and main -\\ntaining bidirectional traceability between stakeholder \\nexpectations, customer requirements, technical prod -\\nuct requirements, product component requirements, \\ndesign documents, and test plans and procedures. \\nThe successful management of requirements involves \\nseveral key activities:\\n• Establish a plan for executing requirements \\nmanagement.\\n• Receive requirements from the system design pro -\\ncesses and organize them in a hierarchical tree \\nstructure.\\n• Maintain bidirectional traceability between \\nrequirements.\\n• Evaluate all change requests to the requirements \\nbaseline over the life of the project and make \\nchanges if approved by change board.\\n• Maintain consistency between the require -\\nments, the ConOps, and the architecture/design, and initiate corrective actions to eliminate \\ninconsistencies.\\n6.2.1.2.3  Conduct Expectations and Requirements \\nTraceability\\nAs each requirement is documented, its bidirectional \\ntraceability should be recorded. Each requirement \\nshould be traced back to a parent/source requirement \\nor expectation in a baselined document or identified \\nas self-derived and concurrence on it sought from the \\nnext higher level requirements sources. Examples of \\nself-derived requirements are requirements that are \\nlocally adopted as good practices or are the result \\nof design decisions made while performing the \\nactivities of the Logical Decomposition and Design \\nSolution  Processes.\\nThe requirements should be evaluated, independently \\nif possible, to ensure that the requirements trace is \\ncorrect and that it fully addresses its parent require -\\nments. If it does not, some other requirement(s) \\nshould complete fulfillment of the parent require -\\nment and be included in the traceability matrix. In \\naddition, ensure that all top-level parent document \\nrequirements have been allocated to the lower level \\nrequirements. If there is no parent for a particular \\nrequirement and it is not an acceptable self-derived \\nrequirement, it should be assumed either that the \\ntraceability process is flawed and should be redone \\nor that the requirement is “gold plating” and should \\nbe eliminated. Duplication between levels should be \\nresolved. If a requirement is simply repeated at a lower \\nlevel and it is not an externally imposed constraint, \\nit may not belong at the higher level. Requirements \\ntraceability is usually recorded in a requirements \\nmatrix or through the use of a requirements model -\\ning application.1336.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n6.2.1.2.4  Managing Expectations and Requirement \\nChanges\\nThroughout early Phase A, changes in requirements \\nand constraints will occur as they are initially defined \\nand matured. It is imperative that all changes be \\nthoroughly evaluated to determine the impacts on \\nthe cost, schedule, architecture, design, interfaces, \\nConOps, and higher and lower level requirements. \\nPerforming functional and sensitivity analyses will \\nensure that the requirements are realistic and evenly \\nallocated. Rigorous requirements verification and \\nvalidation will ensure that the requirements can \\nbe satisfied and conform to mission objectives. All \\nchanges should be subjected to a review and approval \\ncycle to maintain traceability and to ensure that the \\nimpacts are fully assessed for all parts of the system.\\nOnce the requirements have been validated and \\nreviewed in the System Requirements Review (SRR) \\nin late Phase A, they are placed under formal con -\\nfiguration control. Thereafter, any changes to the \\nrequirements should be approved by a Configuration \\nControl Board (CCB) or equivalent authority. The \\nsystems engineer, project manager, and other key \\nengineers usually participate in the CCB approval \\nprocesses to assess the impact of the change including \\ncost, performance, programmatic, and safety.\\nRequirement changes during Phases B and C are \\nmore likely to cause significant adverse impacts to \\nthe project cost and schedule. It is even more import -\\nant that these late changes are carefully evaluated to \\nfully understand their impact on cost, schedule, and \\ntechnical designs.\\nThe technical team should also ensure that the \\napproved requirements are communicated in a timely \\nmanner to all relevant people. Each project should \\nhave already established the mechanism to track and \\ndisseminate the latest project information. Further \\ninformation on Configuration Management (CM) \\ncan be found in Section 6.5 .6.2.1.2.5 Key Issues for Requirements Management\\nRequirements Changes\\nEffective management of requirements changes \\nrequires a process that assesses the impact of the pro -\\nposed changes prior to approval and implementation \\nof the change. This is normally accomplished through \\nthe use of the Configuration Management Process. \\nIn order for CM to perform this function, a baseline \\nconfiguration should be documented and tools used \\nto assess impacts to the baseline. Typical tools used to \\nanalyze the change impacts are as follows:\\n• Performance Margins: This tool is a list of key \\nperformance margins for the system and the \\ncurrent status of the margin. For example, the \\npropellant performance margin will provide the \\nnecessary propellant available versus the propel -\\nlant necessary to complete the mission. Changes \\nshould be assessed for their impact on perfor -\\nmance margins.\\n• CM Topic Evaluators List: This list is developed \\nby the project office to ensure that the appropriate \\npersons are evaluating the changes and provid -\\ning impacts to the change. All changes need to \\nbe routed to the appropriate individuals to ensure \\nthat the change has had all impacts identified. \\nThis list will need to be updated periodically.\\n• Risk System and Threats List:  The risk system \\ncan be used to identify risks to the project and \\nthe cost, schedule, and technical aspects of the \\nrisk. Changes to the baseline can affect the con -\\nsequences and likelihood of identified risk or can \\nintroduce new risk to the project. A threats list \\nis normally used to identify the costs associated \\nwith all the risks for the project. Project reserves \\nare used to mitigate the appropriate risk. Analyses \\nof the reserves available versus the needs identified \\nby the threats list assist in the prioritization for \\nreserve use.1346.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nThe process for managing requirements changes needs \\nto take into account the distribution of information \\nrelated to the decisions made during the change pro -\\ncess. The Configuration Management Process needs \\nto communicate the requirements change decisions to \\nthe affected organizations. During a board meeting to \\napprove a change, actions to update documentation \\nneed to be included as part of the change package. \\nThese actions should be tracked to ensure that affected \\ndocumentation is updated in a timely manner.\\nRequirements Creep\\n“Requirements creep” is the term used to describe \\nthe subtle way that requirements grow imperceptibly \\nduring the course of a project. The tendency for the \\nset of requirements is to relentlessly increase in size \\nduring the course of development, resulting in a sys -\\ntem that is more expensive and complex than origi -\\nnally intended. Often the changes are quite innocent \\nand what appear to be changes to a system are really \\nenhancements in disguise.\\nHowever, some of the requirements creep involves \\ntruly new requirements that did not exist, and could \\nnot have been anticipated, during the Technical \\nRequirements Definition Process. These new require -\\nments are the result of evolution, and if we are to \\nbuild a relevant system, we cannot ignore them.\\nThere are several techniques for avoiding or at least \\nminimizing requirements creep:\\n• The first line of defense is a good ConOps that has \\nbeen thoroughly discussed and agreed-to by the \\ncustomer and relevant stakeholders.\\n• In the early requirements definition phase, flush \\nout the conscious, unconscious, and undreamed-of \\nrequirements that might otherwise not be stated.• Establish a strict process for assessing require -\\nment changes as part of the Configuration \\nManagement Process.\\n• Establish official channels for submitting change \\nrequests. This will determine who has the author -\\nity to generate requirement changes and submit \\nthem formally to the CCB (e.g., a contractor-des -\\nignated representative, project technical leads, \\ncustomer/science team lead, or user).\\n• Measure the functionality of each requirement \\nchange request and assess its impact on the rest of \\nthe system. Compare this impact with the conse -\\nquences of not approving the change. What is the \\nrisk if the change is not approved?\\n• Determine if the proposed change can be accom -\\nmodated within the fiscal and technical resource \\nbudgets. If it cannot be accommodated within \\nthe established resource margins, then the change \\nmost likely should be denied.\\n6.2.1.2.6 Capture Work Products\\nThese products include maintaining and reporting \\ninformation on the rationale for and disposition and \\nimplementation of change actions, current require -\\nment compliance status and expectation, and require -\\nment baselines.\\n6.2.1.3 Outputs\\nTypical outputs from the requirements management \\nactivities are:\\n• Requirements Documents:  Requirements doc -\\numents are submitted to the Configuration \\nManagement Process when the requirements are \\nbaselined. The official controlled versions of these \\ndocuments are generally maintained in electronic \\nformat within the requirements management tool 1356.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nthat has been selected by the project. In this way, \\nthey are linked to the requirements matrix with \\nall of its traceable relationships.\\n• Approved Changes to the Requirements \\nBaselines:  Approved changes to the require -\\nments baselines are issued as an output of the \\nRequirements Management Process after careful \\nassessment of all the impacts of the requirements \\nchange across the entire product or system. A sin -\\ngle change can have a far-reaching ripple effect, \\nwhich may result in several requirement changes \\nin a number of documents.\\n• Various Requirements Management Work \\nProducts: Requirements management work prod -\\nucts are any reports, records, and undeliverable out -\\ncomes of the Requirements Management Process. \\nFor example, the bidirectional traceability status \\nwould be one of the work products that would be \\nused in the verification and validation reports.\\n6.2.2   Requirements Management \\nGuidance\\nRefer to Section 6.2.2 in the NASA Expanded \\nGuidance for Systems Engineering at https://nen.nasa.\\ngov/web/se/doc-repository  for additional guidance on:\\n• the Requirements Management Plan and\\n• requirements management tools.\\n6.3 Interface Management\\nThe definition, management, and control of inter -\\nfaces are crucial to successful programs or projects. \\nInterface management is a process to assist in con -\\ntrolling product development when efforts are divided \\namong parties (e.g., Government, contractors, geo -\\ngraphically diverse technical teams, etc.) and/or to \\ndefine and maintain compliance among the products \\nthat should interoperate.The basic tasks that need to be established involve \\nthe management of internal and external interfaces \\nof the various levels of products and operator tasks \\nto support product integration. These basic tasks are \\nas follows:\\n• Define interfaces;\\n• Identify the characteristics of the interfaces (phys -\\nical, electrical, mechanical, human, etc.);\\n• Ensure interface compatibility at all defined inter -\\nfaces by using a process documented and approved \\nby the project;\\n• Strictly control all of the interface processes \\nduring design, construction, operation, etc.;\\n• Identify lower level products to be assembled and \\nintegrated (from the Product Transition Process);\\n• Identify assembly drawings or other documenta -\\ntion that show the complete configuration of the \\nproduct being integrated, a parts list, and any \\nassembly instructions (e.g., torque requirements \\nfor fasteners);\\n• Identify end-product, design-definition-specified \\nrequirements (specifications), and configuration \\ndocumentation for the applicable work break -\\ndown structure model, including interface spec -\\nifications, in the form appropriate to satisfy the \\nproduct life cycle phase success criteria (from the \\nConfiguration Management Process); and\\n• Identify product integration-enabling products \\n(from existing resources or the Product Transition \\nProcess for enabling product realization).1366.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n6.3.1  Process Description\\nFIGURE 6.3-1  provides a typical flow diagram for the \\nInterface Management Process and identifies typical \\ninputs, outputs, and activities to consider in address -\\ning interface management.From Project and\\nTechnical Assessment\\nProcessesFrom user or program and\\nsystem design processes \\nTo Technical Data\\nManagement ProcessTo Configuration\\nManagement Process \\nInterface\\nRequirements Interface Con trol\\nDocuments \\nApproved Interface\\nRequirement\\nChanges\\nInterface Management\\nWork ProductsPrepare or update interface\\nmanagement procedures \\nConduc t interface management during\\nsystem design activities for each WBS-\\nlike model in the system structure\\nConduct  interface control Conduc t interface management during\\nproduct  integratio n activities  \\nCaptu re wor k products from interface\\nmanagement activities Interface\\nChanges \\nFIGURE 6.3 -1 Int erface Management Process\\n6.3.1.1  Inputs\\nTypical inputs needed to understand and address interface management would include the following:\\n• Interface Requirements:  These include the inter -\\nnal and external functional, physical, and perfor -\\nmance interface requirements developed as part of the Technical Requirements Definition Process for the product(s).\\n• Interface Change Requests: These include \\nchanges resulting from program or project agree -\\nments or changes on the part of the technical team as part of the Technical Assessment Process.Other inputs that might be useful are:\\n• System Description: This allows the design of \\nthe system to be explored and examined to deter -\\nmine where system interfaces exist. Contractor arrangements will also dictate where interfaces are needed.\\n• System Boundaries:  Documented physical \\nboundaries, components, and/or subsystems, which are all drivers for determining where inter -\\nfaces exist.\\n• Organizational Structure:  Decisions on which \\norganization will dictate interfaces, particularly when there is the need to jointly agree on shared interface parameters of a system. The program and project WBS will also provide organizational interface boundaries.\\n• Boards Structure:  Defined board structure that \\nidentifies organizational interface responsibilities.1376.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n6.3.1.2 Process Activities\\n6.3.1.2.1  Prepare or Update Interface Management \\nProcedures\\nThese procedures establish the interface management \\nresponsibilities, what process will be used to main -\\ntain and control the internal and external functional \\nand physical interfaces (including human), and how \\nthe change process will be conducted. Training of the \\ntechnical teams or other support may also be required \\nand planned.\\n6.3.1.2.2  Conduct Interface Management during \\nSystem Design Activities\\nDuring project Formulation, the ConOps of the \\nproduct is analyzed to identify both external and \\ninternal interfaces. This analysis will establish the \\norigin, destination, stimuli, and special character -\\nistics of the interfaces that need to be documented \\nand maintained. As the system structure and archi -\\ntecture emerges, interfaces will be added and existing \\ninterfaces will be changed and should be maintained. \\nThus, the Interface Management Process has a close \\nrelationship to other areas, such as requirements \\ndefinition and configuration management, during \\nthis period.\\n6.3.1.2.3  Conduct Interface Management during \\nProduct Integration\\nDuring product integration, interface management \\nactivities would support the review of integration and \\nassembly procedures to ensure interfaces are prop -\\nerly marked and compatible with specifications and \\ninterface control documents. The interface manage -\\nment process has a close relationship to verification \\nand validation. Interface control documentation and \\napproved interface requirement changes are used as \\ninputs to the Product Verification Process and the \\nProduct Validation Process, particularly where ver -\\nification test constraints and interface parameters \\nare needed to set the test objectives and test plans. Interface requirements verification is a critical aspect \\nof the overall system verification.\\n6.3.1.2.4 Conduct Interface Control\\nTypically, an Interface Working Group (IWG) estab -\\nlishes communication links between those responsi -\\nble for interfacing systems, end products, enabling \\nproducts, and subsystems. The IWG has the respon -\\nsibility to ensure accomplishment of the planning, \\nscheduling, and execution of all interface activities. \\nAn IWG is typically a technical team with appro -\\npriate technical membership from the interfacing \\nparties (e.g., the project, the contractor, etc.). The \\nIWG may work independently or as a part of a larger \\nchange control board.\\n6.3.1.2.5 Capture Work Products\\nWork products include the strategy and procedures \\nfor conducting interface management, rationale \\nfor interface decisions made, assumptions made in \\napproving or denying an interface change, actions \\ntaken to correct identified interface anomalies, les -\\nsons learned and updated support and interface \\nagreement documentation.\\n6.3.1.3 Outputs\\nTypical outputs needed to capture interface manage -\\nment would include:\\n• Interface control documentation.  This is the \\ndocumentation that identifies and captures the \\ninterface information and the approved inter -\\nface change requests. Types of interface docu -\\nmentation include the Interface Requirements \\nDocument (IRD), Interface Control Document/\\nDrawing (ICD), Interface Definition Document \\n(IDD), and Interface Control Plan (ICP). These \\noutputs will then be maintained and approved \\nusing the Configuration Management Process \\nand become a part of the overall technical data \\npackage for the project.1386.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n• Approved interface requirement changes.  After \\nthe interface requirements have been baselined, \\nthe Requirements Management Process should \\nbe used to identify the need for changes, eval -\\nuate the impact of the proposed change, docu -\\nment the final approval/disapproval, and update \\nthe requirements documentation/tool/database. \\nFor interfaces that require approval from all \\nsides, unanimous approval is required. Changing \\ninterface requirements late in the design or \\nimplementation life cycle is more likely to have a \\nsignificant impact on the cost, schedule, or tech -\\nnical design/operations.\\n• Other work products. These work products \\ninclude the strategy and procedures for con -\\nducting interface management, the rationale for \\ninterface decisions made, the assumption made \\nin approving or denying an interface change, \\nthe actions taken to correct identified interface \\nanomalies, the lessons learned in performing the \\ninterface management activities, and the updated \\nsupport and interface agreement documentation.\\n6.3.2  Interface Management Guidance\\nRefer to Section 6.3.2 in the NASA Expanded \\nGuidance for Systems Engineering at https://nen.nasa.\\ngov/web/se/doc-repository  for additional guidance on:\\n• interface requirements documents,\\n• interface control documents,\\n• interface control drawings,\\n• interface definition documents,\\n• the interface control plans, and\\n• interface management tasks.6.4 Technical Risk Management\\nThe Technical Risk Management Process is one of \\nthe crosscutting technical management processes. \\nRisk is the potential for performance shortfalls, \\nwhich may be realized in the future, with respect \\nto achieving explicitly established and stated per -\\nformance requirements. The performance shortfalls \\nmay be related to institutional support for mission \\nexecution or related to any one or more of the follow -\\ning mission execution domains:\\n• safety\\n• technical\\n• cost\\n• schedule\\nSystems engineers are involved in this process to help \\nidentify potential technical risks, develop mitigation \\nplans, monitor progress of the technical effort to deter -\\nmine if new risks arise or old risks can be retired, and \\nto be available to answer questions and resolve issues. \\nThe following is guidance in implementation of risk \\nmanagement in general. Thus, when implementing \\nrisk management on any given program/project, the \\nresponsible systems engineer should direct the effort \\naccordingly. This may involve more or less rigor and \\nformality than that specified in governing docu -\\nments such as NPRs. Of course, if deviating from \\nNPR “requirements,” the responsible engineer must \\nfollow the deviation approval process. The idea is to \\ntailor the risk management process so that it meets \\nthe needs of the individual program/project being \\nexecuted while working within the bounds of the \\ngoverning documentation (e.g., NPRs). For detailed \\ninformation on the Risk Management Process, refer \\nto the NASA Risk Management Handbook (NASA/\\nSP-2011-3422) .1396.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nRisk is characterized by three basic components:\\n1. The scenario(s) leading to degraded performance \\nwith respect to one or more performance mea -\\nsures (e.g., scenarios leading to injury, fatality, destruction of key assets; scenarios leading to exceedance of mass limits; scenarios leading to cost overruns; scenarios leading to schedule slippage);\\n2. The likelihood(s) (qualitative or quantitative) of those scenario(s); and\\n3. The consequence(s) (qualitative or quantitative severity of the performance degradation) that would result if the scenario(s) was (were) to occur.Uncertainties are included in the evaluation of likeli -\\nhoods and consequences.\\nScenarios begin with a set of initiating events that \\ncause the activity to depart from its intended state. For each initiating event, other events that are rele -\\nvant to the evolution of the scenario may (or may not) occur and may have either a mitigating or exacerbat -\\ning effect on the scenario progression. The frequen -\\ncies of scenarios with undesired consequences are determined. Finally, the multitude of such scenarios is put together, with an understanding of the uncer -\\ntainties, to create the risk profile of the system.\\nThis “risk triplet” conceptualization of risk is illus-\\ntrated in \\nFIGURES 6.4-1  and 6.4-2 .Uncertainty AnalysisScenario Development \\nScenario \\nModelingConsequence\\nModelingInitiating \\nEvent \\nSelection1. What can go wrong?\\n    (definition of scenarios)2. How frequently does  it happen? (scenario frequency quantification)3. What are the  consequences? (scenario consequence  quantification)\\nScenario \\nFrequency\\nEvaluation\\nFIGURE 6.4-1  Ris k Scenario Development (Source: NASA/SP-2011-3421)\\nRISK =  Structure of ScenarioLikelihood and\\nIts UncertaintyConsequence Severity\\nand Its Uncertainty\\nStructure of ScenarioLikelihood and\\nIts UncertaintyConsequence Severity\\nand Its Uncertainty\\nStructure of ScenarioLikelihood and\\nIts UncertaintyConsequence Severity\\nand Its Uncertainty\\nFIGURE 6.4-2  Ris k as an Aggregate Set of Risk Triplets1406.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nKey Concepts in Risk Management  Risk: Risk is the potential for shortfalls, which may be realized in the \\nfuture with respect to achieving explicitly-stated requirements. The performance shortfalls may be related to \\ninstitutional support for mission execution, or related to any one or more of the following mission execution \\ndomains: safety, technical, cost, schedule. Risk is characterized as a set of triplets:\\n >The scenario(s) leading to degraded performance in one or more performance measures.\\n >The likelihood(s) of those scenarios.\\n >The consequence(s), impact, or severity of the impact on performance that would result if those \\nscenarios were to occur.\\nUncertainties are included in the evaluation of likelihoods and consequences.\\nCost Risk:  This is the risk associated with the ability of the program/project to achieve its life-cycle cost \\nobjectives and secure appropriate funding. Two risk areas bearing on cost are (1) the risk that the cost \\nestimates and objectives are not accurate and reasonable; and (2) the risk that program execution will not \\nmeet the cost objectives as a result of a failure to handle cost, schedule, and performance risks.\\nSchedule Risk: Schedule risks are those associated with the adequacy of the time estimated and allocated \\nfor the development, production, implementation, and operation of the system. Two risk areas bearing on \\nschedule risk are (1) the risk that the schedule estimates and objectives are not realistic and reasonable; and \\n(2) the risk that program execution will fall short of the schedule objectives as a result of failure to handle \\ncost, schedule, or performance risks.\\nTechnical Risk: This is the risk associated with the evolution of the design and the production of the \\nsystem of interest affecting the level of performance necessary to meet the stakeholder expectations and \\ntechnical requirements. The design, test, and production processes (process risk) influence the technical \\nrisk and the nature of the product as depicted in the various levels of the PBS (product risk).\\nProgrammatic Risk:  This is the risk associated with action or inaction from outside the project, over \\nwhich the project manager has no control, but which may have significant impact on the project. These \\nimpacts may manifest themselves in terms of technical, cost, and/or schedule. This includes such activities \\nas: International Traffic in Arms Regulations (ITAR), import/export control, partner agreements with other \\ndomestic or foreign organizations, congressional direction or earmarks, Office of Management and Budget \\n(OMB) direction, industrial contractor restructuring, external organizational changes, etc.\\nScenario:  A sequence of credible events that specifies the evolution of a system or process from a given \\nstate to a future state. In the context of risk management, scenarios are used to identify the ways in which a \\nsystem or process in its current state can evolve to an undesirable state.1416.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nUndesired scenario(s) might come from technical or \\nprogrammatic sources (e.g., a cost overrun, schedule slippage, safety mishap, health problem, malicious activities, environmental impact, or failure to achieve a needed scientific or technological objective or suc -\\ncess criterion). Both the likelihood and consequences may have associated uncertainties.\\n6.4.1   Risk Management Process \\nDescription\\nFIGURE 6.4-3  provides a typical flow diagram for \\nthe Risk Management Process and identifies typical \\ninputs, activities, and outputs to consider in address -\\ning risk management.\\nFrom Technical\\nAssessment and\\nDecision Analysis\\nProcesses  From project and all\\ntechnical processes\\nTo project and Technical\\nData Management\\nProcess \\nTo Technical Data\\nManagement ProcessTo Technical\\nPlanning Process \\nTechnical Risk Status\\nMea surements \\nTechnical Risk\\nReporting\\nRequirements Technical Risk\\nIssuesTechnical Risk\\nMitigation and/or\\nContingency Actions \\nTechnical Risk\\nReports \\nWork Products of\\nTechnical Risk\\nManagement From project and\\nTechnical Assessment\\nProcess From project\\nProject Risk\\nManagement Plan Prepare a strategy to conduct technical\\nrisk management \\nIdentify technical risks\\nPrepare for technical risk mitigationConduct technical risk assessment\\nMonitor the status of each technical\\nrisk periodically \\nImplement technical risk mitigation and\\ncontingency action plans as triggered \\nCaptur e wor k products from technical\\nrisk management activities\\nFIGURE 6.4-3  Risk  Management Process6.4.1.1  Inputs\\nThe following are typical inputs to risk management:• Project Risk Management Plan:  The Risk \\nManagement Plan is developed under the Technical Planning Process and defines how risk will be identified, mitigated, monitored, and con -\\ntrolled within the project.\\n• Technical Risk Issues: These will be the techni -\\ncal issues identified as the project progresses that pose a risk to the successful accomplishment of the project mission/goals.\\n• Technical Risk Status Measurements: These \\nare any measures that are established that help to monitor and report the status of project \\ntechnical  risks.1426.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n• Technical Risk Reporting Requirements: \\nIncludes requirements of how technical risks will \\nbe reported, how often, and to whom.\\nAdditional inputs that may be useful:\\n• Other Plans and Policies:  Systems Engineering \\nManagement Plan, form of technical data prod -\\nucts, and policy input to metrics and thresholds.\\n• Technical Inputs: Stakeholder expectations, con -\\ncept of operations, imposed constraints, tracked \\nobservables, current program baseline, perfor -\\nmance requirements, and relevant experience data.\\n6.4.1.2 Activities\\n6.4.1.2.1  Prepare a Strategy to Conduct T echnical Risk \\nManagement\\nThis strategy would include documenting how the \\nprogram/project risk management plan (as devel -\\noped during the Technical Planning Process) will be \\nimplemented, identifying any additional technical \\nrisk sources and categories not captured in the plan, \\nidentifying what will trigger actions and how these \\nactivities will be communicated to the internal and \\nexternal teams.\\n6.4.1.2.2 Identify T echnical Risks\\nOn a continuing basis, the technical team will iden -\\ntify technical risks including their source, analyze \\nthe potential consequence and likelihood of the risks \\noccurring, and prepare clear risk statements for entry \\ninto the program/project risk management system. \\nCoordination with the relevant stakeholders for the \\nidentified risks is included. For more information on \\nidentifying technical risks, see Section 6.4.2.1.\\n6.4.1.2.3 Conduct T echnical Risk Assessment\\nUntil recently, NASA’s Risk Management (RM) \\napproach was based almost exclusively on Continuous \\nRisk Management (CRM), which stresses the management of individual risk issues during imple -\\nmentation. In December of 2008, NASA revised its \\nRM approach in order to more effectively foster pro -\\nactive risk management. The new approach, which is \\noutlined in NPR 8000.4, Agency Risk Management \\nProcedural Requirements and further developed \\nin NASA/SP-2011-3422, NASA Risk Management \\nHandbook , evolves NASA‘s risk management to \\nentail two complementary processes: Risk-Informed \\nDecision Making (RIDM) and CRM. RIDM is \\nintended to inform direction-setting systems engi -\\nneering (SE) decisions (e.g., design decisions) through \\nbetter use of risk and uncertainty information in \\nselecting alternatives and establishing baseline perfor -\\nmance requirements (for additional RIDM technical \\ninformation, guidance, and process description, see \\nNASA/SP-2010-576 Version 1, NASA Risk-Informed \\nDecision Making Handbook ).\\nCRM is then used to manage risks over the course \\nof the development and implementation phases of \\nthe life cycle to assure that requirements related to \\nsafety, technical, cost, and schedule are met. In the \\npast, RM was considered equivalent to the CRM \\nprocess; now, RM is defined as comprising both the \\nRIDM and CRM processes, which work together to \\nassure proactive risk management as NASA programs \\nand projects are conceived, developed, and executed. \\nFIGURE 6.4-4  illustrates the concept.\\nFIGURE 6.4-4 Risk Management as the Interaction of \\nRisk-Informed Decision Making and Continuous Risk \\nManagement (Source: NASA/SP-2011-3422)1436.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n6.4.1.2.4 Prepare for T echnical Risk Mitigation\\nThis includes selecting the risks that will be mitigated \\nand more closely monitored, identifying the risk level \\nor threshold that will trigger a risk mitigation action \\nplan, and identifying for each risk which stakeholders \\nwill need to be informed that a mitigation/contin -\\ngency action is determined as well as which organi -\\nzations will need to become involved to perform the \\nmitigation/contingency action.\\n6.4.1.2.5  Monitor the Status of Each T echnical Risk \\nPeriodically\\nRisk status will need to be monitored periodically at \\na frequency identified in the risk plan. Risks that are \\napproaching the trigger thresholds will be monitored \\non a more frequent basis. Reports of the status are \\nmade to the appropriate program/project manage -\\nment or board for communication and for decisions \\nwhether to trigger a mitigation action early. Risk sta -\\ntus will also be reported at most life cycle reviews.\\n6.4.1.2.6  Implement T echnical Risk Mitigation and \\nContingency Action Plans as Triggered\\nWhen the applicable thresholds are triggered, the \\ntechnical risk mitigation and contingency action \\nplans are implemented. This includes monitoring the \\nresults of the action plan implementation and mod -\\nifying them as necessary, continuing the mitigation \\nuntil the residual risk and/or consequence impacts \\nare acceptable, and communicating the actions and \\nresults to the identified stakeholders. Action plan \\nreports are prepared and results reported at appropri -\\nate boards and at life cycle reviews.\\n6.4.1.2.7 Capture Work Products\\nWork products include the strategy and procedures for \\nconducting technical risk management; the rationale \\nfor decisions made; assumptions made in prioritizing, \\nhandling, and reporting technical risks and action \\nplan effectiveness; actions taken to correct action plan \\nimplementation anomalies; and lessons learned.6.4.1.3 Outputs\\nFollowing are key risk outputs from activities:\\n• Technical Risk Mitigation and/or Contingency \\nActions: Actions taken to mitigate identified risks \\nor contingency actions taken in case risks are \\nrealized.\\n• Technical Risk Reports: Reports of the techni -\\ncal risk policies, status, remaining residual risks, \\nactions taken, etc. Output at the agreed-to fre -\\nquency and recipients.\\n• Work Products: Includes the procedures for con -\\nducting technical risk management; rationale for \\ndecisions made; selected decision alternatives; \\nassumptions made in prioritizing, handling, and \\nreporting technical risks; and lessons learned.\\n6.4.2   Risk Management Process \\nGuidance\\nFor additional guidance on risk management, refer to \\nNASA/SP-2010-576, NASA RIDM Handbook  and NASA/\\nSP-2011-3422, NASA Risk Management Handbook .\\n6.5 Configuration Management\\nConfiguration management is a management disci -\\npline applied over the product’s life cycle to provide \\nvisibility into and to control changes to perfor -\\nmance and functional and physical characteristics. \\nAdditionally, according to SAE Electronic Industries \\nAlliance (EIA) 649B, improper configuration man -\\nagement may result in incorrect, ineffective, and/or \\nunsafe products being released. Therefore, in order to \\nprotect and ensure the integrity of NASA products, \\nNASA has endorsed the implementation of the five \\nconfiguration management functions and the asso -\\nciated 37 underlying principles defined within SAE/\\nEIA-649-2 Configuration Management Requirements \\nfor NASA Enterprises.1446.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nTogether, these standards address what configuration \\nmanagement activities are to be done, when they are \\nto happen in the product life cycle, and what plan -\\nning and resources are required. Configuration man -\\nagement is a key systems engineering practice that, \\nwhen properly implemented, provides visibility of a \\ntrue representation of a product and attains the prod -\\nuct’s integrity by controlling the changes made to the \\nbaseline configuration and tracking such changes. \\nConfiguration management ensures that the con -\\nfiguration of a product is known and reflected in \\nproduct information, that any product change is ben -\\neficial and is effected without adverse consequences, \\nand that changes are managed.\\nCM reduces technical risks by ensuring correct prod -\\nuct configurations, distinguishes among product \\nversions, ensures consistency between the product \\nand information about the product, and avoids the \\nembarrassment cost of stakeholder dissatisfaction and \\ncomplaint. In general, NASA adopts the CM prin -\\nciples as defined by SAE/EIA 649B, Configuration \\nManagement Standard , in addition to implementa -\\ntion as defined by NASA CM professionals and as \\napproved by NASA management.\\nWhen applied to the design, fabrication/assembly, \\nsystem/subsystem testing, integration, and opera -\\ntional and sustaining activities of complex technol -\\nogy items, CM represents the “backbone” of the \\nenterprise structure. It instills discipline and keeps \\nthe product attributes and documentation consistent. \\nCM enables all stakeholders in the technical effort, at \\nany given time in the life of a product, to use identical \\ndata for development activities and decision-making. \\nCM principles are applied to keep the documentation \\nconsistent with the approved product, and to ensure \\nthat the product conforms to the functional and \\nphysical requirements of the approved design.6.5.1  Process Description\\nFIGURE 6.5-1  provides a typical flow diagram for the \\nConfiguration Management Process and identifies \\ntypical inputs, outputs, and activities to consider in \\naddressing CM.\\n6.5.1.1 Inputs\\nThe inputs for this process are:\\n• CM plan: This plan would have been developed \\nunder the Technical Planning Process and serves \\nas the overall guidance for this process for the \\nprogram/project\\n• Engineering change proposals:  These are the \\nrequests for changes to the established baselines \\nin whatever form they may appear throughout the \\nlife cycle.\\n• Expectation, requirements and interface docu -\\nments:  These baselined documents or models are \\nkey to the design and development of the product.\\n• Approved requirements baseline changes: The \\napproved requests for changes will authorize the \\nupdate of the associated baselined document or \\nmodel.\\n• Designated configuration items to be controlled: \\nAs part of technical planning, a list or philosophy \\nwould have been developed that identifies the \\ntypes of items that will need to be placed under \\nconfiguration control.\\n6.5.1.2 Process Activities\\nThere are five elements of CM (see FIGURE 6.5-2 ):\\n• configuration planning and management\\n• configuration identification,1456.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n• configuration change management,\\n• Configuration Status Accounting (CSA), and\\n• configuration verification.From project\\nFrom Requirements\\n and Interface \\nManagement Processes\\nTo Technical Data\\nManagement ProcessTo applicable\\ntechnical processes \\nTo project and\\nTechnical Data\\nManagement ProcessExpectation, \\nRequirements, and\\nInterface Documents Engineering Change\\nProposalsProject Configuration\\nManagement Plan \\nList of Configuration\\nItems Under\\nControl\\nConfiguration\\nManagement\\nWork Products Current\\nBaselines\\nConfiguration\\nManagement\\nReports Approved\\nRequirement\\nBaseline ChangesPrepare a strategy to conduct\\nconfiguration management\\nIdentify baseline to be under\\nconfiguration control\\nMaintain the status of configuration Maintain the status of configuration \\ndocumentation Manage configuration change control\\nConduct configuration audits\\nCaptur e work  products from\\nconfiguration management activities  \\nIdentified Risk\\nto Be Contr olledDesignated\\nConfiguration Items\\nto Be Contr olled\\nFIGURE 6.5 -1 Confi guration Management Process\\n6.5.1.2.1  Prepare a Strategy to Conduct CM\\nCM planning starts at a program’s or project’s incep -\\ntion. The CM office should carefully weigh the \\nvalue of prioritizing resources into CM tools or into CM surveillance of the contractors. Reviews by the Ce\\nnter Configuration Management Organization \\n(CMO) are warranted and will cost resources and time, but the correction of systemic CM problems before they erupt into losing configuration control are always preferable to explaining why incorrect or misidentified parts are causing major problems in the program/project.\\nConfiguration\\nVerificationConfiguration\\nStatus\\nAccountingConfiguration\\nChange\\nManagementConfiguration\\nPlanning andConfiguration\\nIdentificationManagementCONFIG URATION\\nMANAGEMENT\\nFIGURE 6.5 -2 Five Elements of Configuration Management1466.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nOne of the key inputs to preparing for CM imple -\\nmentation is a strategic plan for the project’s com -\\nplete CM process. This is typically contained in a \\nCM plan. See Appendix M  for an outline of a typical \\nCM plan.\\nThis plan has both internal and external uses:\\n• Internal:  It is used within the program/project \\noffice to guide, monitor, and measure the over -\\nall CM process. It describes all the CM activities \\nand the schedule for implementing those activities \\nwithin the program/project.\\n• External:  The CM plan is used to communicate \\nthe CM process to the contractors involved in \\nthe program/project. It establishes consistent CM \\nprocesses and working relationships.\\nThe CM plan may be a standalone document or it \\nmay be combined with other program/project plan -\\nning documents. It should describe the criteria for \\neach technical baseline creation, technical approvals, \\nand audits.\\n6.5.1.2.2  Identify Baseline to be Under Configuration \\nControl\\nConfiguration identification is the systematic process \\nof selecting, organizing, and stating the product attri -\\nbutes. Identification requires unique identifiers for a \\nproduct and its configuration documentation. The \\nCM activity associated with identification includes \\nselecting the Configuration Items (CIs), determin -\\ning the CIs’ associated configuration documentation, \\ndetermining the appropriate change control author -\\nity, issuing unique identifiers for both CIs and CI \\ndocumentation, releasing configuration documenta -\\ntion, and establishing configuration baselines.\\nNASA has four baselines, each of which defines a dis -\\ntinct phase in the evolution of a product design. The baseline identifies an agreed-to description of attri -\\nbutes of a CI at a point in time and provides a known \\nconfiguration to which changes are addressed. \\nBaselines are established by agreeing to (and docu -\\nmenting) the stated definition of a CI’s attributes. \\nThe approved “current” baseline defines the basis of \\nthe subsequent change. The system specification is \\ntypically finalized following the SRR. The functional \\nbaseline is established at the SDR and will usually \\ntransfer to NASA’s control at that time for contract -\\ning efforts. For in-house efforts, the baseline is set/\\ncontrolled by the NASA program/project.\\nThe four baselines (see FIGURE 6.5-3 ) normally con -\\ntrolled by the program, project, or Center are the \\nfollowing:\\n• Functional Baseline: The functional baseline \\nis the approved configuration documentation \\nthat describes a system’s or top-level CI’s perfor -\\nmance requirements (functional, interoperability, \\nand interface characteristics) and the verification \\nrequired to demonstrate the achievement of those \\nspecified characteristics. The functional baseline \\nis established at the SDR by the NASA program/\\nproject. The program/project will direct through \\ncontractual agreements, how the functional base -\\nlines are managed at the different functional lev -\\nels. (Levels 1–4)\\n• Allocated Baseline:  The allocated baseline is \\nthe approved performance-oriented configura -\\ntion documentation for a CI to be developed \\nthat describes the functional, performance, and \\ninterface characteristics that are allocated from \\na higher level requirements document or a CI \\nand the verification required to demonstrate \\nachievement of those specified characteristics. \\nThe allocated baseline extends the top-level per -\\nformance requirements of the functional baseline \\nto sufficient detail for defining the functional and 1476.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nperformance characteristics and for initiating \\ndetailed design for a CI. The allocated baseline is usually controlled by the design organization until all design requirements have been verified. The allocated baseline is typically established at the successful completion of the PDR. Prior to CDR, NASA normally reviews design output for conformance to design requirements through incremental deliveries of engineering data. NASA control of the allocated baseline occurs through review of the engineering deliveries as data items.\\n• Product Baseline:  The product baseline is the \\napproved technical documentation that describes Level 1 Technical\\nRequirements\\nFUNCTIO NAL \\nBASELINEConfiguration Information Non-Configuration \\nInformation (examples)Milestones\\nMajor Architecture\\nAspects of DesignComplete\\nALLO CATED\\nBASELINE\\nImplementation\\nAspects of DesignComplete \\nPROD UCT\\nBASELINE\\nRealization Aspects of \\nDesign Complete; Fabricationand Text Complete MDRConc ept\\nPartial analyses\\nand studies\\nProgram and\\nproject plans\\nDevelopmental\\ndata \\n \\nComplete\\nanalyses\\nDevelopmental\\ndata\\nTest plans \\n System Spec\\n \\nTest information\\nManuals\\nCertifications\\nProduct \\nstructure  SDRSRR\\nPDR\\nORRSARCDR\\n Segment Spec\\nPrime Item\\nDesign-to-Spec\\nEnd Item\\nDesign-to-Spec\\nEnd Item\\nBuild-to-SpecEnd Item\\nDesign-to-SpecEnd Item\\nDesign-to-SpecSegment Spec Segment Spec\\nPrime Item\\nDesign-to-SpecPrime Item\\nDesign-to-Spec\\nFIGURE 6.5 -3 Ev olution of Technical Baseline\\nthe configuration of a CI during the production, \\nfielding/deployment, and operational support phases of its life cycle. The established product baseline is controlled as described in the config -\\nuration management plan that was developed during Phase A. The product baseline is typically established at the completion of the CDR. The product baseline describes:\\n »Detailed physical or form, fit, and function characteristics of a CI;\\n »The selected functional characteristics desig -\\nnated for production acceptance testing; and\\n »The production acceptance test requirements.1486.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n• As-Deployed Baseline: The as-deployed baseline \\noccurs at the ORR. At this point, the design is consid -\\nered to be functional and ready for flight. All changes \\nwill have been incorporated into the documentation.\\n6.5.1.2.3  Manage Configuration Change Control\\nConfiguration change management is a process to manage approved designs and the implementation of approved changes. Configuration change man -\\nagement is achieved through the systematic proposal, justification, and evaluation of proposed changes followed by incorporation of approved changes and verification of implementation. Implementing con -\\nfiguration change management in a given program/project requires unique knowledge of the program/project objectives and requirements. The first step establishes a robust and well-disciplined internal NASA Configuration Control Board (CCB) system, which is chaired by someone with program/project change authority. CCB members represent the stake -\\nholders with authority to commit the team they rep -\\nresent. The second step creates configuration change management surveillance of the contractor’s activity. The CM office advises the NASA program or project manager to achieve a balanced configuration change management implementation that suits the unique program/project situation. See \\nFIGURE 6.5-4  for an \\nexample of a typical configuration change manage -\\nment control process.\\nOriginatorNORMAL CONFIGURATION CHANGE PROCESS\\nCM Function ReviewersResponsible\\nOrganization CCB Actionees\\n1.\\n• Prepare and\\n submit change request\\n• Propose reviewers3. 4. 2.\\n5.• Receive and log \\n request in system\\n• Screen change\\n package\\n• Release for reviewReview and \\nsubmit comments\\nSchedule CCB and \\nprepare agenda \\n(if needed)\\n8b.Collect, track, \\nand adjudicate\\ncomments (with\\nreviewers and\\noriginator)\\n6.\\nPrepare decision\\npackage\\n9.7. 8a.\\nComplete\\nassigned actions• Disposition\\n change request\\n• Assign action\\n items as needed\\n• Release CCB minutes\\n• Track actions\\n10.\\n• Verify release\\n package\\n• Release final\\n documentation• Close change\\n request• Execute approved  changes \\n• Finalize\\n documentation\\nFIGURE 6.5 -4 Ty pical Change Control Process1496.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nTYPES OF CONFIGURATION MANAGEMENT CHANGES\\n• Engineering Change: An engineering change is an iteration in the baseline. Changes can be major or \\nminor. They may or may not include a specification change. Changes affecting an external interface must be coordinated and approved by all stakeholders affected.\\n> A “major” change is a change to the baseline configuration documentation that has significant impact (i.e., requires retrofit of delivered products or affects the baseline specification, cost, safety, compatibility with interfacing products, or operator, or maintenance training).\\n> A ”minor” change corrects or modifies configuration documentation or processes without impact to the interchangeability of products or system elements in the system structure.\\n• Wa\\niver: A waiver is a documented agreement intentionally releasing a program or project from meeting a \\nrequirement. (Some Centers use deviations prior to Implementation and waivers during Implementation.) Authorized waivers do not constitute a change to a baseline.\\n6.5.1.2.4   Maintain the Status of Configuration \\nDocumentation\\nConfiguration Status Accounting (CSA) is the \\nrecording and reporting of configuration data nec -\\nessary to manage CIs effectively. An effective CSA system provides timely and accurate configuration information such as:\\n• Complete current and historical configuration documentation and unique identifiers.\\n• Status of proposed changes, deviations, and waiv -\\ners from initiation to implementation.\\n• Status and final disposition of identified discrep -\\nancies and actions identified during each config -\\nuration audit.\\nSome useful purposes of the CSA data include:\\n• An aid for proposed change evaluations, change decisions, investigations of design problems, war -\\nranties, and shelf-life calculations• Historical traceability\\n• Software trouble reporting\\n• Performance measurement data\\nThe following are critical functions or attributes to consider if designing or purchasing software to assist with the task of managing configuration.\\n• Ability to share data real time with internal and external stakeholders securely;\\n• Version control and comparison (track history of an object or product);\\n• Secure user checkout and check in;\\n• Tracking capabilities for gathering metrics (i.e., time, date, who, time in phases, etc.);\\n• Web based;\\n• Notification capability via e-mail;\\n• Integration with other databases or legacy systems;\\n• Compatible with required support contractors and/or suppliers (i.e., can accept data from a third party as required);\\n• Integration with drafting and modeling programs as required;1506.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n• Provide neutral format viewer for users;\\n• License agreement allows for multiple users within \\nan agreed-to number;\\n• Workflow and life cycle management;\\n• Limited customization;\\n• Migration support for software upgrades;\\n• User friendly;\\n• Consideration for users with limited access;\\n• Ability to attach standard format files from \\ndesktop\\n• Workflow capability (i.e., route a CI as required \\nbased on a specific set of criteria); and\\n• Capable of acting as the one and only source for \\nreleased information.\\n6.4.1.2.5 Conduct Configuration Audits\\nConfiguration verification is accomplished by \\ninspecting documents, products, and records; \\nreviewing procedures, processes, and systems of \\noperations to verify that the product has achieved its \\nrequired performance requirements and functional \\nattributes; and verifying that the product’s design is \\ndocumented. This is sometimes divided into func -\\ntional and physical configuration audits. (See Section \\n6.7.2.3 for more on technical reviews.)\\n6.4.1.2.6 Capture work Products\\nThese include the strategy and procedures for config -\\nuration management, the list of identified configu -\\nration items, descriptions of the configuration items, \\nchange requests, disposition of the requests, rational \\nfor dispositions, reports, and audit results.\\n6.5.1.3 Outputs\\nNPR 7120.5 defines a project’s life cycle in progressive \\nphases. Beginning with Pre-Phase A, these steps in \\nturn are grouped under the headings of Formulation \\nand Implementation. Approval is required to transition between these phases. Key Decision Points \\n(KDPs) define transitions between the phases. CM \\nplays an important role in determining whether a \\nKDP has been met. Major outputs of CM are:\\n• List of configuration items under control \\n(Configuration Status Accounting (CSA) reports): \\nThis output is the list of all the items, documents, \\nhardware, software, models, etc., that were iden -\\ntified as needing to be placed under configuration \\ncontrol. CSA reports are updated and maintained \\nthroughout the program and project life cycle.\\n• Current baselines: Baselines of the current configu -\\nrations of all items that are on the CM list are made \\navailable to all technical teams and stakeholders.\\n• CM reports: Periodic reports on the status of the \\nCM items should be available to all stakeholders \\non an agreed-to frequency and at key life cycle \\nreviews.\\n• Other CM work products: Other work products \\ninclude the strategy and procedures used for CM; \\ndescriptions, drawings and/or models of the CM \\nitems; change requests and their disposition and \\naccompanying rationale; reports; audit results as \\nwell as any corrective actions needed.\\n6.5.2  CM Guidance\\nRefer to Section 6.5.2 in the NASA Expanded \\nGuidance for Systems Engineering at https://nen.nasa.\\ngov/web/se/doc-repository  for additional guidance on:\\n• the impact of not doing CM,\\n• warning signs when you know you are in trouble, \\nand\\n• when it is acceptable to use redline drawings.1516.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n6.6 Technical Data Management\\nThe Technical Data Management Process is used \\nto plan for, acquire, access, manage, protect, and use data of a technical nature to support the total life cycle of a system. Data Management (DM) includes the development, deployment, operations and support, eventual retirement, and retention of appropriate technical, to include mission and science, data beyond system retirement as required by NPR 1441.1, NASA Records Retention Schedules.\\nDM is illustrated in \\nFIGURE 6.6-1 . Key aspects of DM \\nfor systems engineering include:\\nFrom all technical\\nprocesses and contractors To all technical processes\\nand  contractors \\nTo project and all\\ntechnical processes Technical Data\\nProduc ts to Be\\nMana ged \\nTechnical Data\\nRequests Technical Data\\nElectronic Exchange\\nFormats Form of Technical\\nData Products \\nDeli vered Technical\\nData  Prepare for technical data\\nmanagement implementation \\nCollect and store required\\ntechnical data  \\nProvide technical data to authorize\\nCapture work products from technical\\ndata management activitiesd\\npartiesMaintain stored technical data\\nFIGURE 6.6 -1 Te chnical Data Management Process• application of policies and procedures for data \\nidentification and control,\\n• timely and economical acquisition of technical data,\\n• assurance of the adequacy of data and its protection,\\n• facilitating access to and distribution of the data to the point of use,• analysis of data use,\\n• evaluation of data for future value to other pro -\\ngrams/projects, and\\n• process access to information written in legacy software.\\nThe Technical Data Management and Configuration Management Processes work side-by-side to ensure all information about the project is safe, known, and accessible. Changes to information under configu -\\nration control require a Change Request (CR) and are typically approved by a Configuration Control Board. Changes to information under Technical Data Management do not need a CR but still need to be managed by identifying who can make changes to each type of technical data.\\n6.6.1  Process Description\\nFIGURE 6.6-1 provides a typical flow diagram for the \\nTechnical Data Management Process and identifies typical inputs, outputs, and activities to consider in addressing technical data management.1526.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n6.6.1.1 Inputs\\nThe inputs for this process are:\\n• Technical data products to be managed: \\nTechnical data, regardless of the form or method \\nof recording and whether the data are generated \\nby the contractor or Government during the life \\ncycle of the system being developed. (Electronic \\ntechnical data should be stored with sufficient \\nmetadata to enable easy retrieval and sorting.)\\n• Technical data requests:  External or internal \\nrequests for any of the technical data generated by \\nthe program/project.\\n6.6.1.2 Process Activities\\nEach Center is responsible for policies and procedures \\nfor technical DM. NPR 7120.5 and NPR 7123.1 \\ndefine the need to manage data, but leave specifics to \\nthe individual Centers. However, NPR 7120.5 does \\nrequire that DM planning be provided as either a \\nsection in the program/project plan, CM plan, or as \\na separate document. The program or project man -\\nager is responsible for ensuring that the data required \\nare captured and stored, data integrity is maintained, \\nand data are disseminated as required.\\nOther NASA policies address the acquisition and \\nstorage of data and not just the technical data used in \\nthe life cycle of a system.\\n6.6.1.2.1  Prepare for T echnical Data Management \\nImplementation\\nThe recommended procedure is that the DM plan be \\na separate plan apart from the program/project plan. \\nDM issues are usually of sufficient magnitude to jus -\\ntify a separate plan. The plan should cover the follow -\\ning major DM topics:\\n• Identification/definition/management of data sets.• Control procedures—receipt, modification, review, \\nand approval.\\n• Guidance on how to access/search for data for users.\\n• Data exchange formats that promote data reuse \\nand help to ensure that data can be used consis -\\ntently throughout the system, family of systems, \\nor system of systems.\\n• Data rights and distribution limitations such as \\nexport-control Sensitive But Unclassified (SBU).\\n• Storage and maintenance of data, including mas -\\nter lists where documents and records are main -\\ntained and managed.\\nPrepare a technical data management strategy. This \\nstrategy can document how the program/project data \\nmanagement plan will be implemented by the tech -\\nnical effort or, in the absence of such a program-level \\nplan, be used as the basis for preparing a detailed \\ntechnical data management plan, including:\\n• Items of data that will be managed according to \\nprogram/project or organizational policy, agree -\\nments, or legislation;\\n• The data content and format;\\n• A framework for data flow within the program/\\nproject and to/from contractors including the lan -\\nguage(s) to be employed in technical effort infor -\\nmation exchanges;\\n• Technical data management responsibilities and \\nauthorities regarding the origin, generation, cap -\\nture, archiving, security, privacy, and disposal of \\ndata products;1536.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\n• Establishing the rights, obligations, and commit -\\nments regarding the retention of, transmission of, \\nand access to data items; and\\n• Relevant data storage, transformation, transmis -\\nsion, and presentation standards and conven -\\ntions to be used according to program/project or \\norganizational policy, agreements, or legislative \\nconstraints.\\n• Obtain strategy/plan commitment from relevant \\nstakeholders.\\n• Prepare procedures for implementing the techni -\\ncal data management strategy for the technical \\neffort and/or for implementing the activities of \\nthe technical data management plan.\\n• Establish a technical database(s) to use for tech -\\nnical data maintenance and storage or work with \\nthe program/project staff to arrange use of the \\nprogram/project database(s) for managing tech -\\nnical data.\\n• Establish data collection tools, as appropriate to \\nthe technical data management scope and avail -\\nable resources.\\n• Establish electronic data exchange interfaces in \\naccordance with international standards/agree -\\nments and applicable NASA standards.\\nTrain appropriate stakeholders and other technical \\npersonnel in the established technical data manage -\\nment strategy/plan, procedures, and data collection \\ntools, as applicable\\nData Identification/Definition\\nEach program/project determines data needs during \\nthe life cycle. Data types may be defined in standard \\ndocuments. Center and Agency directives sometimes \\nspecify content of documents and are appropriately used for in-house data preparation. The standard \\ndescription is modified to suit program/project-spe -\\ncific needs, and appropriate language is included \\nin SOWs to implement actions resulting from the \\ndata evaluation. “Data suppliers” may be contrac -\\ntors, academia, or the Government. Procurement of \\ndata from an outside supplier is a formal procure -\\nment action that requires a procurement document; \\nin-house requirements may be handled using a less \\nformal method. Below are the different types of data \\nthat might be utilized within a program/project:\\n• Data\\n »“Data” is defined in general as “recorded \\ninformation regardless of the form or method \\nof recording.” However, the terms “data” and \\n“information” are frequently used inter -\\nchangeably. To be more precise, data gener -\\nally should be processed in some manner to \\ngenerate useful, actionable information.\\n »“Data,” as used in SE DM, includes technical \\ndata; computer software documentation; and \\nrepresentation of facts, numbers, or data of \\nany nature that can be communicated, stored, \\nand processed to form information required \\nby a contract or agreement to be delivered to, \\nor accessed by, the Government.\\n »Data include that associated with system \\ndevelopment, modeling and simulation used \\nin development or test, test and evaluation, \\ninstallation, parts, spares, repairs, usage \\ndata required for product sustainability, and \\nsource and/or supplier data.\\n »Data specifically not included in Technical \\nData Management would be data relating \\nto general NASA workforce operations \\ninformation, communications information \\n(except where related to a specific require -\\nment), financial transactions, personnel data, 1546.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\ntransactional data, and other data of a purely \\nbusiness nature.\\n• Data Call: Solicitation from Government stake -\\nholders (specifically Integrated Product Team \\n(IPT) leads and functional managers) identi -\\nfies and justifies their data requirements from \\na proposed contracted procurement. Since \\ndata provided by contractors have a cost to the \\nGovernment, a data call (or an equivalent activ -\\nity) is a common control mechanism used to \\nensure that the requested data are truly needed. \\nIf approved by the data call, a description of each \\ndata item needed is then developed and placed \\non contract.\\n• Information:  Information is generally consid -\\nered as processed data. The form of the processed \\ndata is dependent on the documentation, report, \\nreview formats, or templates that are applicable.\\n• Technical Data Package:  A technical data pack -\\nage is a technical description of an item adequate \\nfor supporting an acquisition strategy, produc -\\ntion, engineering, and logistics support. The \\npackage defines the required design configuration \\nand procedures to ensure adequacy of item per -\\nformance. It consists of all applicable items such \\nas drawings, associated lists, specifications, stan -\\ndards, performance requirements, quality assur -\\nance provisions, and packaging details.\\n• Technical Data Management System:  The strat -\\negies, plans, procedures, tools, people, data for -\\nmats, data exchange rules, databases, and other \\nentities and descriptions required to manage the \\ntechnical data of a program/project.\\n6.6.1.2.2 Collect and Store Data\\nSubsequent activities collect, store, and maintain \\ntechnical data and provide it to authorized par -\\nties as required. Some considerations that impact these activities for implementing Technical Data \\nManagement include:\\n• Requirements relating to the flow/delivery of data \\nto or from a contractor should be specified in the \\ntechnical data management plan and included in \\nthe Request for Proposal (RFP) and contractor \\nagreement.\\n• NASA should not impose changes on existing \\ncontractor data management systems unless the \\nprogram/project technical data management \\nrequirements, including data exchange require -\\nments, cannot otherwise be met.\\n• Responsibility for data inputs into the technical \\ndata management system lies solely with the orig -\\ninator or generator of the data.\\n• The availability/access of technical data lies with \\nthe author, originator, or generator of the data in \\nconjunction with the manager of the technical \\ndata management system.\\n• The established availability/access description and \\nlist should be baselined and placed under config -\\nuration control.\\n• For new programs/projects, a digital generation \\nand delivery medium is desired. Existing pro -\\ngrams/projects should weigh the cost/benefit \\ntrades of digitizing hard copy data.\\nTABLE 6.6-1  defines the tasks required to capture \\ntechnical data.\\n6.6.1.2.3 Provide Data to Authorized Parties\\nAll data deliverables should include distribution state -\\nments and procedures to protect all data that contain \\ncritical technology information, as well as to ensure \\nthat limited distribution data, intellectual property \\ndata, or proprietary data are properly handled during 1556.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nsystems engineering activities. This injunction applies \\nwhether the data are hard copy or digital.\\nAs part of overall asset protection planning, NASA \\nhas established special procedures for the protection of Critical Program Information (CPI). CPI may include components; engineering, design, or manu -\\nfacturing processes; technologies; system capabilities, and vulnerabilities; and any other information that gives a system its distinctive operational capability.\\nCPI protection should be a key consideration for the \\ntechnical data management effort and is part of the asset protection planning process.\\n6.6.1.3  Outputs\\nOutputs include timely, secure availability of needed data in various representations to those authorized to receive it. Major outputs from the Technical Data Management Process include the following (see \\nFIGURE 6.6-1 ):\\n• Form of Technical Data Products:  How each \\ntype of data is held and stored such as textual, graphic, video, etc.\\n• Technical Data Electronic Exchange Formats:  \\nDescription and perhaps templates, models or other ways to capture the formats used for the var\\nious data exchanges.\\n• Delivered Technical Data:  The data that were \\ndelivered to the requester.DATA COLLECTION CHECKLIST\\n• Have the frequency of collection and the points in the technical and technical management processes \\nwhe\\nn data inputs will be available been determined?\\n• Has th\\ne timeline that is required to move data from the point of origin to storage repositories or \\nstakeholders been established?\\n• Who is r\\nesponsible for the input of the data?\\n• Who is r\\nesponsible for data storage, retrieval, and security?\\n• Have n\\necessary supporting tools been developed or acquired?\\nOth\\ner work products generated as part of this process \\ninclude the strategy and procedures used for techni -\\ncal data management, request dispositions, decisions, \\nand assumptions.\\n6.6.2   Technical Data Management \\nGuidance\\nRefer to Section 6.6.2 in the NASA Expanded \\nGuidance for Systems Engineering at https://nen.nasa.\\ngov/web/se/doc-repository  for additional guidance on:\\n• data security and\\n• ITAR.\\n6.7 Technical Assessment\\nTechnical assessment is the crosscutting process used to help monitor technical progress of a program/proj -\\nect through periodic technical reviews and through monitoring of technical indicators such as MOEs, MOPs, Key Performance Parameters (KPPs), and TPMs. The reviews and metrics also provide status 1566.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nTABLE 6.6 -1 Technical Data Tasks\\nDescription Tasks Expected Outcomes\\nTechnical \\ndata captureCollect and store inputs and technical effort outcomes from the technical and technical management processes, including:\\n• results from technical assessments;\\n• descriptions of methods, tools, and metrics used;\\n• recommendations, decisions, assumptions, and impacts of technical efforts and decisions;\\n• lessons learned;\\n• deviations from plan;\\n• anomalies and out-of-tolerances relative to requirements; and\\n• other data for tracking requirements.\\nPerform data integrity checks on collected data to ensure compliance with content and format as well as technical data checks to ensure there are no errors in specifying or recording the data.\\nReport integrity check anomalies or variances to the authors or generators of the data for \\ncorrection.\\nPrioritize, review, and update data collection and storage procedures as part of regularly \\nscheduled maintenance.Sharable data needed to perform and control the technical and technical management processes is collected and stored.\\nStored data inventory.\\nTechnical \\ndata maintenanceImplement technical management roles and responsibilities with technical data products received.\\nManage database(s) to ensure that collected data have proper quality and integrity; and \\nare properly retained, secure, and available to those with access authority.\\nPeriodically review technical data management activities to ensure consistency and \\nidentify anomalies and variances.\\nReview stored data to ensure completeness, integrity, validity, availability, accuracy, \\ncurrency, and traceability.\\nPerform technical data maintenance, as required.\\nIdentify and document significant issues, their impacts, and changes made to technical \\ndata to correct issues and mitigate impacts.\\nMaintain, control, and prevent the stored data from being used inappropriately.Store data in a manner that enables easy and speedy retrieval.Maintain stored data in a manner that protects the technical data against foreseeable \\nhazards, e.g., fire, flood, earthquake, etc.Records of technical data maintenance.\\nTechnical effort data, \\nincluding captured work \\nproducts, contractor-delivered documents, and acquirer-provided documents are controlled and maintained.\\nStatus of data stored is \\nmaintained to include: version description, timeline, and security classification.\\nTechnical data/information distributionMaintain an information library or reference index to provide technical data availability and access instructions.\\nReceive and evaluate requests to determine data requirements and delivery instructions.Process special requests for technical effort data or information according to established \\nprocedures for handling such requests.\\nEnsure that required and requested data are appropriately distributed to satisfy the \\nneeds of the acquirer and requesters in accordance with the agreement, program/project \\ndirectives, and technical data management plans and procedures.\\nEnsure that electronic access rules are followed before database access is allowed or any \\nrequested data are electronically released/transferred to the requester.\\nProvide proof of correctness, reliability, and security of technical data provided to internal \\nand external recipients.Access information (e.g., available data, access means, security procedures, time period for availability, and personnel cleared for access) is readily available.\\nTechnical data are provided \\nto authorize requesters in the appropriate format, with the appropriate content, and by a secure mode of delivery, as applicable.\\n(continued)1576.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nDescription Tasks Expected Outcomes\\nData \\nmanagement \\nsystem \\nmaintenanceImplement safeguards to ensure protection of the technical database and of en route  \\ntechnical data from unauthorized access or intrusion.\\nEstablish proof of coherence of the overall technical dataset to facilitate effective and \\nefficient use.\\nMaintain, as applicable, backups of each technical database.\\nEvaluate the technical data management system to identify collection and storage \\nperformance issues and problems; satisfaction of data users; risks associated with \\ndelayed or corrupted data, unauthorized access, or survivability of information from \\nhazards such as fire, flood, earthquake, etc.\\nReview systematically the technical data management system, including the database \\ncapacity, to determine its appropriateness for successive phases of the Defense \\nAcquisition Framework.\\nRecommend improvements for discovered risks and problems:\\nHandle risks identified as part of technical risk management.\\nControl recommended changes through established program/project change management \\nactivities.Current technical data \\nmanagement system.\\nTechnical data are \\nappropriately and regularly \\nbacked up to prevent data \\nloss.\\ninformation to support assessing system design, prod -\\nuct realization, and technical management decisions.\\nNASA has multiple review cycle processes for both \\nspace flight programs and projects (see NPR 7120.5), \\nand research and technology programs and projects. \\n(See NPR 7120.8, NASA Research and Technology \\nProgram and Project Management Requirements.) \\nThese different review cycles all support the same \\nbasic goals but with differing formats and formalities \\nbased on the particular program or project needs.\\n6.7.1 Process Description\\nFIGURE 6.7-1  provides a typical flow diagram for the \\nTechnical Assessment Process and identifies typical \\ninputs, outputs, and activities to consider in address -\\ning technical assessment. Technical assessment is \\nfocused on providing a periodic assessment of the \\nprogram/project’s technical and programmatic sta -\\ntus and health at key points in the life cycle. There \\nare 6 criteria considered in this assessment process: \\nalignment with and contribution to Agency strategic \\ngoals; adequacy of management approach; adequacy \\nof technical approach; adequacy of the integrated \\ncost and schedule estimates and funding strategy; adequacy and availability of non-budgetary resources, \\nand adequacy of the risk management approach.\\n6.7.1.1 Inputs\\nTypical inputs needed for the Technical Assessment \\nProcess would include the following:\\n• Technical Plans: These are the planning doc -\\numents that will outline the technical reviews/\\nassessment process as well as identify the techni -\\ncal product/process measures that will be tracked \\nand assessed to determine technical progress. \\nExamples of these plans are the program (or \\nproject) plan, SEMP (if applicable), review plans \\n(which may be part of the program or project \\nplan), ILS plan, and EVM plan (if applicable). \\nThese plans contain the information and descrip -\\ntions of the program/project’s alignment with \\nand contribution to Agency strategic goals, its \\nmanagement approach, its technical approach, its \\nintegrated cost and schedule, its budget, resource \\nallocations, and its risk management approach.\\n• Technical Process and Product Measures: These \\nare the identified technical measures that will be 1586.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nassessed or tracked to determine technical prog -\\nress. These measures are also referred to as MOEs, \\nMOPs, KPPs, and TPMs. (See Section 6.7.2.6.2 in the NASA Expanded Guidance for Systems Engineering at \\nhttps://nen.nasa.gov/web/se/doc-re -\\npository .) They provide indications of the program/\\nproject’s performance in key management, techni -\\ncal, cost (budget), schedule, and risk areas.\\n• Reporting Requirements:  These are the require -\\nments on the methodology in which the status of the technical measures will be reported with regard to management, technical cost (budget), sched -\\nule, and risk. The requirements apply internally to the program/project and are used externally by the Centers and Mission Directorates to assess the performance of the program or project. The meth -\\nodology and tools used for reporting the status \\nwill be established on a project-by-project  basis.To Decision Analysis Process\\nAnalysis Support \\nRequests\\nWork Products from\\nTechnical Assessment To Technical Data\\nManagement Process To Technical Planning , \\nRequirements , and  \\nInterface Management Processes  \\nCorrective Action\\nRecommendationsTechnical Cost and Schedule\\nStatus Reports\\nProduct Measurements\\nDecision Support\\nRecommendations and\\nImp acts From Product Verification  and\\nProduct Validation Processes \\nFrom Decision Analysis Process Product and Process\\nMeasures \\nTechnical PlansFrom Technical\\nPlanning Process\\nFrom project\\nRisk Reporting\\nRequirements\\nTechnical Review\\nReports To project and Technical Data\\nManagement Process Assessment Results/\\nFindings To Technical Planning , \\nRisk Management, and Requirements\\nManagement Processes\\nPrepare strategy for conducting\\ntechnical assessments\\nAssess technical work product ivity \\n(measure progress against plans)\\nConduct horizontal and vertical progress\\ntechnical reviewsAssess technical produc t qua lity\\n(measure progress against requirements)\\nCapture work products from technical\\nassessment activities\\nFIGURE 6.7 -1 Tech nical Assessment Process\\n6.7.1.2  Process Activities\\n6.7.1.2.1   Prepare Strategy for Conducting T echnical \\nAssessments\\nAs outlined in FIGURE 6.7-1 , the technical plans pro -\\nvide the initial inputs into the Technical Assessment \\nProcess. These documents outline the technical reviews/assessment approach as well as identify the technical measures that will be tracked and assessed to determine technical progress. An important part of the technical planning is determining what is needed in time, resources, and performance to com -\\nplete a system that meets desired goals and objectives. Project managers need visibility into the progress of those plans in order to exercise proper management control. Typical activities in determining progress against the identified technical measures include sta -\\ntus reporting and assessing the data. Status report -\\ning will identify where the project stands with regard to a particular technical measure. Assessing will 1596.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nanalytically convert the output of the status report -\\ning into a more useful form from which trends can \\nbe determined and variances from expected results \\ncan be understood. Results of the assessment activ -\\nity then feed into the Decision Analysis Process (see \\nSection 6.8 ) where potential corrective action may be \\nnecessary.\\nThese activities together form the feedback loop \\ndepicted in FIGURE 6.7-2 .\\n(Re-)\\nPlanningStatus\\nReportingAssessingDecision-\\nmakingStatus Not OK\\nStatus OKExecute\\nFIGURE 6.7 -2  Planning and Status  \\nReporting Feedback Loop\\nThis loop takes place on a continual basis throughout the project life cycle. This loop is applicable at each level of the project hierarchy. Planning data, status \\nreporting data, and assessments flow up the hierar -\\nchy with appropriate aggregation at each level; deci -\\nsions cause actions to be taken down the hierarchy. \\nManagers at each level determine (consistent with policies established at the next higher level of the project hierarchy) how often and in what form sta -\\ntus data should be reported and assessments should be made. In establishing these status reporting and assessment requirements, some principles of good practice are as follows:\\n• Use an agreed-upon set of well-defined technical measures. (See Section 6.7.2.6.2 in the NASA Expanded Guidance for Systems Engineering at \\nhttps://nen.nasa.gov/web/se/doc-repository .)\\n• Report these technical measures in a consistent format at all project levels.• Maintain historical data for both trend identifica -\\ntion and cross-project analyses.\\n• Encourage a logical process of rolling up technical measures (e.g., use the WBS or PBS for project progress status).\\n• Support assessments with quantitative risk measures.\\n• Summarize the condition of the project by using color-coded (red, yellow, and green) alert zones for all technical measures.\\n6.7.1.2.2   Assess T echnical Work Productivity and Product \\nQuality and Conduct Progress Reviews\\nRegular, periodic (e.g., monthly) tracking of the \\ntechnical measures is recommended, although some measures should be tracked more often when there is \\nrapid change or cause for concern. Key reviews, such \\nas PDRs and CDRs, or status reviews are points at which technical measures and their trends should be carefully scrutinized for early warning signs of potential problems. Should there be indications that existing trends, if allowed to continue, will yield \\nan unfavorable outcome, corrective action should \\nbegin as soon as practical. Section 6.7.2.6.1 in the NASA Expanded Guidance for Systems Engineering at \\nhttps://nen.nasa.gov/web/se/doc-repository  pro-\\nvides additional information on status reporting and assessment techniques for costs and schedules \\n(including EVM), technical performance, and sys -\\ntems engineering process metrics.\\nThe measures are predominantly assessed during \\nthe program and project technical reviews. Typical activities performed for technical reviews include \\n(1) identifying, planning, and conducting phase-to-\\nphase technical reviews; (2) establishing each review’s purpose, objective, and entry and success criteria; (3) 1606.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nestablishing the makeup of the review team; and (4) \\nidentifying and resolving action items resulting from \\nthe review. Section 6.7.2.3 summarizes the types of \\ntechnical reviews typically conducted on a program/\\nproject and the role of these reviews in supporting \\nmanagement decision processes. This section address \\nthe types of technical reviews typically conducted \\nfor both space flight and research and technology \\nprograms/projects and the role of these reviews in \\nsupporting management decision processes. It also \\nidentifies some general principles for holding reviews, \\nbut leaves explicit direction for executing a review to \\nthe program/project team to define.\\nThe process of executing technical assessment has \\nclose relationships to other areas, such as risk man -\\nagement, decision analysis, and technical planning. \\nThese areas may provide input into the Technical \\nAssessment Process or be the benefactor of outputs \\nfrom the process.\\nTABLE 6.7-1  provides a summary of the types of reviews \\nfor a space flight project, their purpose, and timing.\\n6.7.1.2.3 Capture Work Products\\nThe work products generated during these activities \\nshould be captured along with key decisions made, \\nsupporting decision rationale and assumptions, \\nand lessons learned in performing the Technical \\nAssessment Process.\\n6.7.1.3 Outputs\\nTypical outputs of the Technical Assessment Process \\nwould include the following:\\n• Assessment Results, Findings, and \\nRecommendations:  This is the collective data \\non the established measures from which trends \\ncan be determined and variances from expected \\nresults can be understood. Results then feed into \\nthe Decision Analysis Process where corrective \\naction may be necessary.• Technical Review Reports/Minutes: This is the \\ncollective information coming out of each review \\nthat captures the results, recommendations, and \\nactions with regard to meeting the review’s suc -\\ncess criteria.\\n• Other Work Products:  These would include strat -\\negies and procedures for technical assessment, key \\ndecisions and associated rationale, assumptions, \\nand lessons learned.\\n6.7.2  Technical Assessment Guidance\\nRefer to Section 6.7.2 in the NASA Expanded \\nGuidance for Systems Engineering at https://nen.\\nnasa.gov/web/se/doc-repository  for additional guid -\\nance on:\\n• the basis of technical reviews,\\n• audits,\\n• Key Decision Points,\\n• required technical reviews for space flight projects,\\n• other reviews,\\n• status reporting and assessment (including MOEs, \\nMOPs, KPPs, TPMs, EVM and other metrics,\\nAdditional information is also available in NASA/\\nSP-2014-3705, NASA Space Flight Program and \\nProject Management Handbook.\\n6.8 Decision Analysis\\nThe purpose of this section is to provide an over -\\nview of the Decision Analysis Process, highlighting \\nselected tools and methodologies. Decision Analysis \\nis a framework within which analyses of diverse types \\nare applied to the formulation and characterization \\nof decision alternatives that best implement the deci -\\nsion-maker’s priorities given the decision-maker’s \\nstate of knowledge.1616.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nTABLE 6.7-1  Purpose and Results for Life-Cycle Reviews for Spaceflight Projects\\nName of \\nReviewPurpose Timing Entrance/Success CriteriaResults of Review\\nMission Concept \\nReview (MCR)The MCR will affirm the mission need and evaluates the proposed objectives and the concept for meeting those objectives.The MCR should be completed prior to entering the concept development phase (Phase A)The MCR entrance and success criteria are defined in Table G-3 of NPR 7123.1.A successful MCR supports the determination that the proposed mission meets the customer need and has sufficient quality and merit to support a field Center management decision to propose further study to the cognizant NASA program Associate Administrator as a candidate Phase A effort.\\nSystem Requirements Review (SRR)The SRR evaluates the functional and performance requirements defined for the system and the preliminary program or project plan and ensures that the requirements and selected concept will satisfy the mission.The SRR is conducted during the concept development phase (Phase A) and before conducting the SDR or MDR.The SRR entrance and success criteria for a program are defined in Table G-1 of NPR 7123.1. The SRR entrance and success criteria for projects and single-project programs are defined in Table G-4 of NPR 7123.1.Successful completion of the SRR freezes program/project requirements and leads to a formal decision by the cognizant program Associate Administrator to proceed with proposal request preparations for project implementation\\nMission Definition \\nReview (MDR)/  \\nSystem \\nDefinition Review (SDR)Sometimes called the MDR by robotic projects and SDR for \\nhuman flight projects, this review \\nevaluates whether the proposed architecture is responsive to the functional and performance requirements and that the requirements have been allocated to all functional elements of the mission/system.The MDR/SDR is conducted \\nduring the concept \\ndevelopment phase (Phase A) prior to KDP B and the start of preliminary design.The MDR/SDR entrance and success criteria for \\na program are defined in \\nTable G-2 of NPR 7123.1. The MDR/SDR entrance and success criteria for projects and single-project programs are defined in Table G-5 of NPR 7123.1.A successful MDR/SDR supports the decision to further develop the \\nsystem architecture/design and any \\ntechnology needed to accomplish the mission. The results reinforce the mission/system’s merit and provide a basis for the system acquisition strategy. As a result of successful completion, the mission/system and its operation are well enough understood to warrant design and acquisition of the end items.\\nPreliminary Design Review (PDR)The PDR demonstrates that the preliminary design meets all system requirements with \\nacceptable risk and within the \\ncost and schedule constraints and establishes the basis for proceeding with detailed design. It shows that the correct design options have been selected, interfaces have been identified, and verification methods have been described. The PDR should address and resolve critical, system-wide issues and show that work can begin on detailed design.PDR occurs near the completion of the preliminary \\ndesign phase \\n(Phase B) as the \\nlast review in the Formulation Phase.The entrance and success criteria for the PDR are defined in Table G-6 of NPR 7123.1.As a result of successful completion of the PDR, the design-to baseline is approved. A successful review result also authorizes the project to proceed into the Implementation Phase and toward final design.\\n(continued)1626.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nName of \\nReviewPurpose Timing Entrance/Success CriteriaResults of Review\\nCritical Design \\nReview (CDR)The CDR demonstrates that the maturity of the design is appropriate to support proceeding with full scale fabrication, assembly, integration, and test. CDR determines if the technical effort is on track to complete the system development, \\nmeeting mission performance \\nrequirements within the identified cost and schedule constraints.CDR occurs during the final design phase (Phase C).The entrance and success criteria for the CDR are defined in Table G-7 of NPR 7123.1.As a result of successful completion of the CDR, the build-to baseline, production, and verification plans are approved. A successful review result also authorizes coding of deliverable software (according to the build-to baseline and coding standards presented in the review) and \\nsystem qualification testing and \\nintegration. All open issues should be resolved with closure actions and schedules.\\nProduction Readiness Review (PRR)A PRR is held for projects developing or acquiring multiple or similar systems greater than three or as determined by the project. The PRR determines the readiness of the system developers to efficiently produce the required number of systems. It ensures that the production plans; fabrication, assembly, and \\nintegration-enabling products; \\nand personnel are in place and ready to begin production.PRR occurs during the final design phase (Phase C).The entrance and success criteria for the PRR are defined in Table G-8 of NPR 7123.1.As a result of successful completion of the PRR, the final production build-to baseline, production, and verification plans are approved. Approved drawings are released and authorized for production. A successful review result also authorizes coding of deliverable software (according to the build-to baseline and coding \\nstandards presented in the review) \\nand system qualification testing and integration. All open issues should be resolved with closure actions and schedules.\\nSystem Integration Review (SIR)An SIR ensures segments, components, and subsystems are on schedule to be integrated into the system. Integration facilities, support personnel, and integration plans and procedures are on schedule to support integration.SIR occurs at the end of the final design phase (Phase C) and before the systems assembly, integration, and test phase (Phase D) begins.The entrance and success criteria for the SIR are defined in Table G-9 of NPR 7123.1.As a result of successful completion of the SIR, the final as-built baseline and verification plans are approved. Approved drawings are released and authorized to support integration. All open issues should be resolved with closure actions and schedules. The subsystems/systems \\nintegration procedures, ground \\nsupport equipment, facilities, logistical needs, and support personnel are planned for and are ready to support integration.\\nSystem Acceptance Review (SAR)The SAR verifies the completeness of the specific end products in relation to their expected maturity level and assesses compliance to stakeholder expectations. It also ensures that the system has sufficient technical maturity to authorize its shipment to the \\ndesignated operational facility or \\nlaunch site.The entrance and success criteria for the SAR are defined in Table G-11 of \\nNPR 7123.1.As a result of successful completion of the SAR, the system is accepted by the buyer, and authorization is given to ship the hardware to the launch site or operational facility and to install software and hardware for operational use.\\n(continued)1636.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nName of \\nReviewPurpose Timing Entrance/Success CriteriaResults of Review\\nOperational \\nReadiness Review (ORR)The ORR examines the actual system characteristics and procedures used in the system or end product’s operation. It ensures that all system and support (flight and ground) hardware, software, personnel, procedures, and user documentation accurately reflect \\nthe deployed state of the system.The entrance and success \\ncriteria for the ORR are defined in Table G-12 of NPR 7123.1.As a result of successful ORR completion, the system is ready to assume normal operations.\\nFlight Readiness Review (FRR)The FRR examines tests, demonstrations, analyses, and audits that determine the system’s readiness for a safe and successful flight or launch and for subsequent flight operations. It also ensures that all flight and ground hardware, software, personnel, and procedures are operationally ready.The entrance and success criteria for the FRR are defined in Table G-13 of NPR 7123.1.As a result of successful FRR completion, technical and procedural maturity exists for system launch and flight authorization and, in some cases, initiation of system operations.\\nPost-Launch Assessment Review (PLAR)A PLAR is a post-deployment evaluation of the readiness of the spacecraft systems to proceed with full, routine operations. The review evaluates the status, performance, and capabilities of the project evident from the flight operations experience since launch. This can also mean assessing readiness to transfer responsibility from the development organization to the operations organization. The review also evaluates the status of the project plans and the capability to conduct the mission with emphasis on near-term operations and mission-critical events.This review is typically held after the early flight operations and initial checkout.The entrance and success criteria for the PLAR are defined in Table G-14 of NPR 7123.1.As a result of successful PLAR completion, the system is ready to assume in-space operations.\\nCritical Event Readiness Review (CERR)A CERR confirms the project’s readiness to execute the mission’s critical activities during flight operation. These include orbital insertion, rendezvous and docking, re-entry, scientific observations/encounters, etc.The CERR entrance and success criteria for a program are defined in Table G-15 of NPR 7123.1.As a result of successful CER completion, the system is ready to assume (or resume) in-space operations.\\nPost-Flight Assessment Review (PFAR)The PFAR evaluates the activities from the flight after recovery. The review identifies all anomalies that occurred during the flight and \\nmission and determines the actions \\nnecessary to mitigate or resolve the anomalies for future flights.The entrance and success criteria for the PFAR are defined in Table G-16 of NPR 7123.1.As a result of successful PFAR completion, the report documenting flight performance and recommendations for future \\nmissions is complete and all \\nanomalies have been documented and dispositioned.\\n(continued)1646.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nName of \\nReviewPurpose Timing Entrance/Success CriteriaResults of Review\\nDecommissioning \\nReview (DR)The DR confirms the decision to terminate or decommission the system and assesses the readiness of the system for the safe decommissioning and disposal of system assets.The DR is normally held near the end of routine mission operations upon accomplishment of planned mission objectives. It may be advanced if some unplanned event gives rise to a need to prematurely terminate the mission, or delayed if operational life is extended to permit additional investigations.The entrance and success criteria for the DR are defined in Table G-17 of NPR 7123.1.A successful DR completion ensures that the decommissioning and disposal of system items and processes are appropriate and effective.\\nDisposal Readiness Review (DRR)A DRR confirms the readiness for the final disposal of the system assets.The DRR is held as major assets are ready for final disposal.The DRR entrance and success criteria for a program are defined in Table G-18 of NPR 7123.1.A successful DRR completion ensures that the disposal of system items and processes are appropriate and effective.\\nThe Decision Analysis Process is used in support of \\ndecision making bodies to help evaluate technical, cost, and schedule issues, alternatives, and their uncertain -\\nties. Decision models have the capacity for accepting and quantifying human subjective inputs: judgments of experts and preferences of decision makers.\\nThe outputs from this process support the decision \\nauthority’s difficult task of deciding among compet -\\ning alternatives without complete knowledge; there -\\nfore, it is critical to understand and document the assumptions and limitation of any tool or method -\\nology and integrate them with other factors when deciding among viable options.\\n6.8.1  Process Description\\nA typical process flow diagram is provided in FIGURE \\n6.8-1, including inputs, activities, and outputs. The \\nfirst step in the process is understanding the deci -\\nsion to be made in the context of the system/mission. Understanding the decision needed requires knowl -\\nedge of the intended outcome in terms of technical performance, cost, and schedule. For an issue that follows the decision analysis process, the definition of the decision criteria or the measures that are import -\\nant to characterize the options for making a decision should be the next step in the process. With this defined, a set of alternative solutions can be defined for evaluation. These solutions should cover the full decision space as defined by the understanding of the decision and definition of the decision criteria. The need for specific decision analysis tools (defined in Section 6.8.3 in the NASA Expanded Guidance for Systems Engineering at \\nhttps://nen.nasa.gov/\\nweb/se/doc-repository ) can then be determined and \\nemployed to support the formulation of a solution. Following completion of the analysis, a description of how each alternative compares with the decision criteria can be captured for submission to the deci -\\nsion-making body or authority. A recommendation is typically provided from the decision analysis, but is not always required depending on the discretion of the decision-making body. A decision analysis report should be generated including: decision to be made, 1656.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\ndecision criteria, alternatives, evaluation methods, \\nevaluation process and results, recommendation, and final decision.From all technical\\nprocesses \\nWork Products From\\nDecision AnalysisTo Technical Data\\nManagement Process Decision Need,\\nAlternatives, Issues, or \\nProblems and\\nSupporting Data \\nTo Technical\\nAssessment Process\\nDecision Support  \\nRecommendations and\\nImpactsTo all technical processes\\nAlternative Selection\\nRecommendations and\\nImpactsFrom Technical\\nAssessment Process \\nAnalysis Support\\nRequestsEstablish guidelines to determine\\nwhich technical issues are subject to a\\nformal analysis/evaluation process\\nIdentify alternative solutions to \\naddress decision issues\\nSelect evaluation methods and toolsfor evaluating\\nalternative solutions\\nEvaluat e alternative solutions with \\nthe established criteria and\\nselected methods\\nSelect recommended solutions from \\nthe alternatives based on the\\nevaluation criteria\\nReport analysis results with \\nrecommendations, impacts, and\\ncorrective actions  \\nCaptu re wor k products from \\ndecision analysis activities\\nFIGURE 6.8 -1 Deci sion Analysis Process\\nDe\\ncision analysis covers a wide range of timeframes. \\nComplex, strategic decisions may require weeks or months to fully assess all alternatives and potential outcomes. Decisions can also be made in hours or in a few days, especially for smaller projects or activi -\\nties. Decisions are also made in emergency situations. Under such conditions, process steps, procedures, and meetings may be combined. In these cases, the focus of the systems engineer is on obtaining accurate deci -\\nsions quickly. Once the decision is made, the report can be generated. The report is usually generated in an ongoing fashion during the decision analysis pro -\\nce\\nss. However, for quick or emergency decisions, the \\nreport information may be captured after the deci -\\nsion has been made.\\nNot all decisions require the same amount of analysis \\neffort. The level and rigor required in a specific situa -\\ntion depend essentially on how clear-cut the decision is. If there is enough uncertainty in the alternatives’ performance that the decision might change if that uncertainty were to be reduced, then consideration needs to be given to reducing that uncertainty. A robust decision is one that is based on sufficient tech -\\nnical evidence and characterization of uncertainties to determine that the selected alternative best reflects 1666.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\ndecision-maker preferences and values given the state \\nof knowledge at the time of the decision. This is sug -\\ngested in FIGURE 6.8-2 .\\nNote that in FIGURE 6.8-2 , the phrase “net beneficial” \\nin the decision node “Net beneficial to reduce uncer -\\ntainty?” is meant to imply consideration of all factors, including whether the project can afford any sched -\\nule slip that might be caused by additional informa -\\ntion collection and additional analysis.Decision \\nAlternatives for \\nAnalysisExamples of Decisions\\n• Architecture A vs. Architecture V vs. Archicture C\\n• Extending the life of existing systems• Contingency Plan A vs. Contingency Plan B• Changing requirements• Launch or no launch• Making changes to existing systems•  Responding to operational occurrences in \\nreal time\\n• Technology A vs. Technology B• Prioritization\\nPreliminary Risk \\n& Performance \\nMeasurement \\nResults\\nNoYes\\nYesNo\\nIteration\\nAdditional Uncertainty Reduction Necessary per StakeholdersRisk Analysis \\nTechniques\\nSpectrum \\nof Available \\nTechniquesQualitative \\nTechniques\\nQuantitative \\nTechniquesRisk & \\nPerformance \\nMeasurement \\nResults\\nIdentifyAnalyzeDeliberation and \\nRecommending a \\nDecision Alternative\\nIs the ranking/\\ncomparison robust?Scoping & \\nDetermination of \\nMethods to  \\nBe Used\\nNet beneficial to \\nreduce uncertainty?AnalyzeIdentify\\nFIGURE 6.8-2  Ris k Analysis of Decision Alternatives\\n6.8.1.1  Inputs\\nThe technical, cost, and schedule inputs need to be \\ncomprehensively understood as part of the general decision definition. Based on this understanding, decision making can be addressed from a simple meeting to a formal analytical analysis. As illustrated in \\nFIGURE 6.8-2 , many decisions do not require exten -\\nsive analysis and can be readily made with clear input from the responsible engineering and programmatic disciplines. Complex decisions may require more for -\\nmal decision analysis when contributing factors have complicated or not well defined relationships. Due to t\\nhis complexity, formal decision analysis has the \\npotential to consume significant resources and time. Typically, its application to a specific decision is war -\\nranted only when some of the following conditions are met:\\n• Complexity:  The actual ramifications of alterna -\\ntives are difficult to understand without detailed analysis;\\n• Uncertainty: Uncertainty in key inputs creates \\nsubstantial uncertainty in the ranking of alter -\\nnatives and points to risks that may need to be managed;\\n• Multiple Attributes: Greater numbers of attributes \\ncause a greater need for formal analysis; and\\n• Diversity of Stakeholders: Extra attention is war -\\nranted to clarify objectives and formulate TPMs 1676.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nwhen the set of stakeholders reflects a diversity of \\nvalues, preferences, and perspectives.\\nSatisfaction of all of these conditions is not a require -\\nment for initiating decision analysis. The point is, \\nrather, that the need for decision analysis increases \\nas a function of the above conditions. In addition, \\noften these decisions have the potential to result in \\nhigh stakes impacts to cost, safety, or mission success \\ncriteria, which should be identified and addressed in \\nthe process. When the Decision Analysis Process is \\ntriggered, the following are inputs:\\n• Decision need, identified alternatives, issues, or \\nproblems and supporting data:  This information \\nwould come from all technical, cost, and sched -\\nule management processes. It may also include \\nhigh-level objectives and constraints (from the \\nprogram/project).\\n• Analysis support requests:  Requests will arise \\nfrom the technical, cost, and schedule assessment \\nprocesses.\\n6.8.1.2 Process Activities\\nFor the Decision Analysis Process, the following \\nactivities are typically performed.\\nIt is important to understand the decision needed in \\nthe context of the mission and system, which requires \\nknowledge of the intended outcome in terms of tech -\\nnical performance, cost, and schedule. A part of this \\nunderstanding is the definition of the decision crite -\\nria, or the measures that are important to character -\\nize the options for making a decision. The specific \\ndecision-making body, whether the program/project \\nmanager, chief engineer, line management, or con -\\ntrol board should also be well defined. Based on this \\nunderstanding, then the specific approach to deci -\\nsion-making can be defined.Decisions are based on facts, qualitative and quan -\\ntitative data, engineering judgment, and open com -\\nmunications to facilitate the flow of information \\nthroughout the hierarchy of forums where technical \\nanalyses and evaluations are presented and assessed \\nand where decisions are made. The extent of technical \\nanalysis and evaluation required should be commen -\\nsurate with the consequences of the issue requiring \\na decision. The work required to conduct a formal \\nevaluation is significant and applicability should be \\nbased on the nature of the problem to be resolved. \\nGuidelines for use can be determined by the magni -\\ntude of the possible consequences of the decision to \\nbe made.\\n6.8.1.2.1  Define the Criteria for Evaluating Alternative \\nSolutions\\nThis step includes identifying the following:\\n• The types of criteria to consider, such as customer \\nexpectations and requirements, technology lim -\\nitations, environmental impact, safety, risks, total \\nownership and life cycle costs, and schedule impact;\\n• The acceptable range and scale of the criteria; and\\n• The rank of each criterion by its importance.\\nDecision criteria are requirements for individually \\nassessing the options and alternatives being consid -\\nered. Typical decision criteria include cost, sched -\\nule, risk, safety, mission success, and supportability. \\nHowever, considerations should also include tech -\\nnical criteria specific to the decision being made. \\nCriteria should be objective and measurable. Criteria \\nshould also permit differentiating among options or \\nalternatives. Some criteria may not be meaningful to \\na decision; however, they should be documented as \\nhaving been considered. Criteria may be mandatory \\n(i.e., “shall have”) or enhancing. An option that does 1686.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nnot meet mandatory criteria should be disregarded. \\nFor complex decisions, criteria can be grouped into \\ncategories or objectives.\\n6.8.1.2.2  Identify Alternative Solutions to Address \\nDecision Issues\\nWith the decision need well understood, alterna -\\ntives can be identified that fit the mission and system \\ncontext. There may be several alternatives that could \\npotentially satisfy the decision criteria. Alternatives \\ncan be found from design options, operational \\noptions, cost options, and/or schedule options.\\nAlmost every decision will have options to choose \\nfrom. These options are often fairly clear within the \\nmission and system context once the decision need is \\nunderstood. In cases where the approach has uncer -\\ntainty, there are several methods to help generate \\nvarious options. Brainstorming decision options with \\nthose knowledgeable of the context and decision can \\nprovide a good list of candidate alternatives. A litera -\\nture search of related systems and approaches to iden -\\ntify options may also provide some possible options. \\nAll possible options should be considered. This can \\nget unwieldy if a large number of variations is possi -\\nble. A “trade tree” (discussed later) is an excellent way \\nto prune the set of variations before extensive analysis \\nis undertaken, and to convey to other stakeholders \\nthe basis for that pruning.\\nA good understanding of decision need and criteria \\nwill include the definition of primary and secondary \\nfactors. Options should be focused on primary fac -\\ntors in the decision as defined by the decision criteria. \\nNon-primary factors (i.e., secondary, tertiary) can be \\nincluded in evaluations but should not, in general, \\ndefine separate alternatives. This will require some \\nengineering judgment that is based on the mission \\nand system context as well as the identified decision \\ncriteria. Some options may quickly drop out of con -\\nsideration as the analysis is conducted. It is important to document the fact that these options were consid -\\nered. A few decisions might only have one option. It \\nis a best practice to document a decision matrix for \\na major decision even if only one alternative is deter -\\nmined to be viable. (Sometimes doing nothing or not \\nmaking a decision is an option.)\\n6.8.1.2.3 Select Evaluation Methods and Tools\\nBased on the decision to be made, various approaches \\ncan be taken to evaluate identified alternatives. These \\ncan range from simple discussion meetings with con -\\ntributing and affected stakeholders to more formal \\nevaluation methods. In selecting the approach, the \\nmission and system context should be kept in mind \\nand the complexity of the decision analysis should \\nfit the complexity of the mission, system, and corre -\\nsponding decision.\\nEvaluation methods and tools/techniques to be used \\nshould be selected based on the purpose for analyzing \\na decision and on the availability of the information \\nused to support the method and/or tool. Typical eval -\\nuation methods include: simulations; weighted trade-\\noff matrices; engineering, manufacturing, cost, and \\ntechnical opportunity trade studies; surveys; human-\\nin-the-loop testing; extrapolations based on field expe -\\nrience and prototypes; user review and comment; and \\ntesting. Section 6.8.2  provides several options.\\n6.8.1.2.4 Evaluate Alternative Solutions with the \\nEstablished Criteria and Selected Methods\\nThe performance of each alternative with respect to \\neach chosen performance measure is evaluated. In all \\nbut the simplest cases, some consideration of uncer -\\ntainty is warranted. Uncertainty matters in a partic -\\nular analysis only if there is a non-zero probability \\nthat uncertainty reduction could alter the ranking \\nof alternatives. If this condition is obtained, then it \\nis necessary to consider the value of reducing that \\nuncertainty, and act accordingly.1696.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nRegardless of the methods or tools used, results \\nshould include the following:\\n• Evaluation of assumptions related to evaluation \\ncriteria and of the evidence that supports the \\nassumptions; and\\n• Evaluation of whether uncertainty in the values \\nfor alternative solutions affects the evaluation.\\nWhen decision criteria have different measurement \\nbases (e.g., numbers, money, weight, dates), normal -\\nization can be used to establish a common base for \\nmathematical operations. The process of “normaliza -\\ntion” is making a scale so that all different kinds of \\ncriteria can be compared or added together. This can \\nbe done informally (e.g., low, medium, high), on a \\nscale (e.g., 1-3-9), or more formally with a tool. No \\nmatter how normalization is done, the most import -\\nant thing to remember is to have operational defi -\\nnitions of the scale. An operational definition is a \\nrepeatable, measurable number. For example, “high” \\ncould mean “a probability of 67 percent and above.” \\n“Low” could mean “a probability of 33 percent and \\nbelow.” For complex decisions, decision tools usually \\nprovide an automated way to normalize. It is import -\\nant to question and understand the operational defi -\\nnitions for the weights and scales of the tool.\\nNOTE: Completing the decision matrix can \\nbe thought of as a default evaluation method. \\nCompleting the decision matrix is iterative. Each \\ncell for each criterion and each option needs to be \\ncompleted by the team. Use evaluation methods as \\nneeded to complete the entire decision matrix.6.8.1.2.5  Select Recommended Solutions from the \\nAlternatives Based on the Evaluation Criteria \\nand Report to the Decision-Maker\\nOnce the decision alternative evaluation is completed, \\nrecommendations should be brought back to the deci -\\nsion maker including an assessment of the robustness \\nof the ranking (i.e., whether the uncertainties are such \\nthat reducing them could credibly change the rank -\\ning of the alternatives). Generally, a single alternative \\nshould be recommended. However, if the alternatives \\ndo not significantly differ, or if uncertainty reduction \\ncould credibly alter the ranking, the recommenda -\\ntion should include all closely ranked alternatives for \\na final selection by the decision-maker. In any case, \\nthe decision-maker is always free to select any alter -\\nnative or ask for additional alternatives to be assessed \\n(often with updated guidance on selection criteria). \\nThis step includes documenting the information, \\nincluding assumptions and limitations of the evalu -\\nation methods used, and analysis of the uncertainty \\nin the analysis of the alternatives’ performance that \\njustifies the recommendations made and gives the \\nimpacts of taking the recommended course of action, \\nincluding whether further uncertainty reduction \\nwould be justifiable.\\nThe highest score (e.g., percentage, total score) is \\ntypically the option that is recommended to man -\\nagement. If a different option is recommended, an \\nexplanation should be provided as to why the lower \\nscore is preferred. Usually, if an alternative having \\na lower score is recommended, the “risks” or “dis -\\nadvantages” were too great for the highest ranking \\nalternative indicating the scoring methods did not \\nproperly rank the alternatives. Sometimes the bene -\\nfits and advantages of a lower or close score outweigh \\nthe highest score. If this occurs, the decision criteria \\nshould be reevaluated, not only the weights, but the 1706.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nbasic definitions of what is being measured for each \\nalternative. The criteria should be updated, with con -\\ncurrence from the decision-maker, to more correctly \\nreflect the suitability of each alternative.\\n6.8.1.2.6 Report Analysis Results\\nThese results are reported to the appropriate stake -\\nholders with recommendations, impacts, and correc -\\ntive actions\\n6.8.1.2.7 Capture Work Products\\nThese work products may include the decision anal -\\nysis guidelines, strategy, and procedures that were \\nused; analysis/evaluation approach; criteria, meth -\\nods, and tools used; analysis/evaluation assumptions \\nmade in arriving at recommendations; uncertainties; \\nsensitivities of the recommended actions or corrective \\nactions; and lessons learned.\\n6.8.1.3 Outputs\\n6.8.1.3.1  Alternative Selection and Decision Support \\nRecommendations and Impacts\\nOnce the technical team recommends an alterna -\\ntive to a NASA decision-maker (e.g., a NASA board, \\nforum, or panel), all decision analysis information \\nshould be documented. The team should produce a \\nreport to document all major recommendations to \\nserve as a backup to any presentation materials used. \\nA report in conjunction with a decision matrix pro -\\nvides clearly documented rationale for the presen -\\ntation materials (especially for complex decisions). \\nDecisions are typically captured in meeting minutes \\nand should be captured in the report as well. Based \\non the mission and system context and the decision \\nmade, the report may be a simple white paper or a \\nmore formally formatted document. The important \\ncharacteristic of the report is the content, which fully \\ndocuments the decision needed, assessments done, \\nrecommendations, and decision finally made.This report includes the following:\\n• mission and system context for the decision\\n• decision needed and intended outcomes\\n• decision criteria\\n• identified alternative solutions\\n• decision evaluation methods and tools employed\\n• assumptions, uncertainties, and sensitivities in \\nthe evaluations and recommendations\\n• results of all alternative evaluations\\n• alternative recommendations\\n• final decision made with rationale\\n• lessons learned\\nTypical information captured in a decision report is \\nshown in TABLE 6.8-1 .\\n6.8.2  Decision Analysis Guidance\\nRefer to Section 6.8.2 in the NASA Expanded \\nGuidance for Systems Engineering at https://nen.\\nnasa.gov/web/se/doc-repository  for additional guid -\\nance on decision analysis methods supporting all SE \\nprocesses and phases including:\\n• trade studies,\\n• cost-benefit analysis,\\n• influence diagrams,\\n• decision trees,\\n• analytic hierarchy process,\\n• Borda counting, and\\n• utility analysis,\\nAdditional information on tools for decision making \\ncan be found in NASA Reference Publication 1358, \\nSystem Engineering “Toolbox” for Design-Oriented \\nEngineers  located at https://nen.nasa.gov/web/se/\\ndoc-repository .1716.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nTABLE 6.8 -1 Typical Information to Capture in a Decision Report\\n#Section Section Description\\n1 Executive \\nSummaryProvide a short half-page executive summary of the report:\\n• Recommendation (short summary—1 sentence)\\n• Problem/issue requiring a decision (short summary—1 sentence)\\n2 Problem/Issue \\nDescriptionDescribe the problem/issue that requires a decision. Provide background, history, the \\ndecision maker(s) (e.g., board, panel, forum, council), and decision recommendation team, \\netc.\\n3 Decision Matrix \\nSetup RationaleProvide the rationale for setting up the decision matrix:\\n• Criteria selected\\n• Options selected\\n• Weights selected\\n• Evaluation methods selected\\nProvide a copy of the setup decision matrix.\\n4 Decision Matrix \\nScoring RationaleProvide the rationale for the scoring of the decision matrix. Provide the results of populating \\nthe scores of the matrix using the evaluation methods selected.\\n5 Final Decision \\nMatrixCut and paste the final spreadsheet into the document. Also include any important \\nsnapshots of the decision matrix.\\n6 Risk/Benefits For the final options being considered, document the risks and benefits of each option.\\n7 Recommendation \\nand/or Final \\nDecisionDescribe the recommendation that is being made to the decision maker(s) and the rationale \\nfor why the option was selected. Can also document the final decision in this section.\\n8 Dissent If applicable, document any dissent with the recommendation. Document how dissent was \\naddressed (e.g., decision matrix, risk).\\n9 References Provide any references.\\nA Appendices Provide the results of the literature search, including lessons learned, previous related \\ndecisions, and previous related dissent. Also document any detailed data analysis and risk \\nanalysis used for the decision. Can also document any decision metrics.1726.0 Crosscutting Technical Management\\nNASA SYSTEMS ENGINEERING HANDBOOK173\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nAppendix A: Acronyms\\nAADL Architecture Analysis and Design Language\\nAD2Advancement Degree of Difficulty \\nAssessment\\nAIAA American Institute of Aeronautics and Astronautics\\nAO Announcement of Opportunity\\nAS9100 Aerospace Quality Management Standard\\nASME American Society of Mechanical Engineers\\nASQ American Society for Quality\\nCAIB Columbia Accident Investigation Board\\nCCB Configuration Control Board\\nCDR Critical Design Review\\nCE Concurrent Engineering or Chief Engineer\\nCEQ Council on Environmental Quality\\nCERR Critical Event Readiness Review\\nCHSIP Commercial Human Systems Integration Processes\\nCI Configuration Item\\nCM Configuration Management\\nCMO Configuration Management Organization\\nConOps Concept of Operations\\nCOSPAR Committee on Space Research\\nCOTS Commercial Off-The-Shelf\\nCPI Critical Program Information\\nCR Change Request\\nCRM Continuous Risk Management\\nCSA Configuration Status Accounting\\nD&C Design and Construction\\nDDT&E Design, Development, Test, and Evaluation\\nDM Data Management\\nDOD (U.S.) Department of Defense\\nDODAF DOD Architecture Framework\\nDR Decommissioning Review\\nDRM Design Reference Mission\\nDRR Disposal Readiness Review\\nEDL Entry, Descent, and LandingEEE Electrical, Electronic, and Electromechanical\\nEFFBD Enhanced Functional Flow Block Diagram\\nEIA Electronic Industries Alliance\\nEMC Electromagnetic Compatibility\\nEMI Electromagnetic Interference\\nEO (U.S.) Executive Order\\nEOM End of Mission\\nEVM Earned Value Management\\nFA Formulation Agreement\\nFAD Formulation Authorization Document\\nFAR Federal Acquisition Regulation\\nFCA Functional Configuration Audit\\nFFBD Functional Flow Block Diagram\\nFIPS Federal Information Processing Standard\\nFM Fault Management\\nFMEA Failure Modes and Effects Analysis\\nFMR Financial Management Requirements\\nFRR Flight Readiness Review\\nFTE Full Time Equivalent\\nGEO Geostationary\\nGOTS Government Off-The-Shelf\\nGSE Government-Supplied Equipment or Ground Support Equipment\\nGSFC Goddard Space Flight Center\\nHCD Human-Centered Design\\nHF Human Factors\\nHITL Human-In-The-Loop\\nHQ Headquarters\\nHSI Human Systems Integration\\nHSIP Human System Integration Plan\\nHWIL HardWare-In-the-Loop\\nI&T Integration and Test\\nICD Interface Control Document/Drawing\\nICP I\\nnterface Control Plan\\nIDD Interface Definition Document174\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix A: Acronyms\\nIDEF0 Integration Definition (for functional modeling)\\nIEEE Institute of Electrical and Electronics \\nEngineers\\nILS Integrated Logistics Support\\nINCOSE International Council on Systems Engineering\\nIPT Integrated Product Team\\nIRD Interface Requirements Document\\nISO International Organization for Standardization\\nIT Information Technology\\nITA Internal Task Agreement\\nITAR International Traffic in Arms Regulation\\nIV&V Independent Verification and Validation\\nIVHM Integrated Vehicle Health Management\\nIWG Interface Working Group\\nJCL Joint (cost and schedule) Confidence Level\\nJPL Jet Propulsion Laboratory\\nKBSI Knowledge Based Systems, Inc.\\nKDP Key Decision Point\\nKDR Key Driving Requirement\\nKPP Key Performance Parameter\\nKSC Kennedy Space Center\\nLCC Life Cycle Cost\\nLEO Low Earth Orbit or Low Earth Orbiting\\nM&S Modeling and Simulation or Models and \\nSimulations\\nMBSE Model-Based Systems Engineering\\nMCR Mission Concept Review\\nMDAA Mission Directorate Associate Administrator\\nMDR Mission Definition Review\\nMEL Master Equipment List\\nMODAF (U.K.) Ministry of Defense Architecture Framework\\nMOE Measure of Effectiveness\\nMOP Measure of Performance\\nMOTS Modified Off-The-Shelf\\nMOU Memorandum of Understanding\\nMRB Material Review Board\\nMRR Mission Readiness Review\\nMSFC Marshall Space Flight Center\\nNASA (U.S.) National Aeronautics and Space Administration\\nNEN NASA Engineering Network\\nNEPA National Environmental Policy Act\\nNFS NASA FAR SupplementNGO Needs, Goals, and Objectives\\nNIAT NASA Integrated Action Team\\nNID NASA Interim Directive\\nNOA New Obligation Authority\\nNOAA (U.S.) National Oceanic and Atmospheric Administration\\nNODIS NASA Online Directives Information System\\nNPD NASA Policy Directive\\nNPR NASA Procedural Requirements\\nNRC (U.S.) Nuclear Regulatory Commission\\nNSTS National Space Transportation System\\nOCE (NASA) Office of the Chief Engineer\\nOCIO (NASA) Office of the Chief Information Officer\\nOCL Object Constraint Language\\nOMB (U.S.) Office of Management and Budget\\nORR Operational Readiness Review\\nOTS Off-the-Shelf\\nOWL Web Ontology Language\\nPBS Product Breakdown Structure\\nPCA Physical Configuration Audit or Program Commitment Agreement\\nPD/NSC (U.S.) Presidential Directive/National \\nSecurity Council\\nPDR Preliminary Design Review\\nPFAR Post-Flight Assessment Review\\nPI\\nPerformance Index or Principal Investigator\\nPIR Program Implementation Review\\nPKI Public Key Infrastructure\\nPLAR Post-Launch Assessment Review\\nPM Program Manager or Project Manager\\nPMC Program Management Council\\nPPD (U.S.) Presidential Policy Directive\\nPRA Probabilistic Risk Assessment\\nPRD Project Requirements Document\\nPRR Production Readiness Review\\nQA Quality Assurance\\nQVT Query View Transformations\\nR&M Reliability and Maintainability\\nR&T Research and Technology\\nRACI Responsible, Accountable, Consulted, \\nInformed\\nREC Record of Environmental Consideration\\nRF Radio Frequency175\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix A: Acronyms\\nRFA Requests for Action\\nRFP Request for Proposal\\nRID Review Item Discrepancy or Review Item \\nDisposition\\nRIDM Risk-Informed Decision-Making\\nRM Risk Management\\nRMA Rapid Mission Architecture\\nRUL Remaining Useful Life\\nSAR System Acceptance Review or Safety \\nAnalysis Report (DOE)\\nSBU Sensitive But Unclassified\\nSDR Program/System Definition Review\\nSE Systems Engineering\\nSECoP Systems Engineering Community of Practice\\nSEMP Systems Engineering Management Plan\\nSI International System of Units (French: \\nSystème international d’unités)\\nSIR System Integration Review\\nSMA Safety and Mission Assurance\\nSME Subject Matter Expert\\nSOW Statement Of Work\\nSP Special Publication\\nSRD System Requirements DocumentSRR Program/System Requirements Review\\nSRS Software Requirements Specification\\nSTI Scientific and Technical Information\\nSTS Space Transportation System\\nSysML System Modeling Language\\nT&E Test and Evaluation\\nTA Technical Authority\\nTBD To Be Determined\\nTBR To Be Resolved\\nToR Terms of Reference\\nTPM Technical Performance Measure\\nTRL Technology Readiness Level\\nTRR Test Readiness Review\\nTVC Thrust Vector Controller\\nUFE Unallocated Future Expenses\\nUML Unified Modeling Language\\nV&V Verification and Validation\\nWBS Work Breakdown Structure\\nWYE Work Year Equivalent\\nXMI XML Metadata Interchange\\nXML Extensible Markup Language176\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nAppendix B: Glossary\\nAcceptable Risk: The risk that is understood and \\nagreed to by the program/project, governing author -\\nity, mission directorate, and other customer(s) such \\nthat no further specific mitigating action is required.\\nAcquisition: The process for obtaining the systems, \\nresearch, services, construction, and supplies that NASA needs to fulfill its missions. Acquisition, which may include procurement (contracting for products and services), begins with an idea or pro -\\nposal that aligns with the NASA Strategic Plan and fulfills an identified need and ends with the comple -\\ntion of the program or project or the final disposition of the product or service.\\nActivity: A set of tasks that describe the technical \\neffort to accomplish a process and help generate expected outcomes.\\nAdvancement Degree of Difficulty Assessment \\n(AD2): The process to develop an understanding \\nof what is required to advance the level of system \\nmaturity.\\nAllocated Baseline (Phase C): The allocated base -\\nline is the approved performance-oriented configu -\\nration documentation for a CI to be developed that describes the functional and interface characteristics that are allocated from a higher level requirements document or a CI and the verification required to demonstrate achievement of those specified charac -\\nteristics. The allocated baseline extends the top-level performance requirements of the functional baseline to sufficient detail for initiating manufacturing or coding of a CI. The allocated baseline is controlled by NASA. The allocated baseline(s) is typically estab -\\nlished at the Preliminary Design Review.\\nAnalysis: Use of mathematical modeling and analytical \\ntechniques to predict the compliance of a design to its requirements based on calculated data or data derived from lower system structure end product validations.\\nAnalysis of Alternatives: A formal analysis method \\nthat compares alternative approaches by estimating their ability to satisfy mission requirements through an effectiveness analysis and by estimating their life cycle costs through a cost analysis. The results of these two analyses are used together to produce a cost-effectiveness comparison that allows decision makers to assess the relative value or potential pro -\\ngrammatic returns of the alternatives. An analysis of alternatives broadly examines multiple elements of program or project alternatives (including technical performance, risk, LCC, and programmatic aspects).\\nAnalytic Hierarchy Process: A multi-attribute meth -\\nodology that provides a proven, effective means to deal with complex decision- making and can assist with identifying and weighting selection criteria, analyzing the data collected for the criteria, and expe -\\nditing the decision-making process.\\nAnomaly: The unexpected performance of intended \\nfunction.177\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix B: Glossary\\nApproval: Authorization by a required management \\nofficial to proceed with a proposed course of action. \\nApprovals are documented.\\nApproval (for Implementation): The acknowledg -\\nment by the decision authority that the program/\\nproject has met stakeholder expectations and for -\\nmulation requirements, and is ready to proceed to \\nimplementation. By approving a program/project, \\nthe decision authority commits the budget resources \\nnecessary to continue into implementation. Approval \\n(for Implementation) is documented.\\nArchitecture (System): Architecture is the high-level \\nunifying structure that defines a system. It provides \\na set of rules, guidelines, and constraints that defines \\na cohesive and coherent structure consisting of con -\\nstituent parts, relationships and connections that \\nestablish how those parts fit and work together. It \\naddresses the concepts, properties and characteristics \\nof the system and is represented by entities such as \\nfunctions, functional flows, interfaces, relationships, \\nresource flow items, physical elements, containers, \\nmodes, links, communication resources, etc. The \\nentities are not independent but interrelated in the \\narchitecture through the relationships between them \\n(NASA HQ).\\nArchitecture (ISO Definition): Fundamental concepts \\nor properties of a system in its environment embod -\\nied in its elements, relationships, and in the principles \\nof its design and evolution (ISO 42010).\\nAs-Deployed Baseline: The as-deployed baseline \\noccurs at the Operational Readiness Review. At this \\npoint, the design is considered to be functional and \\nready for flight. All changes will have been incorpo -\\nrated into the documentation.\\nAutomated: Automation refers to the allocation of \\nsystem functions to machines (hardware or software) \\nversus humans.Autonomous: Autonomy refers to the relative loca -\\ntions and scope of decision-making and control func -\\ntions between two locations within a system or across \\nthe system boundary.\\nBaseline : An agreed-to set of requirements, designs, \\nor documents that will have changes controlled \\nthrough a formal approval and monitoring process.\\nBidirectional Traceability : The ability to trace any \\ngiven requirement/expectation to its parent require -\\nment/expectation and to its allocated children \\nrequirements/expectations.\\nBrassboard: A medium fidelity functional unit that \\ntypically tries to make use of as much operational \\nhardware/software as possible and begins to address \\nscaling issues associated with the operational sys -\\ntem. It does not have the engineering pedigree in \\nall aspects, but is structured to be able to operate in \\nsimulated operational environments in order to assess \\nperformance of critical functions.\\nBreadboard: A low fidelity unit that demonstrates \\nfunction only, without respect to form or fit in the \\ncase of hardware, or platform in the case of software. \\nIt often uses commercial and/or ad hoc components \\nand is not intended to provide definitive information \\nregarding operational performance.\\nComponent Facilities: Complexes that are geo -\\ngraphically separated from the NASA Center or \\ninstitution to which they are assigned, but are still \\npart of the Agency.\\nConcept of Operations (ConOps) (Concept \\nDocumentation): Developed early in Pre-Phase A, \\nthe ConOps describes the overall high-level concept \\nof how the system will be used to meet stakeholder \\nexpectations, usually in a time-sequenced manner. It \\ndescribes the system from an operational perspective \\nand helps facilitate an understanding of the system 178\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix B: Glossary\\ngoals. It stimulates the development of the require -\\nments and architecture related to the user elements \\nof the system. It serves as the basis for subsequent \\ndefinition documents and provides the foundation \\nfor the long-range operational planning activities.\\nConcurrence: A documented agreement by a manage -\\nment official that a proposed course of action is acceptable.\\nConcurrent Engineering: Design in parallel rather \\nthan serial engineering fashion. It is an approach to \\nproduct development that brings manufacturing, \\ntesting, assurance, operations and other disciplines \\ninto the design cycle to ensure all aspects are incorpo -\\nrated into the design and thus reduce overall product \\ndevelopment time.\\nConfiguration Items (CI): Any hardware, software, or \\ncombination of both that satisfies an end use func -\\ntion and is designated for separate configuration \\nmanagement. For example, configuration items can \\nbe referred to by an alphanumeric identifier which \\nalso serves as the unchanging base for the assignment \\nof serial numbers to uniquely identify individual \\nunits of the CI.\\nConfiguration Management Process: A management \\ndiscipline that is applied over a product’s life cycle to \\nprovide visibility into and to control changes to perfor -\\nmance and functional and physical characteristics. It \\nensures that the configuration of a product is known \\nand reflected in product information, that any prod -\\nuct change is beneficial and is effected without adverse \\nconsequences, and that changes are managed.\\nContext Diagram: A diagram that shows external sys -\\ntems that impact the system being designed.\\nContinuous Risk Management: A systematic and \\niterative process that efficiently identifies, ana -\\nlyzes, plans, tracks, controls, communicates, and documents risks associated with implementation of \\ndesigns, plans, and processes.\\nContract: A mutually binding legal relationship obli -\\ngating the seller to furnish the supplies or services \\n(including construction) and the buyer to pay for \\nthem. It includes all types of commitments that obli -\\ngate the Government to an expenditure of appropri -\\nated funds and that, except as otherwise authorized, \\nare in writing. In addition to bilateral instruments, \\ncontracts include (but are not limited to) awards and \\nnotices of awards; job orders or task letters issued \\nunder basic ordering agreements; letter contracts; \\norders, such as purchase orders under which the \\ncontract becomes effective by written acceptance or \\nperformance; and bilateral contract modifications. \\nContracts do not include grants and cooperative \\nagreements.\\nContractor: An individual, partnership, company, \\ncorporation, association, or other service having a \\ncontract with the Agency for the design, develop -\\nment, manufacture, maintenance, modification, \\noperation, or supply of items or services under the \\nterms of a contract to a program or project. Research \\ngrantees, research contractors, and research subcon -\\ntractors are excluded from this definition.\\nControl Account Manager: A manager responsible \\nfor a control account and for the planning, develop -\\nment, and execution of the budget content for those \\naccounts.\\nControl Gate (or milestone): A defined point in \\nthe program/project life cycle where the decision \\nauthority can evaluate progress and determine next \\nactions. These may include a key decision point, life \\ncycle review, or other milestones identified by the \\nprogram/project.179\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix B: Glossary\\nCost-Benefit Analysis: A methodology to determine \\nthe advantage of one alternative over another in terms \\nof equivalent cost or benefits. It relies on totaling pos -\\nitive factors and subtracting negative factors to deter -\\nmine a net result.\\nCost-Effectiveness Analysis: A systematic quanti -\\ntative method for comparing the costs of alternative \\nmeans of achieving the same equivalent benefit for a \\nspecific objective.\\nCritical Design Review: A review that demonstrates \\nthat the maturity of the design is appropriate to sup -\\nport proceeding with full-scale fabrication, assembly, \\nintegration, and test, and that the technical effort is \\non track to complete the system development meet -\\ning performance requirements within the identified \\ncost and schedule constraints.\\nCritical Event (or key event): An event in the opera -\\ntions phase of the mission that is time-sensitive and is \\nrequired to be accomplished successfully in order to \\nachieve mission success. These events should be consid -\\nered early in the life cycle as drivers for system design.\\nCritical Event Readiness Review: A review that eval -\\nuates the readiness of a project’s flight system to exe -\\ncute the critical event during flight operation.\\nCustomer: The organization or individual that has \\nrequested a product and will receive the product to \\nbe delivered. The customer may be an end user of \\nthe product, the acquiring agent for the end user, or \\nthe requestor of the work products from a technical \\neffort. Each product within the system hierarchy has \\na customer.\\nData Management: DM is used to plan for, acquire, \\naccess, manage, protect, and use data of a technical \\nnature to support the total life cycle of a system.Decision Analysis Process: A methodology for \\nmaking decisions that offers techniques for model -\\ning decision problems mathematically and finding \\noptimal decisions numerically. The methodology \\nentails identifying alternatives, one of which should \\nbe decided upon; possible events, one of which occurs \\nthereafter; and outcomes, each of which results from \\na combination of decision and event.\\nDecision Authority: The individual authorized by the \\nAgency to make important decisions for programs \\nand projects under his or her authority.\\nDecision Matrix: A methodology for evaluating \\nalternatives in which valuation criteria are typically \\ndisplayed in rows on the left side of the matrix and \\nalternatives are the column headings of the matrix. A \\n“weight” is typically assigned to each criterion.\\nDecision Support Package: Documentation submitted \\nin conjunction with formal reviews and change requests.\\nDecision Tree: A decision model that displays the \\nexpected consequences of all decision alternatives by \\nmaking discreet all “chance” nodes, and, based on \\nthis, calculating and appropriately weighting the pos -\\nsible consequences of all alternatives.\\nDecommissioning Review: A review that confirms \\nthe decision to terminate or decommission a system \\nand assess the readiness for the safe decommissioning \\nand disposal of system assets. The DR is normally \\nheld near the end of routine mission operations upon \\naccomplishment of planned mission objectives. It \\nmay be advanced if some unplanned event gives rise \\nto a need to prematurely terminate the mission, or \\ndelayed if operational life is extended to permit addi -\\ntional investigations.\\nDeliverable Data Item: Consists of technical \\ndata, such as requirements specifications, design 180\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix B: Glossary\\ndocuments, management data plans, and metrics \\nreports, that have been identified as items to be deliv -\\nered with an end product.\\nDemonstration: Showing that the use of an end \\nproduct achieves the individual specified requirement \\n(verification) or stakeholder expectation (validation). \\nIt is generally a basic confirmation of performance \\ncapability, differentiated from testing by the lack of \\ndetailed data gathering. Demonstrations can involve \\nthe use of physical models or mock-ups; for example, \\na requirement that all controls shall be reachable by \\nthe pilot could be verified by having a pilot perform \\nflight-related tasks in a cockpit mock-up or simulator. \\nA demonstration could also be the actual operation \\nof the end product by highly qualified personnel, \\nsuch as test pilots, who perform a one-time event that \\ndemonstrates a capability to operate at extreme limits \\nof system performance.\\nDerived Requirements: Requirements arising from \\nconstraints, consideration of issues implied but not \\nexplicitly stated in the high-level direction provided \\nby NASA Headquarters and Center institutional \\nrequirements, factors introduced by the selected \\narchitecture, and the design. These requirements are \\nfinalized through requirements analysis as part of \\nthe overall systems engineering process and become \\npart of the program or project requirements baseline. \\nRequirements arising from constraints, consideration \\nof issues implied but not explicitly stated in the high-\\nlevel direction provided by NASA Headquarters and \\nCenter institutional requirements, factors introduced \\nby the selected architecture, and the design. These \\nrequirements are finalized through requirements \\nanalysis as part of the overall systems engineering \\nprocess and become part of the program or project \\nrequirements baseline.\\nDescope: As a verb, take out of (or remove from) \\nthe scope of a project. As a noun, as in “performance descope,” it indicates the process or the result of the \\nprocess of narrowing the scope; i.e., removing part of \\nthe original scope.\\nDesign Solution Definition Process: The process used \\nto translate the outputs of the logical decomposition \\nprocess into a design solution definition. It includes \\ntransforming the defined logical decomposition \\nmodels and their associated sets of derived technical \\nrequirements into alternative solutions and analyzing \\neach alternative to be able to select a preferred alterna -\\ntive and fully define that alternative into a final design \\nsolution that will satisfy the technical requirements.\\nDesignated Governing Authority: For the technical \\neffort, this is the Center Director or the person that \\nhas been designated by the Center Director to ensure \\nthe appropriate level of technical management over -\\nsight. For large programs, this will typically be the \\nEngineering Technical Authority. For smaller proj -\\nects, this function can be delegated to line managers.\\nDetection: Determination that system state or behav -\\nior is different from expected performance.\\nDiagnosis: Determining the possible locations and/\\nor causes of an anomaly or a failure.\\nDiscrepancy: Any observed variance from, lack of \\nagreement with, or contradiction to the required or \\nexpected outcome, configuration, or result.\\nEarned Value: The sum of the budgeted cost for tasks \\nand products that have actually been produced (com -\\npleted or in progress) at a given time in the schedule.\\nEarned Value Management: A tool for measur -\\ning and assessing project performance through the \\nintegration of technical scope with schedule and \\ncost objectives during the execution of the project. \\nEVM provides quantification of technical progress, 181\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix B: Glossary\\nenabling management to gain insight into project sta -\\ntus and project completion costs and schedules. Two \\nessential characteristics of successful EVM are EVM \\nsystem data integrity and carefully targeted monthly \\nEVM data analyses (i.e., risky WBS elements).\\nEmergent Behavior: An unanticipated behavior \\nshown by a system due to interactions between large \\nnumbers of simple components of that system.\\nEnd Product: The hardware/software or other prod -\\nuct that performs the operational functions. This \\nproduct is to be delivered to the next product layer or \\nto the final customer.\\nEnabling Products: The life cycle support products \\nand services (e.g., production, test, deployment, \\ntraining, maintenance, and disposal) that facili -\\ntate the progression and use of the operational end \\nproduct through its life cycle. Since the end prod -\\nuct and its enabling products are interdependent, \\nthey are viewed as a system. Project responsibility \\nthus extends to acquiring services from the relevant \\nenabling products in each life cycle phase. When a \\nsuitable enabling product does not already exist, \\nthe project that is responsible for the end product \\nmay also be responsible for creating and using the \\nenabling product.\\nEngineering Unit: A high fidelity unit that demon -\\nstrates critical aspects of the engineering processes \\ninvolved in the development of the operational unit. \\nEngineering test units are intended to closely resem -\\nble the final product (hardware/software) to the max -\\nimum extent possible and are built and tested so as to \\nestablish confidence that the design will function in \\nthe expected environments. In some cases, the engi -\\nneering unit will become the final product, assuming \\nthat proper traceability has been exercised over the \\ncomponents and hardware handling.Enhanced Functional Flow Block Diagram: A block \\ndiagram that represents control flows and data flows \\nas well as system functions and flow.\\nEntrance Criteria: Guidance for minimum accom -\\nplishments each project needs to fulfill prior to a life \\ncycle review.\\nEnvironmental Impact: The direct, indirect, or \\ncumulative beneficial or adverse effect of an action \\non the environment.\\nEnvironmental Management: The activity of ensur -\\ning that program and project actions and decisions \\nthat potentially impact or damage the environment \\nare assessed and evaluated during the formulation \\nand planning phase and reevaluated throughout \\nimplementation. This activity is performed according \\nto all NASA policy and Federal, state, and local envi -\\nronmental laws and regulations.\\nEstablish (with respect to processes): The act of \\ndeveloping policy, work instructions, or procedures \\nto implement process activities.\\nEvaluation: The continual self- and independent \\nassessment of the performance of a program or proj -\\nect and incorporation of the evaluation findings to \\nensure adequacy of planning and execution accord -\\ning to plan.\\nExtensibility: The ability of a decision to be extended \\nto other applications.\\nFailure: The inability of a system, subsystem, com -\\nponent, or part to perform its required function \\nwithin specified limits (Source: NPR 8715.3 and \\nAvizienis  2004).\\nFailure Tolerance: The ability to sustain a certain \\nnumber of failures and still retain capability (Source: 182\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix B: Glossary\\nNPR 8705.2). A function should be preserved despite \\nthe presence of any of a specified number of coinci -\\ndent, independent failure causes of specified types.\\nFault: A physical or logical cause, which explains a \\nfailure (Source: Avizienis 2004).\\nFault Identification: Determining the possible loca -\\ntions of a failure or anomaly cause(s), to a defined \\nlevel of granularity.\\nFault Isolation: The act of containing the effects of a \\nfault to limit the extent of failure.\\nFault Management: A specialty engineering disci -\\npline that encompasses practices that enable an oper -\\national system to contain, prevent, detect, diagnose, \\nidentify, respond to, and recover from conditions that \\nmay interfere with nominal mission operations.\\nFault Tolerance: See “Failure Tolerance.”\\nFeasible: Initial evaluations show that the concept \\ncredibly falls within the technical cost and schedule \\nconstraints for the project.\\nFlexibility: The ability of a decision to support more \\nthan one current application.\\nFlight Readiness Review: A review that examines \\ntests, demonstrations, analyses, and audits that \\ndetermine the system’s readiness for a safe and \\nsuccessful flight/launch and for subsequent flight \\noperations. It also ensures that all flight and ground \\nhardware, software, personnel, and procedures are \\noperationally ready.\\nFloat: The amount of time that a task in a project net -\\nwork schedule can be delayed without causing a delay \\nto subsequent tasks or the project completion date.Formulation Phase: The first part of the NASA man -\\nagement life cycle defined in NPR 7120.5 where sys -\\ntem requirements are baselined, feasible concepts are \\ndetermined, a system definition is baselined for the \\nselected concept(s), and preparation is made for pro -\\ngressing to the Implementation Phase.\\nFunctional Analysis: The process of identifying, \\ndescribing, and relating the functions a system should \\nperform to fulfill its goals and objectives.\\nFunctional Baseline (Phase B): The functional base -\\nline is the approved configuration documentation \\nthat describes a system’s or top-level CIs’ performance \\nrequirements (functional, interoperability, and inter -\\nface characteristics) and the verification required \\nto demonstrate the achievement of those specified \\ncharacteristics.\\nFunctional Configuration Audit (FCA): Examines the \\nfunctional characteristics of the configured product \\nand verifies that the product has met, via test results, \\nthe requirements specified in its functional baseline \\ndocumentation approved at the PDR and CDR plus \\nany approved changes thereafter. FCAs will be con -\\nducted on both hardware- and software-configured \\nproducts and will precede the PCA of the configured \\nproduct.\\nFunctional Decomposition: A subfunction under \\nlogical decomposition and design solution definition, \\nit is the examination of a function to identify sub -\\nfunctions necessary for the accomplishment of that \\nfunction and functional relationships and interfaces.\\nFunctional Flow Block Diagram: A block diagram \\nthat defines system functions and the time sequence \\nof functional events.\\nGantt Chart: A bar chart depicting start and finish \\ndates of activities and products in the WBS.183\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix B: Glossary\\nGoal: Goals elaborate on the need and constitute a \\nspecific set of expectations for the system. They fur -\\nther define what we hope to accomplish by address -\\ning the critical issues identified during the problem \\nassessment. Goals need not be in a quantitative or \\nmeasurable form, but they must allow us to assess \\nwhether the system has achieved them.\\nGovernment Mandatory Inspection Points: \\nInspection points required by Federal regulations \\nto ensure 100 percent compliance with safety/mis -\\nsion-critical attributes when noncompliance can \\nresult in loss of life or loss of mission.\\nHealth Assessment: The activity under Fault Man -\\nagement that carries out detection, diagnosis, and \\nidentification of faults and prediction of fault prop -\\nagation states into the future.\\nHealth Monitoring: The activity under Fault \\nManagement that implements system state data col -\\nlection, storage, and reporting though sensing and \\ncommunication.\\nHeritage (or legacy): Refers to the original manufactur -\\ner’s level of quality and reliability that is built into the \\nparts, which have been proven by (1) time in service, \\n(2) number of units in service, (3) mean time between \\nfailure performance, and (4) number of use cycles.\\nHuman-Centered Design: An approach to the devel -\\nopment of interactive systems that focuses on making \\nsystems usable by ensuring that the needs, abilities, \\nand limitations of the human user are met through -\\nout the system’s life cycle.\\nHuman Factors Engineering: The discipline that \\nstudies human-system interfaces and provides \\nrequirements, standards, and guidelines to ensure the \\nhuman component of an integrated system is able to \\nfunction as intended.Human Systems Integration: An interdisciplinary \\nand comprehensive management and technical pro -\\ncess that focuses on the integration of human consid -\\nerations into the system acquisition and development \\nprocesses to enhance human system design, reduce \\nlife cycle ownership cost, and optimize total system \\nperformance.\\nImplementation Phase: The part of the NASA man -\\nagement life cycle defined in NPR 7120.5 where the \\ndetailed design of system products is completed and the \\nproducts to be deployed are fabricated, assembled, inte -\\ngrated, and tested and the products are deployed to their \\ncustomers or users for their assigned use or mission.\\nIncommensurable Costs: Costs that cannot be easily \\nmeasured, such as controlling pollution on launch or \\nmitigating debris.\\nInfluence Diagram: A compact graphical and mathe -\\nmatical representation of a decision state. Its elements are \\ndecision nodes, chance nodes, value nodes, and arrows \\nto indicate the relationships among these elements.\\nInspection: The visual examination of a realized end \\nproduct. Inspection is generally used to verify phys -\\nical design features or specific manufacturer identi -\\nfication. For example, if there is a requirement that \\nthe safety arming pin has a red flag with the words \\n“Remove Before Flight” stenciled on the flag in black \\nletters, a visual inspection of the arming pin flag can \\nbe used to determine if this requirement was met.\\nIntegrated Logistics Support: The management, \\nengineering activities, analysis, and information \\nmanagement associated with design requirements \\ndefinition, material procurement and distribution, \\nmaintenance, supply replacement, transportation, \\nand disposal that are identified by space flight and \\nground systems supportability objectives.184\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix B: Glossary\\nInterface Management Process: The process to assist \\nin controlling product development when efforts are \\ndivided among parties (e.g., Government, contrac -\\ntors, geographically diverse technical teams) and/or \\nto define and maintain compliance among the prod -\\nucts that should interoperate.\\nIterative: Application of a process to the same product \\nor set of products to correct a discovered discrepancy \\nor other variation from requirements. (See “recursive” \\nand “repeatable.”)\\nKey Decision Point: The event at which the decision \\nauthority determines the readiness of a program/proj -\\nect to progress to the next phase of the life cycle (or \\nto the next KDP).\\nKey Event (or Critical Event): See “Critical Event.”\\nKey Performance Parameter: Those capabilities or \\ncharacteristics (typically engineering-based or related to \\nhealth and safety or operational performance) consid -\\nered most essential for successful mission accomplish -\\nment. They characterize the major drivers of operational \\nperformance, supportability, and interoperability.\\nKnowledge Management: A collection of policies, \\nprocesses, and practices relating to the use of intellec -\\ntual- and knowledge-based assets in an organization.\\nLeast-Cost Analysis: A methodology that identifies \\nthe least-cost project option for meeting the techni -\\ncal requirements.\\nLiens: Requirements or tasks not satisfied that have \\nto be resolved within a certain assigned time to allow \\npassage through a control gate to proceed.\\nLife Cycle Cost (LCC): The total of the direct, indi -\\nrect, recurring, nonrecurring, and other related \\nexpenses both incurred and estimated to be incurred in the design, development, verification, production, \\ndeployment, prime mission operation, maintenance, \\nsupport, and disposal of a project, including closeout, \\nbut not extended operations. The LCC of a project or \\nsystem can also be defined as the total cost of own -\\nership over the project or system’s planned life cycle \\nfrom Formulation (excluding Pre–Phase A) through \\nImplementation (excluding extended operations). \\nThe LCC includes the cost of the launch vehicle.\\nLogical Decomposition Models: Mathematical \\nor visual representations of the relationships \\nbetween requirements as identified in the Logical \\nDecomposition Process.\\nLogical Decomposition Process: A process used \\nto improve understanding of the defined technical \\nrequirements and the relationships among the require -\\nments (e.g., functional, behavioral, performance, and \\ntemporal) and to transform the defined set of techni -\\ncal requirements into a set of logical decomposition \\nmodels and their associated set of derived technical \\nrequirements for lower levels of the system and for \\ninput to the Design Solution Definition Process.\\nLogistics (or Integrated Logistics Support): See \\n“Integrated Logistics Support.”\\nLoosely Coupled Program: Programs that address \\nspecific objectives through multiple space flight proj -\\nects of varied scope. While each individual project \\nhas an assigned set of mission objectives, architec -\\ntural and technological synergies and strategies that \\nbenefit the program as a whole are explored during \\nthe formulation process. For instance, Mars orbiters \\ndesigned for more than one Mars year in orbit are \\nrequired to carry a communication system to support \\npresent and future landers.\\nMaintain (with respect to establishment of pro -\\ncesses): The act of planning the process, providing 185\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix B: Glossary\\nresources, assigning responsibilities, training people, \\nmanaging configurations, identifying and involving \\nstakeholders, and monitoring process effectiveness.\\nMaintainability: The measure of the ability of an item \\nto be retained in or restored to specified conditions \\nwhen maintenance is performed by personnel having \\nspecified skill levels, using prescribed procedures and \\nresources, at each prescribed level of maintenance.\\nMargin: The allowances carried in budget, projected \\nschedules, and technical performance parameters \\n(e.g., weight, power, or memory) to account for \\nuncertainties and risks. Margins are allocated in the \\nformulation process based on assessments of risks and \\nare typically consumed as the program/project pro -\\nceeds through the life cycle.\\nMaster Equipment List (MEL): The MEL is a listing \\nof all the parts of a system and includes pertinent \\ninformation such as serial numbers, model numbers, \\nmanufacturer, equipment type, system/element it is \\nlocated within, etc.\\nMeasure of Effectiveness (MOE): A measure by \\nwhich a stakeholder’s expectations are judged in \\nassessing satisfaction with products or systems pro -\\nduced and delivered in accordance with the associated \\ntechnical effort. The MOE is deemed to be critical to \\nnot only the acceptability of the product by the stake -\\nholder but also critical to operational/mission usage. \\nA MOE is typically qualitative in nature or not able \\nto be used directly as a design-to requirement.\\nMeasure of Performance (MOP): A quantitative \\nmeasure that, when met by the design solution, helps \\nensure that a MOE for a product or system will be \\nsatisfied. These MOPs are given special attention \\nduring design to ensure that the MOEs to which \\nthey are associated are met. There are generally two \\nor more measures of performance for each MOE.Metric: The result of a measurement taken over a \\nperiod of time that communicates vital information \\nabout the status or performance of a system, process, \\nor activity. A metric should drive appropriate action.\\nMission: A major activity required to accomplish an \\nAgency goal or to effectively pursue a scientific, tech -\\nnological, or engineering opportunity directly related \\nto an Agency goal. Mission needs are independent of \\nany particular system or technological solution.\\nMission Concept Review: A review that affirms the \\nmission/project need and examines the proposed \\nmission’s objectives and the ability of the concept to \\nfulfill those objectives.\\nMission Definition Review: A life cycle review that \\nevaluates whether the proposed mission/system \\narchitecture is responsive to the program mission/sys -\\ntem functional and performance requirements and \\nrequirements have been allocated to all functional \\nelements of the mission/system.\\nMitigation: An action taken to mitigate the effects \\nof a fault towards achieving existing or redefined \\nsystem goals.\\nModel: A model is a physical, mathematical, or logi -\\ncal representation of reality.\\nNeed: A single statement that drives everything else. \\nIt should relate to the problem that the system is sup -\\nposed to solve, but not be the solution.\\nNonconforming product: Software, hardware, or \\ncombination, either produced, acquired, or in some \\ncombination that is identified as not meeting docu -\\nmented requirements.\\nObjective: Specific target levels of outputs the sys -\\ntem must achieve. Each objective should relate to a 186\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix B: Glossary\\nparticular goal. Generally, objectives should meet \\nfour criteria:\\n1. Specific:  Objectives should aim at results and \\nreflect what the system needs to do, but they don’t \\noutline how to implement the solution. They need \\nto be specific enough to provide clear direction, so \\ndevelopers, customers, and testers can understand \\nthem.\\n2. Measurable:  Objectives need to be quantifiable \\nand verifiable. The project needs to monitor the \\nsystem’s success in achieving each objective.\\n3. Aggressive, but attainable:  Objectives need to be \\nchallenging but reachable, and targets need to be \\nrealistic. At first, objectives “To Be Determined” \\n(TBD) may be included until trade studies \\noccur, operations concepts solidify, or technology \\nmatures. But objectives need to be feasible before \\nstarting to write requirements and design systems.\\n4. Results-oriented:  Objectives need to focus on \\ndesired outputs and outcomes, not on the meth -\\nods used to achieve the target (what, not how).\\nObjective Function (sometimes Cost Function): \\nA mathematical expression of the values of combi -\\nnations of possible outcomes as a single measure of \\ncost-effectiveness.\\nOperational Environment: The environment in \\nwhich the final product will be operated. In the case \\nof space flight hardware/software, it is space. In the \\ncase of ground-based or airborne systems that are not \\ndirected toward space flight, it is the environments \\ndefined by the scope of operations. For software, the \\nenvironment is defined by the operational platform.\\nOperational Readiness Review: A review that exam -\\nines the actual system characteristics and the proce -\\ndures used in the system or product’s operation and ensures that all system and support (flight and ground) \\nhardware, software, personnel, procedures, and user \\ndocumentation accurately reflects the deployed state \\nof the system and are operationally ready.\\nOperations Concept: A description of how the flight \\nsystem and the ground system are used together to \\nensure that the concept of operation is reasonable. This \\nmight include how mission data of interest, such as \\nengineering or scientific data, are captured, returned to \\nEarth, processed, made available to users, and archived \\nfor future reference. (Source: NPR 7120.5)\\nOptimal Solution: A feasible solution that best meets \\ncriteria when balanced at a system level.\\nOther Interested Parties (Stakeholders): A subset of \\n“stakeholders,” other interested parties are groups or \\nindividuals who are not customers of a planned tech -\\nnical effort but may be affected by the resulting prod -\\nuct, the manner in which the product is realized or \\nused, or have a responsibility for providing life cycle \\nsupport services.\\nPeer Review: Independent evaluation by internal or \\nexternal subject matter experts who do not have a \\nvested interest in the work product under review. Peer \\nreviews can be planned, focused reviews conducted \\non selected work products by the producer’s peers to \\nidentify defects and issues prior to that work product \\nmoving into a milestone review or approval cycle.\\nPerformance Standards: Defines what constitutes \\nacceptable performance by the provider. Common \\nmetrics for use in performance standards include cost \\nand schedule.\\nPhysical Configuration Audits (PCA) or configura -\\ntion inspection: The PCA examines the physical con -\\nfiguration of the configured product and verifies that \\nthe product corresponds to the build-to (or code-to) \\nproduct baseline documentation previously approved 187\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix B: Glossary\\nat the CDR plus the approved changes thereafter. \\nPCAs are conducted on both hardware-and soft -\\nware-configured products.\\nPost-Flight Assessment Review: Evaluates how well \\nmission objectives were met during a mission and \\nidentifies all flight and ground system anomalies \\nthat occurred during the flight and determines the \\nactions necessary to mitigate or resolve the anomalies \\nfor future flights of the same spacecraft design.\\nPost-Launch Assessment Review: A review that \\nevaluates the readiness of the spacecraft systems \\nto proceed with full, routine operations after post-\\nlaunch deployment. The review also evaluates the sta -\\ntus of the project plans and the capability to conduct \\nthe mission with emphasis on near-term operations \\nand mission-critical events.\\nPrecedence Diagram: Workflow diagram that places \\nactivities in boxes connected by dependency arrows; \\ntypical of a Gantt chart.\\nPreliminary Design Review: A review that demon -\\nstrates that the preliminary design meets all system \\nrequirements with acceptable risk and within the cost \\nand schedule constraints and establishes the basis for \\nproceeding with detailed design. It will show that the \\ncorrect design option has been selected, interfaces \\nhave been identified, and verification methods have \\nbeen described.\\nProcess: A set of activities used to convert inputs \\ninto desired outputs to generate expected outcomes \\nand satisfy a purpose.\\nProducibility: A system characteristic associated with \\nthe ease and economy with which a completed design \\ncan be transformed (i.e., fabricated, manufactured, or \\ncoded) into a hardware and/or software realization.Product: A part of a system consisting of end prod -\\nucts that perform operational functions and enabling \\nproducts that perform life cycle services related to \\nthe end product or a result of the technical efforts in \\nthe form of a work product (e.g., plan, baseline, or \\ntest result).\\nProduct Baseline (Phase D/E): The product base -\\nline is the approved technical documentation that \\ndescribes the configuration of a CI during the produc -\\ntion, fielding/deployment, and operational support \\nphases of its life cycle. The product baseline describes \\ndetailed physical or form, fit, and function character -\\nistics of a CI; the selected functional characteristics \\ndesignated for production acceptance testing; and the \\nproduction acceptance test requirements.\\nProduct Breakdown Structure: A hierarchical break -\\ndown of the hardware and software products of a \\nprogram/project.\\nProduct Implementation Process: A process used to \\ngenerate a specified product of a product layer through \\nbuying, making, or reusing in a form consistent with \\nthe product life cycle phase exit (success) criteria and \\nthat satisfies the design solution definition-specified \\nrequirements (e.g., drawings, specifications).\\nProduct Integration Process: A process used to \\ntransform the design solution definition into the \\ndesired end product of the product layer through \\nassembly and integration of lower-level validated end \\nproducts in a form that is consistent with the product \\nlife cycle phase exit (success) criteria and that satis -\\nfies the design solution definition requirements (e.g., \\ndrawings, specifications).\\nProduct Realization: The act of making, buying, or \\nreusing a product, or the assembly and integration \\nof lower-level realized products into a new product, \\nas well as the verification and validation that the 188\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix B: Glossary\\nproduct satisfies its appropriate set of requirements \\nand the transition of the product to its customer.\\nProduct Transition Process: A process used to tran -\\nsition a verified and validated end product that has \\nbeen generated by product implementation or prod -\\nuct integration to the customer at the next level in the \\nsystem structure for integration into an end product \\nor, for the top-level end product, transitioned to the \\nintended end user.\\nProduct Validation Process: A process used to con -\\nfirm that a verified end product generated by product \\nimplementation or product integration fulfills (satis -\\nfies) its intended use when placed in its intended envi -\\nronment and to assure that any anomalies discovered \\nduring validation are appropriately resolved prior to \\ndelivery of the product (if validation is done by the \\nsupplier of the product) or prior to integration with \\nother products into a higher-level assembled product \\n(if validation is done by the receiver of the product). \\nThe validation is done against the set of baselined \\nstakeholder expectations.\\nProduct Verification Process: A process used to \\ndemonstrate that an end product generated from \\nproduct implementation or product integration con -\\nforms to its design solution definition requirements \\nas a function of the product life cycle phase and the \\nlocation of the product layer end product in the sys -\\ntem structure.\\nProduction Readiness Review (PRR): A review for \\nprojects developing or acquiring multiple or similar \\nsystems greater than three or as determined by the \\nproject. The PRR determines the readiness of the \\nsystem developers to efficiently produce the required \\nnumber of systems. It ensures that the production \\nplans, fabrication, assembly, integration-enabling \\nproducts, operational support, and personnel are in \\nplace and ready to begin production.Prognosis: The prediction of a system’s future health \\nstates, degradation, and Remaining Useful Life (RUL).\\nProgram: A strategic investment by a mission direc -\\ntorate or mission support office that has a defined \\narchitecture and/or technical approach, require -\\nments, funding level, and a management structure \\nthat initiates and directs one or more projects. A pro -\\ngram defines a strategic direction that the Agency has \\nidentified as critical.\\nProgram/System Definition Review: A review that \\nexamines the proposed program architecture and the \\nflowdown to the functional elements of the system. \\nThe proposed program’s objectives and the concept \\nfor meeting those objectives are evaluated. Key tech -\\nnologies and other risks are identified and assessed. \\nThe baseline program plan, budgets, and schedules \\nare presented.\\nProgram Requirements: The set of requirements \\nimposed on the program office, which are typically \\nfound in the program plan plus derived requirements \\nthat the program imposes on itself.\\nProgram System Requirements Review: A review \\nthat evaluates the credibility and responsiveness of a \\nproposed program requirements/architecture to the \\nmission directorate requirements, the allocation of \\nprogram requirements to the projects, and the matu -\\nrity of the program’s mission/system definition.\\nProgrammatic Requirements: Requirements set \\nby the mission directorate, program, project, and \\nPI, if applicable. These include strategic scientific \\nand exploration requirements, system performance \\nrequirements, and schedule, cost, and similar non -\\ntechnical constraints.\\nProject: A specific investment having defined goals, \\nobjectives, requirements, life cycle cost, a beginning, 189\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix B: Glossary\\nand an end. A project yields new or revised products \\nor services that directly address NASA’s strategic \\nneeds. The products may be produced or the services \\nperformed wholly in-house; by partnerships with \\nGovernment, industry, or academia; or through con -\\ntracts with private industry.\\nProject Plan: The document that establishes the \\nproject’s baseline for implementation, signed by the \\nresponsible program manager, Center Director, proj -\\nect manager, and the MDAA, if required.\\nProject Requirements: The set of requirements \\nimposed on the project and developer, which are typ -\\nically found in the project plan plus derived require -\\nments that the project imposes on itself. It includes \\nidentification of activities and deliverables (end prod -\\nucts and work products) and outputs of the develop -\\nment and operations.\\nPhase Product: An end product that is to be provided \\nas a result of the activities of a given life cycle phase. \\nThe form depends on the phase—a product of early \\nphases might be a simulation or model; a product of \\nlater phases may be the (final) end product itself.\\nProduct Form: A representation of a product that \\ndepends on the development phase, current use, and \\nmaturity. Examples include mock-up, model, engi -\\nneering unit, prototype unit, and flight unit.\\nProduct Realization: The desired output from the appli -\\ncation of the four product realization processes. The \\nform of this product is dependent on the phase of the \\nproduct life cycle and the phase exit (success) criteria.\\nPrototype: The prototype unit demonstrates form, \\nfit, and function at a scale deemed to be representa -\\ntive of the final product operating in its operational \\nenvironment. A subscale test article provides fidelity \\nsufficient to permit validation of analytical mod -\\nels capable of predicting the behavior of full-scale systems in an operational environment. The proto -\\ntype is used to “wring out” the design solution so that \\nexperience gained from the prototype can be fed back \\ninto design changes that will improve the manufac -\\nture, integration, and maintainability of a single flight \\nitem or the production run of several flight items.\\nQuality Assurance: An independent assessment per -\\nformed throughout a product’s life cycle in order to \\nacquire confidence that the system actually produced \\nand delivered is in accordance with its functional, \\nperformance, and design requirements.\\nRealized Product: The end product that has been \\nimplemented/integrated, verified, validated, and \\ntransitioned to the next product layer.\\nRecovery: An action taken to restore the functions \\nnecessary to achieve existing or redefined system \\ngoals after a fault/failure occurs.\\nRecursive: Value is added to the system by the \\nrepeated application of processes to design next low -\\ner-layer system products or to realize next upper-layer \\nend products within the system structure. This also \\napplies to repeating the application of the same pro -\\ncesses to the system structure in the next life cycle \\nphase to mature the system definition and satisfy \\nphase exit (success) criteria.\\nRelevant Stakeholder: A subset of the term “stake -\\nholder” that applies to people or roles that are des -\\nignated in a plan for stakeholder involvement. Since \\n“stakeholder” may describe a very large number of \\npeople, a lot of time and effort would be consumed \\nby attempting to deal with all of them. For this rea -\\nson, “relevant stakeholder” is used in most practice \\nstatements to describe the people identified to con -\\ntribute to a specific task.\\nRelevant Environment: Not all systems, subsys -\\ntems, and/or components need to be operated in 190\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix B: Glossary\\nthe operational environment in order to satisfacto -\\nrily address performance margin requirements or \\nstakeholder expectations. Consequently, the relevant \\nenvironment is the specific subset of the operational \\nenvironment that is required to demonstrate critical \\n“at risk” aspects of the final product performance in \\nan operational environment.\\nReliability: The measure of the degree to which a \\nsystem ensures mission success by functioning prop -\\nerly over its intended life. It has a low and acceptable \\nprobability of failure, achieved through simplicity, \\nproper design, and proper application of reliable parts \\nand materials. In addition to long life, a reliable sys -\\ntem is robust and fault tolerant.\\nRepeatable: A characteristic of a process that can be \\napplied to products at any level of the system struc -\\nture or within any life cycle phase.\\nRequirement: The agreed-upon need, desire, want, \\ncapability, capacity, or demand for personnel, equip -\\nment, facilities, or other resources or services by \\nspecified quantities for specific periods of time or \\nat a specified time expressed as a “shall” statement. \\nAcceptable form for a requirement statement is indi -\\nvidually clear, correct, feasible to obtain, unambig -\\nuous in meaning, and can be validated at the level \\nof the system structure at which it is stated. In pairs \\nof requirement statements or as a set, collectively, \\nthey are not redundant, are adequately related with \\nrespect to terms used, and are not in conflict with \\none another.\\nRequirements Allocation Sheet: Documents the \\nconnection between allocated functions, allocated \\nperformance, and the physical system.\\nRequirements Management Process: A process \\nused to manage the product requirements identified, \\nbaselined, and used in the definition of the products of each product layer during system design. It pro -\\nvides bidirectional traceability back to the top prod -\\nuct layer requirements and manages the changes to \\nestablished requirement baselines over the life cycle \\nof the system products.\\nRisk: In the context of mission execution, risk is the \\npotential for performance shortfalls that may be real -\\nized in the future with respect to achieving explicitly \\nestablished and stated performance requirements. \\nThe performance shortfalls may be related to any one \\nor more of the following mission execution domains: \\n(1) safety, (2) technical, (3) cost, and (4) schedule. \\n(Source: NPR 8000.4, Agency Risk Management \\nProcedural Requirements)\\nRisk Assessment: An evaluation of a risk item that \\ndetermines (1) what can go wrong, (2) how likely it \\nis to occur, (3) what the consequences are, and (4) \\nwhat the uncertainties associated with the likelihood \\nand consequences are, and 5) what the mitigation \\nplans are.\\nRisk-Informed Decision Analysis Process: A five-\\nstep process focusing first on objectives and next \\non developing decision alternatives with those \\nobjectives clearly in mind and/or using decision \\nalternatives that have been developed under other \\nsystems engineering processes. The later steps of the \\nprocess interrelate heavily with the Technical Risk \\nManagement Process.\\nRisk Management: Risk management includes \\nRisk-Informed Decision-Making (RIDM) and \\nContinuous Risk Management (CRM) in an inte -\\ngrated framework. RIDM informs systems engi -\\nneering decisions through better use of risk and \\nuncertainty information in selecting alternatives and \\nestablishing baseline requirements. CRM manages \\nrisks over the course of the development and the \\nImplementation Phase of the life cycle to ensure that 191\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix B: Glossary\\nsafety, technical, cost, and schedule requirements \\nare met. This is done to foster proactive risk man -\\nagement, to better inform decision-making through \\nbetter use of risk information, and then to more effec -\\ntively manage Implementation risks by focusing the \\nCRM process on the baseline performance require -\\nments emerging from the RIDM process. (Source: \\nNPR 8000.4, Agency Risk Management Procedural \\nRequirements) These processes are applied at a level \\nof rigor commensurate with the complexity, cost, and \\ncriticality of the program.\\nSafety: Freedom from those conditions that can \\ncause death, injury, occupational illness, damage to \\nor loss of equipment or property, or damage to the \\nenvironment.\\nSearch Space (or Alternative Space): The envelope \\nof concept possibilities defined by design constraints \\nand parameters within which alternative concepts \\ncan be developed and traded off.\\nSingle-Project Programs: Programs that tend to \\nhave long development and/or operational lifetimes, \\nrepresent a large investment of Agency resources, and \\nhave contributions from multiple organizations/agen -\\ncies. These programs frequently combine program \\nand project management approaches, which they \\ndocument through tailoring.\\nSoftware: Computer programs, procedures, rules, \\nand associated documentation and data pertaining \\nto the development and operation of a computer sys -\\ntem. Software also includes Commercial Off-The-\\nShelf (COTS), Government Off-The-Shelf (GOTS), \\nModified Off-The-Shelf (MOTS), embedded soft -\\nware, reuse, heritage, legacy, autogenerated code, \\nfirmware, and open source software components.\\nNote 1:  For purposes of the NASA Software Release \\nprogram only, the term “software,” as redefined in NPR 2210.1, Release of NASA Software, does not include \\ncomputer databases or software documentation.\\nNote 2: Definitions for the terms COTS, GOTS, \\nheritage software, MOTS, legacy software, software \\nreuse, and classes of software are provided in NPR \\n7150.2, NASA Software Engineering Requirements. \\n(Source: NPD 7120.4, NASA Engineering and \\nProgram/Project Management Policy)\\nSolicitation: The vehicle by which information is \\nsolicited from contractors for the purpose of award -\\ning a contract for products or services. Any request \\nto submit offers or quotations to the Government. \\nSolicitations under sealed bid procedures are called \\n“invitations for bids.” Solicitations under negoti -\\nated procedures are called “requests for proposals.” \\nSolicitations under simplified acquisition procedures \\nmay require submission of either a quotation or \\nan offer.\\nSpecification: A document that prescribes com -\\npletely, precisely, and verifiably the requirements, \\ndesign, behavior, or characteristics of a system or \\nsystem component. In NPR 7123.1, “specification” is \\ntreated as a “requirement.”\\nStakeholder: A group or individual who is affected \\nby or has an interest or stake in a program or project. \\nThere are two main classes of stakeholders. See “cus -\\ntomers” and “other interested parties.”\\nStakeholder Expectations: A statement of needs, \\ndesires, capabilities, and wants that are not expressed \\nas a requirement (not expressed as a “shall” statement) \\nis referred to as an “expectation.” Once the set of \\nexpectations from applicable stakeholders is collected, \\nanalyzed, and converted into a “shall” statement, the \\nexpectation becomes a requirement. Expectations \\ncan be stated in either qualitative (non-measurable) \\nor quantitative (measurable) terms. Requirements are 192\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix B: Glossary\\nalways stated in quantitative terms. Expectations can \\nbe stated in terms of functions, behaviors, or con -\\nstraints with respect to the product being engineered \\nor the process used to engineer the product.\\nStakeholder Expectations Definition Process: A \\nprocess used to elicit and define use cases, scenarios, \\nconcept of operations, and stakeholder expectations \\nfor the applicable product life cycle phases and prod -\\nuct layer. The baselined stakeholder expectations are \\nused for validation of the product layer end product.\\nStanding Review Board: The board responsible for \\nconducting independent reviews (life-cycle and spe -\\ncial) of a program or project and providing objective, \\nexpert judgments to the convening authorities. The \\nreviews are conducted in accordance with approved \\nTerms of Reference (ToR) and life cycle requirements \\nper NPR 7123.1.\\nState Diagram: A diagram that shows the flow in the \\nsystem in response to varying inputs in order to char -\\nacterize the behavior of the system.\\nSuccess Criteria: Specific accomplishments that \\nneed to be satisfactorily demonstrated to meet the \\nobjectives of a technical review so that a technical \\neffort can progress further in the life cycle. Success \\ncriteria are documented in the corresponding techni -\\ncal review plan. Formerly referred to as “exit” criteria, \\na term still used in some NPDs/NPRs.\\nSurveillance : The monitoring of a contractor’s activ -\\nities (e.g., status meetings, reviews, audits, site visits) \\nfor progress and production and to demonstrate fis -\\ncal responsibility, ensure crew safety and mission suc -\\ncess, and determine award fees for extraordinary (or \\npenalty fees for substandard) contract execution.\\nSystem: (1) The combination of elements that func -\\ntion together to produce the capability to meet a need. The elements include all hardware, software, \\nequipment, facilities, personnel, processes, and pro -\\ncedures needed for this purpose. (2) The end product \\n(which performs operational functions) and enabling \\nproducts (which provide life cycle support services to \\nthe operational end products) that make up a system.\\nSystem Acceptance Review: The SAR verifies the \\ncompleteness of the specific end products in relation \\nto their expected maturity level, assesses compliance \\nto stakeholder expectations, and ensures that the \\nsystem has sufficient technical maturity to authorize \\nits shipment to the designated operational facility or \\nlaunch site.\\nSystem Definition Review: The Mission/System \\nDefinition Review (MDR/SDR) evaluates whether \\nthe proposed mission/system architecture is respon -\\nsive to the program mission/system functional and \\nperformance requirements and requirements have \\nbeen allocated to all functional elements of the mis -\\nsion/system. This review is used for projects and for \\nsingle-project programs.\\nSystem Integration Review: A SIR ensures that seg -\\nments, components, and subsystems are on schedule \\nto be integrated into the system and that integration \\nfacilities, support personnel, and integration plans and \\nprocedures are on schedule to support integration.\\nSystem Requirements Review: For a program, the \\nSRR is used to ensure that its functional and per -\\nformance requirements are properly formulated and \\ncorrelated with the Agency and mission directorate \\nstrategic objectives.\\nFor a system/project, the SRR evaluates whether the \\nfunctional and performance requirements defined for \\nthe system are responsive to the program’s require -\\nments and ensures that the preliminary project plan \\nand requirements will satisfy the mission.193\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix B: Glossary\\nSystem Safety Engineering: The application of engi -\\nneering and management principles, criteria, and \\ntechniques to achieve acceptable mishap risk within \\nthe constraints of operational effectiveness and suit -\\nability, time, and cost throughout all phases of the \\nsystem life cycle.\\nSystem Structure: A system structure is made up \\nof a layered structure of product-based WBS mod -\\nels. (See “Work Breakdown Structure” and Product \\nBreakdown Structure.”)\\nSystems Approach: The application of a systematic, \\ndisciplined engineering approach that is quantifiable, \\nrecursive, iterative, and repeatable for the develop -\\nment, operation, and maintenance of systems inte -\\ngrated into a whole throughout the life cycle of a \\nproject or program.\\nSystems Engineering (SE) Engine: The SE model \\nshown in Figure 2.1-1 that provides the 17 technical \\nprocesses and their relationships with each other. The \\nmodel is called an “SE engine” in that the appropriate \\nset of processes is applied to the products being engi -\\nneered to drive the technical effort.\\nSystems Engineering Management Plan (SEMP): \\nThe SEMP identifies the roles and responsibility \\ninterfaces of the technical effort and specifies how \\nthose interfaces will be managed. The SEMP is the \\nvehicle that documents and communicates the tech -\\nnical approach, including the application of the com -\\nmon technical processes; resources to be used; and \\nthe key technical tasks, activities, and events along \\nwith their metrics and success criteria.\\nTailoring: A process used to adjust or seek relief from \\na prescribed requirement to accommodate the needs \\nof a specific task or activity (e.g., program or proj -\\nect). The tailoring process results in the generation of deviations and waivers depending on the timing \\nof the request.\\nOR\\nThe process used to seek relief from NPR 7123.1 \\nrequirements consistent with program or project \\nobjectives, allowable risk, and constraints.\\nTechnical Assessment Process: A process used to \\nhelp monitor progress of the technical effort and \\nprovide status information for support of the system \\ndesign, product realization, and technical manage -\\nment processes. A key aspect of the process is con -\\nducting life cycle and technical reviews throughout \\nthe system life cycle.\\nTechnical Cost Estimate: The cost estimate of the \\ntechnical work on a project created by the techni -\\ncal team based on its understanding of the system \\nrequirements and operational concepts and its vision \\nof the system architecture.\\nTechnical Data Management Process: A process \\nused to plan for, acquire, access, manage, protect, \\nand use data of a technical nature to support the total \\nlife cycle of a system. This process is used to cap -\\nture trade studies, cost estimates, technical analyses, \\nreports, and other important information.\\nTechnical Data Package: An output of the Design \\nSolution Definition Process, it evolves from phase to \\nphase, starting with conceptual sketches or models \\nand ending with complete drawings, parts list, and \\nother details needed for product implementation or \\nproduct integration.\\nTechnical Measures: An established set of measures \\nbased on the expectations and requirements that will \\nbe tracked and assessed to determine overall system or 194\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix B: Glossary\\nproduct effectiveness and customer satisfaction. Com -\\nmon terms for these measures are Measures Of Effec -\\ntiveness (MOEs), Measures Of Performance (MOPs), \\nand Technical Performance Measures (TPMs).\\nTechnical Performance Measures: A set of perfor -\\nmance measures that are monitored by comparing \\nthe current actual achievement of the parameters \\nwith that anticipated at the current time and on \\nfuture dates. TPMs are used to confirm progress and \\nidentify deficiencies that might jeopardize meeting a \\nsystem requirement. Assessed parameter values that \\nfall outside an expected range around the anticipated \\nvalues indicate a need for evaluation and corrective \\naction. Technical performance measures are typi -\\ncally selected from the defined set of Measures Of \\nPerformance (MOPs).\\nTechnical Planning Process: A process used to plan \\nfor the application and management of each common \\ntechnical process. It is also used to identify, define, \\nand plan the technical effort applicable to the prod -\\nuct life cycle phase for product layer location within \\nthe system structure and to meet project objectives \\nand product life cycle phase exit (success) criteria. A \\nkey document generated by this process is the SEMP.\\nTechnical Requirements: A set of requirements imposed \\non the end products of the system, including the system \\nitself. Also referred to as “product requirements.”\\nTechnical Requirements Definition Process: A pro -\\ncess used to transform the stakeholder expectations \\ninto a complete set of validated technical require -\\nments expressed as “shall” statements that can be \\nused for defining a design solution for the Product \\nBreakdown Structure (PBS) model and related \\nenabling products.\\nTechnical Risk: Risk associated with the achievement \\nof a technical goal, criterion, or objective. It applies to undesired consequences related to technical perfor -\\nmance, human safety, mission assets, or environment.\\nTechnical Risk Management Process: A process \\nused to make risk-informed decisions and examine, \\non a continuing basis, the potential for deviations \\nfrom the project plan and the consequences that \\ncould result should they occur.\\nTechnical Team: A group of multidisciplinary indi -\\nviduals with appropriate domain knowledge, experi -\\nence, competencies, and skills who are assigned to a \\nspecific technical task.\\nTechnology Readiness Assessment Report: A doc -\\nument required for transition from Phase B to Phase \\nC/D demonstrating that all systems, subsystems, and \\ncomponents have achieved a level of technological \\nmaturity with demonstrated evidence of qualification \\nin a relevant environment.\\nTechnology Assessment: A systematic process that \\nascertains the need to develop or infuse technologi -\\ncal advances into a system. The technology assess -\\nment process makes use of basic systems engineering \\nprinciples and processes within the framework of the \\nProduct Breakdown Structure (PBS). It is a two-step \\nprocess comprised of (1) the determination of the cur -\\nrent technological maturity in terms of Technology \\nReadiness Levels (TRLs) and (2) the determination \\nof the difficulty associated with moving a technol -\\nogy from one TRL to the next through the use of the \\nAdvancement Degree of Difficulty Assessment (AD2).\\nTechnology Development Plan: A document required \\nfor transition from Phase A to Phase B identifying \\ntechnologies to be developed, heritage systems to be \\nmodified, alternative paths to be pursued, fallback \\npositions and corresponding performance descopes, \\nmilestones, metrics, and key decision points. It is \\nincorporated in the preliminary project plan.195\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix B: Glossary\\nTechnology Maturity Assessment: A process to \\ndetermine a system’s technological maturity based on \\nTechnology Readiness Levels (TRLs).\\nTechnology Readiness Level: Provides a scale against \\nwhich to measure the maturity of a technology. TRLs \\nrange from 1, basic technology research, to 9, systems \\ntest, launch, and operations. Typically, a TRL of 6 \\n(i.e., technology demonstrated in a relevant environ -\\nment) is required for a technology to be integrated \\ninto an SE process.\\nTest: The use of a realized end product to obtain \\ndetailed data to verify or validate performance or to \\nprovide sufficient information to verify or validate \\nperformance through further analysis.\\nTest Readiness Review: A review that ensures that \\nthe test article (hardware/software), test facility, sup -\\nport personnel, and test procedures are ready for test -\\ning and data acquisition, reduction, and control.\\nThreshold Requirements: A minimum acceptable set \\nof technical and project requirements; the set could \\nrepresent the descope position of the project.\\nTightly Coupled Programs: Programs with multi -\\nple projects that execute portions of a mission(s). No \\nsingle project is capable of implementing a complete \\nmission. Typically, multiple NASA Centers con -\\ntribute to the program. Individual projects may be \\nmanaged at different Centers. The program may also \\ninclude contributions from other agencies or interna -\\ntional partners.\\nTraceability: A discernible association among two or \\nmore logical entities such as requirements, system ele -\\nments, verifications, or tasks.\\nTrade Study: A means of evaluating system designs \\nby devising alternative means to meet functional requirements, evaluating these alternatives in terms of \\nthe measures of effectiveness and system cost, rank -\\ning the alternatives according to appropriate selection \\ncriteria, dropping less promising alternatives, and \\nproceeding to the next level of resolution, if needed.\\nTrade Study Report: A report written to document \\na trade study. It should include: the system under \\nanalysis; system goals, objectives (or requirements, as \\nappropriate to the level of resolution), and constraints; \\nmeasures and measurement methods (models) used; \\nall data sources used; the alternatives chosen for anal -\\nysis; computational results, including uncertainty \\nranges and sensitivity analyses performed; the selec -\\ntion rule used; and the recommended alternative.\\nTrade Tree: A representation of trade study alter -\\nnatives in which each layer represents some system \\naspect that needs to be treated in a trade study to \\ndetermine the best alternative.\\nTransition: The act of delivery or moving of a product \\nfrom one location to another. This act can include \\npackaging, handling, storing, moving, transporting, \\ninstalling, and sustainment activities.\\nUncoupled Programs: Programs implemented \\nunder a broad theme and/or a common program \\nimplementation concept, such as providing frequent \\nflight opportunities for cost-capped projects selected \\nthrough AO or NASA Research Announcements. \\nEach such project is independent of the other projects \\nwithin the program.\\nUtility: A measure of the relative value gained from an \\nalternative. The theoretical unit of measurement for \\nutility is the “util.”\\nValidated Requirements: A set of requirements that \\nare well formed (clear and unambiguous), complete \\n(agree with customer and stakeholder needs and 196\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix B: Glossary\\nexpectations), consistent (conflict free), and individ -\\nually verifiable and traceable to a higher level require -\\nment or goal.\\nValidation (of a product): The process of showing \\nproof that the product accomplishes the intended \\npurpose based on stakeholder expectations and the \\nConcept of Operations. May be determined by a \\ncombination of test, analysis, demonstration, and \\ninspection. (Answers the question, “Am I building \\nthe right product?”)\\nVariance: In program control terminology, a differ -\\nence between actual performance and planned costs \\nor schedule status.\\nVerification (of a product): Proof of compliance with \\nspecifications. Verification may be determined by \\ntest, analysis, demonstration, or inspection or a com -\\nbination thereof. (Answers the question, “Did I build \\nthe product right?”)\\nWaiver: A documented authorization releasing a pro -\\ngram or project from meeting a requirement after the requirement is put under configuration control at the \\nlevel the requirement will be implemented.\\nWork Breakdown Structure (WBS): A product-ori -\\nented hierarchical division of the hardware, software, \\nservices, and data required to produce the program/\\nproject’s end product(s) structured according to the \\nway the work will be performed, reflecting the way \\nin which program/project costs, schedule, technical, \\nand risk data are to be accumulated, summarized, \\nand reported.\\nWBS Model: A WBS model describes a system that \\nconsists of end products and their subsystems (which \\nperform the operational functions of the system), the \\nsupporting or enabling products, and any other work \\nproducts (plans, baselines) required for the develop -\\nment of the system.\\nWorkflow Diagram: A scheduling chart that \\nshows activities, dependencies among activities, \\nand milestones.197\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nAppendix C:  Ho w to Write a Good Requirement— \\nChecklist\\nC.1 Use of Correct Terms\\n \\x86Shall = requirement\\n \\x86Will = facts or declaration of purpose\\n \\x86Should = goal\\nC.2 Editorial Checklist\\nPersonnel Requirement\\n \\x86The requirement is in the form “responsible party \\nshall perform such and such.” In other words, use the active, rather than the passive voice. A require -\\nment should state who shall (do, perform, provide, weigh, or other verb) followed by a description of what should be performed.\\nProduct Requirement\\n \\x86The requirement is in the form “product ABC shall XYZ.” A requirement should state “The product shall” (do, perform, provide, weigh, or other verb) followed by a description of what should be done.\\n \\x86The requirement uses consistent terminology to refer to the product and its lower-level entities.\\n \\x86Complete with tolerances for qualitative/perfor -\\nmance values (e.g., less than, greater than or equal to, plus or minus, 3 sigma root sum squares).\\n \\x86Is the requirement free of implementation? (Requirements should state WHAT is needed, NOT HOW to provide it; i.e., state the problem not the solution. Ask, “Why do you need the requirement?” The answer may point to the real requirement.)\\n \\x86Free of descriptions of operations? (Is this a need the product should satisfy or an activity involv -\\ning the product? Sentences like “The operator shall…” are almost always operational statements not requirements.)\\nExample Product Requirements\\n \\x86The system shall operate at a power level of…\\n \\x86The software shall acquire data from the…\\n \\x86The structure shall withstand loads of…\\n \\x86The hardware shall have a mass of…\\nC.3 General Goodness Checklist\\n \\x86The requirement is grammatically correct.\\n \\x86The requirement is free of typos, misspellings, and punctuation errors.\\n \\x86The requirement complies with the project’s tem -\\nplate and style rules.\\n \\x86The requirement is stated positively (as opposed to negatively, i.e., “shall not”).\\n \\x86The use of “To Be Determined” (TBD) values should be minimized. It is better to use a best 198\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix C: How to Write a Good Requirement— Checklist\\nestimate for a value and mark it “To Be Resolved” \\n(TBR) with the rationale along with what should \\nbe done to eliminate the TBR, who is responsi -\\nble for its elimination, and by when it should be \\neliminated.\\n \\x86The requirement is accompanied by an intel -\\nligible rationale, including any assumptions. \\nCan you validate (concur with) the assump -\\ntions? Assumptions should be confirmed before \\nbaselining.\\n \\x86The requirement is located in the proper section of \\nthe document (e.g., not in an appendix).\\nC.4 Requirements Validation \\nChecklist\\nClarity\\n \\x86Are the requirements clear and unambiguous? \\n(Are all aspects of the requirement understand -\\nable and not subject to misinterpretation? Is the \\nrequirement free from indefinite pronouns (this, \\nthese) and ambiguous terms (e.g., “as appropri -\\nate,” “etc.,” “and/or,” “but not limited to”)?)\\n \\x86Are the requirements concise and simple?\\n \\x86Do the requirements express only one thought per \\nrequirement statement, a stand-alone statement as \\nopposed to multiple requirements in a single state -\\nment, or a paragraph that contains both require -\\nments and rationale?\\n \\x86Does the requirement statement have one subject \\nand one predicate?\\nCompleteness\\n \\x86Are requirements stated as completely as possible? \\nHave all incomplete requirements been captured as TBDs or TBRs and a complete listing of them \\nmaintained with the requirements?\\n \\x86Are any requirements missing? For example, \\nhave any of the following requirements areas \\nbeen overlooked: functional, performance, inter -\\nface, environment (development, manufacturing, \\ntest, transport, storage, and operations), facility \\n(manufacturing, test, storage, and operations), \\ntransportation (among areas for manufacturing, \\nassembling, delivery points, within storage facil -\\nities, loading), training, personnel, operability, \\nsafety, security, appearance and physical charac -\\nteristics, and design.\\n \\x86Have all assumptions been explicitly stated?\\nCompliance\\n \\x86Are all requirements at the correct level (e.g., sys -\\ntem, segment, element, subsystem)?\\n \\x86Are requirements free of implementation specif -\\nics? (Requirements should state what is needed, \\nnot how to provide it.)\\n \\x86Are requirements free of descriptions of opera -\\ntions? (Don’t mix operation with requirements: \\nupdate the ConOps instead.)\\n \\x86Are requirements free of personnel or task assign -\\nments? (Don’t mix personnel/task with product \\nrequirements: update the SOW or Task Order \\ninstead.)\\nConsistency\\n \\x86Are the requirements stated consistently without \\ncontradicting themselves or the requirements of \\nrelated systems?\\n \\x86Is the terminology consistent with the user and \\nsponsor’s terminology? With the project glossary?199\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix C: How to Write a Good Requirement— Checklist\\n \\x86Is the terminology consistently used throughout \\nthe document? Are the key terms included in the \\nproject’s glossary?\\nTraceability\\n \\x86Are all requirements needed? Is each requirement \\nnecessary to meet the parent requirement? Is each \\nrequirement a needed function or characteristic? \\nDistinguish between needs and wants. If it is not \\nnecessary, it is not a requirement. Ask, “What is \\nthe worst that could happen if the requirement \\nwas not included?”\\n \\x86Are all requirements (functions, structures, and \\nconstraints) bidirectionally traceable to high -\\ner-level requirements or mission or system-of-in -\\nterest scope (i.e., need(s), goals, objectives, \\nconstraints, or concept of operations)?\\n \\x86Is each requirement stated in such a manner that it \\ncan be uniquely referenced (e.g., each requirement \\nis uniquely numbered) in subordinate documents?\\nCorrectness\\n \\x86Is each requirement correct?\\n \\x86Is each stated assumption correct? Assumptions \\nshould be confirmed before the document can be \\nbaselined.\\n \\x86Are the requirements technically feasible?\\nFunctionality\\n \\x86Are all described functions necessary and together \\nsufficient to meet mission and system goals and \\nobjectives?\\nPerformance\\n \\x86Are all required performance specifications and \\nmargins listed (e.g., consider timing, throughput, \\nstorage size, latency, accuracy and precision)? \\x86Is each performance requirement realistic?\\n \\x86Are the tolerances overly tight? Are the tolerances \\ndefendable and cost-effective? Ask, “What is the \\nworst thing that could happen if the tolerance was \\ndoubled or tripled?”\\nInterfaces\\n \\x86Are all external interfaces clearly defined?\\n \\x86Are all internal interfaces clearly defined?\\n \\x86Are all interfaces necessary, sufficient, and consis -\\ntent with each other?\\nMaintainability\\n \\x86Have the requirements for maintainability of the \\nsystem been specified in a measurable, verifiable \\nmanner?\\n \\x86Are requirements written so that ripple effects \\nfrom changes are minimized (i.e., requirements \\nare as weakly coupled as possible)?\\nReliability\\n \\x86Are clearly defined, measurable, and verifiable \\nreliability requirements specified?\\n \\x86Are there error detection, reporting, handling, \\nand recovery requirements?\\n \\x86Are undesired events (e.g., single-event upset, data \\nloss or scrambling, operator error) considered and \\ntheir required responses specified?\\n \\x86Have assumptions about the intended sequence \\nof functions been stated? Are these sequences \\nrequired?\\n \\x86Do these requirements adequately address the \\nsurvivability after a software or hardware fault of 200\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix C: How to Write a Good Requirement— Checklist\\nthe system from the point of view of hardware, \\nsoftware, operations, personnel and procedures?\\nVerifiability/Testability\\n \\x86Can the system be tested, demonstrated, \\ninspected, or analyzed to show that it satisfies \\nrequirements? Can this be done at the level of the \\nsystem at which the requirement is stated? Does \\na means exist to measure the accomplishment of \\nthe requirement and verify compliance? Can the \\ncriteria for verification be stated?\\n \\x86Are the requirements stated precisely to facilitate \\nspecification of system test success criteria and \\nrequirements? \\x86Are the requirements free of unverifiable terms \\n(e.g., flexible, easy, sufficient, safe, ad hoc, ade -\\nquate, accommodate, user-friendly, usable, when \\nrequired, if required, appropriate, fast, portable, \\nlight-weight, small, large, maximize, minimize, \\nsufficient, robust, quickly, easily, clearly, other \\n“ly” words, other “ize” words)?\\nData Usage\\n \\x86Where applicable, are “don’t care” conditions \\ntruly “don’t care”? (“Don’t care” values identify \\ncases when the value of a condition or flag is irrel -\\nevant, even though the value may be important \\nfor other cases.) Are “don’t care” conditions values \\nexplicitly stated? (Correct identification of “don’t \\ncare” values may improve a design’s portability.)201\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nAppendix D: Requirements Verification Matrix\\nWhen developing requirements, it is important to \\nidentify an approach for verifying the requirements. This appendix provides an example matrix that defines how all the requirements are verified. Only “shall” requirements should be included in these matrices. The matrix should identify each “shall” by unique identifier and be definitive as to the source, i.e., document from which the requirement is taken. This matrix could be divided into multiple matri -\\nces (e.g., one for each requirements document) to delineate sources of requirements depending on the project. The example is shown to provide suggested guidelines for the minimum information that should be included in the verification matrix.\\nNOTE: \\nSee Appendix I  for an outline of the \\nVerification and Validation Plan. The matrix shown \\nhere ( TABLE D-1 ) is Appendix C in that outline.202\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix D: Requirements Verification MatrixTABLE D-1 Requirements Verification Matrix \\nRequire -\\nment No.Document Para -\\ngraphShall \\nState -\\nmentVerifi -\\ncation \\nSuccess \\nCriteriaVerifi -\\ncation \\nMethodFacility or \\nLabPhaseaAccep -\\ntance \\nRequire -\\nment?Preflight \\nAccep -\\ntance?Perform -\\ning Orga -\\nnizationResults\\nUnique iden -\\ntifier or each \\nrequirementDocument \\nnumber the \\nrequirement \\nis contained \\nwithinParagraph \\nnumber of the \\nrequirementText (within \\nreason) of the \\nrequirement, \\ni.e., the \\n“shall”Success \\ncriteria for the \\nrequirementVerification \\nmethod \\nfor the \\nrequirement \\n(analysis, \\ninspection, \\ndemonstration, \\ntest)Facility or \\nlaboratory \\nused to per -\\nform the ver -\\nification and \\nvalidation.Phase in \\nwhich the \\nverification \\nand valida -\\ntion will be \\nperformed.Indicate \\nwhether this \\nrequirement \\nis also ver -\\nified during \\ninitial accep -\\ntance testing \\nof each unit.Indicate \\nwhether this \\nrequirement \\nis also ver -\\nified during \\nany pre-flight \\nor recurring \\nacceptance \\ntesting of \\neach unitOrganization \\nresponsible \\nfor per -\\nforming the \\nverificationIndicate \\ndocuments \\nthat contain \\nthe objective \\nevidence that \\nrequirement \\nwas satisfied\\nP-1 xxx 3.2.1.1 \\nCapability: \\nSupport \\nUplinked Data \\n(LDR)System X \\nshall provide \\na max. \\nground-to-\\nstation uplink \\nof…1. System \\nX locks to \\nforward link \\nat the min \\nand max \\ndata rate \\ntolerances\\n2. System \\nX locks to \\nthe forward \\nlink at the \\nmin and max \\noperating \\nfrequency \\ntolerancesTest xxx 5 Yes No xxx TPS  \\nxxxx\\nP-i xxx Other \\nparagraphsOther “shalls” \\nin PTRSOther criteria xxx xxx xxx Yes/No Yes/No xxx Memo xxx\\nS-i or other \\nunique \\ndesignatorxxxxx (other \\nspecs, ICDs, \\netc.)Other \\nparagraphsOther \\n“shalls” in \\nspecs, ICDs, \\netc.Other criteria xxx xxx xxx Yes/No Yes/No xxx Report xxx\\na  Phases defined as: (1) Pre-Declared Development, (2) Formal Box-Level Functional, (3) Formal Box-Level Environmental, (4) Formal System-Level Environmental, (5) \\nFormal System-Level Functional, (6) Formal End-to-End Functional, (7) Integrated Vehicle Functional,  \\n(8) On-Orbit Functional  .203\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nAppendix E:  C reating the Validation Plan with a \\nValidation Requirements Matrix\\nNOTE:  See Appendix I  for an outline of the \\nVerification and Validation Plan. The matrix shown \\nhere ( TABLE E-1 ) is Appendix D  in that outline.\\nWhen developing requirements, it is important to \\nidentify a validation approach for how additional val -\\nidation evaluation, testing, analysis, or other demon -\\nstrations will be performed to ensure customer/sponsor satisfaction.\\nThere are a number of sources to draw from for creat -\\ning the validation plan:\\n• ConOps\\n• Stakeholder/customer needs, goals, and objectives \\ndocumentation\\n• Rationale statements for requirements and in ver -\\nification requirements\\n• Lessons learned database\\n• System architecture modeling\\n• Test-as-you-fly design goals and constraints\\n• SEMP, HSIP, V&V plansValidation products can take the form of a wide range of deliverables, including:\\n• Stakeholder evaluation and feedback\\n• Peer reviews\\n• Physical models of all fidelities\\n• Simulations\\n• Virtual modeling\\n• Tests\\n• Fit-checks\\n• Procedure dry-runs\\n• Integration activities (to inform on-orbit mainte -\\nnance procedures)\\n• Phase-level review solicitation and feedback\\nParticular attention should be paid to the planning \\nfor life cycle phase since early validation can have a \\nprofound impact on the design and cost in the later life cycle phases.\\nTABLE E-1 shows an example validation matrix.204\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix E: Creating the Validation Plan with a Validation Requirements Matrix\\nTABLE E-1 Validation Requirements Matrix\\nValidation \\nProduct #Activity Objective Validation \\nMethodFacility \\nor LabPhase Performing \\nOrganizationResults\\nUnique \\nidentifier for \\nvalidation \\nproductDescribe \\nevaluation \\nby the \\ncustomer/\\nsponsor \\nthat will be \\nperformedWhat is to be \\naccomplished \\nby the \\ncustomer/\\nsponsor \\nevaluationValidation \\nmethod \\nfor the \\nrequirement \\n(analysis, \\ninspection, \\ndemonstra -\\ntion, or test)Facility or \\nlaboratory \\nused to \\nperform \\nthe \\nvalidationPhase in \\nwhich the \\nverification/\\nvalidation \\nwill be \\nperformedaOrganization \\nresponsible for \\ncoordinating \\nthe validation \\nactivityIndicate the \\nobjective \\nevidence \\nthat \\nvalidation \\nactivity \\noccurred\\n1 Customer/\\nsponsor \\nwill \\nevaluate \\nthe \\ncandidate \\ndisplays1. Ensure \\nlegibility is \\nacceptable 2. \\nEnsure overall \\nappearance is \\nacceptableTest xxx Phase A xxx TPS 123456\\na .  Example: (1) during product selection process, (2) prior to final product selection (if COTS) or prior to PDR, (3) prior to CDR, \\n(4) during box-level functional, (5) during system-level functional, (6) during end-to-end functional, (7) during integrated vehicle \\nfunctional, (8) during on-orbit functional  .205\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nAppendix F: Functional, Timing, and State Analysis\\nThis appendix was removed. For additional guidance \\non functional flow block diagrams, requirements allo -\\ncation sheets/models, N-squared diagrams, timing analysis, and state analysis refer to Appendix F in the NASA Expanded Guidance for Systems Engineering at \\nhttps://nen.nasa.gov/web/se/doc-repository .206\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nAppendix G: Technology Assessment/Insertion\\nG.1 Introduction, Purpose,  \\nand Scope\\nIn 2014, the Headquarters Office of Chief Engineer \\nand Office of Chief Technologist conducted an Agency-wide study on Technical Readiness Level (TRL) usage and Technology Readiness Assessment (TRA) implementation. Numerous findings, observa -\\ntions, and recommendations were identified, as was a wealth of new guidance, best practices, and clarifica -\\ntions on how to interpret TRL and perform TRAs. These are presently being collected into a NASA TRA Handbook (in work), which will replace this appendix. In the interim, contact HQ/Steven Hirshorn on any specific questions on interpretation and application of TRL/TRA. Although the information contained in this appendix may change, it does provide some infor -\\nmation until the TRA Handbook can be completed.\\nAgency programs and projects frequently require \\nthe development and infusion of new technologi -\\ncal advances to meet mission goals, objectives, and resulting requirements. Sometimes the new techno -\\nlogical advancement being infused is actually a heri -\\ntage system that is being incorporated into a different architecture and operated in a different environment from that for which it was originally designed. It is important to recognize that the adaptation of heritage systems frequently requires technological advance -\\nment. Failure to account for this requirement can result in key steps of the development process being given short shrift—often to the detriment of the program/project. In both contexts of technological advancement (new and adapted heritage), infusion is a complex process that is often dealt with in an ad hoc manner differing greatly from project to project with varying degrees of success.\\nTechnology infusion frequently results in schedule \\nslips, cost overruns, and occasionally even in cancel -\\nlations or failures. In post mortem, the root cause of such events is often attributed to “inadequate defi -\\nnition of requirements.” If such is indeed the root cause, then correcting the situation is simply a matter of defining better requirements, but this may not be the case—at least not totally.\\nIn fact, there are many contributors to schedule \\nslip, cost overrun, and project cancellation and fail -\\nure—among them lack of adequate requirements definition. The case can be made that most of these contributors are related to the degree of uncertainty at the outset of the project and that a dominant factor in the degree of uncertainty is the lack of understand -\\ning of the maturity of the technology required to bring the project to fruition and a concomitant lack of understanding of the cost and schedule reserves required to advance the technology from its present state to a point where it can be qualified and suc -\\ncessfully infused with a high degree of confidence. Although this uncertainty cannot be eliminated, it can be substantially reduced through the early appli -\\ncation of good systems engineering practices focused on understanding the technological requirements; the maturity of the required technology; and the technological advancement required to meet pro -\\ngram/project goals, objectives, and requirements.207\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix G: Technology Assessment/Insertion\\nA number of processes can be used to develop the \\nappropriate level of understanding required for successful technology insertion. The intent of this appendix is to describe a systematic process that can be used as an example of how to apply standard systems engineering practices to perform a compre -\\nhensive Technology Assessment (TA). The TA com -\\nprises two parts, a Technology Maturity Assessment (TMA) and an Advancement Degree of Difficulty Assessment (AD\\n2). The process begins with the TMA \\nwhich is used to determine technological maturity via NASA’s Technology Readiness Level (TRL) scale. It then proceeds to develop an understanding of what is required to advance the level of maturity through the AD\\n2. It is necessary to conduct TAs at various \\nstages throughout a program/project to provide the Key Decision Point (KDP) products required for transition between phases. (See \\nTABLE G.1-1 .)\\nThe initial TMA provides the baseline maturity of the system’s required technologies at program/project outset and allows monitoring progress throughout development. The final TMA is performed just prior to the Preliminary Design Review (PDR). It forms the basis for the Technology Readiness Assessment Report (TRAR), which documents the maturity of the technological advancement required by the systems, subsystems, and components demonstrated through test and analysis. The initial AD\\n2 provides the \\nmaterial necessary to develop preliminary cost and to schedule plans and preliminary risk assessments. In subsequent assessments, the information is used to build the Technology Development Plan and in the process, identify alternative paths, fallback positions, and performance descope options. The information is also vital to preparing milestones and metrics for \\nsubsequent Earned Value Management  (EVM).\\nThe TMA is performed against the hierarchical breakdown of the hardware and software products of the program/project PBS to achieve a systematic, overall understanding at the system, subsystem, and component levels. (See \\nFIGURE G.1-1 .)TABLE G.1-1 Products Provided by the TA as a Function of Program/Project Phase\\nGate Product\\nKDP A:  Transition from Requires an assessment of potential technology needs versus current and planned \\nPre-Phase A to Phase A technology readiness levels, as well as potential opportunities to use commercial, \\nacademic, and other government agency sources of technology. Included as part of the draft integrated baseline. Technology Development Plan is baselined that identifies technologies to be developed, heritage systems to be modified, alternative paths to be pursued, fallback positions and corresponding performance descopes, milestones, metrics, and key decision points. Initial Technology Readiness Assessment (TRA) is available.\\nKDP B:  Transition from Technology Development Plan and Technology Readiness Assessment (TRA) are updated. \\nPhase A to Phase B Incorporated in the preliminary project plan.\\nKDP C:  Transition from Requires a TRAR demonstrating that all systems, subsystems, and components have \\nPhase B to Phase C/D achieved a level of technological maturity with demonstrated evidence of qualification in a relevant environment.\\nSource: NPR 7120  .5 .\\nG.2 Inputs/Entry Criteria\\nIt is extremely important that a TA process be defined \\nat the beginning of the program/project and that it be performed at the earliest possible stage (concept development) and throughout the program/proj -\\nect through PDR. Inputs to the process will vary in level of detail according to the phase of the program/project, and even though there is a lack of detail in Pre-Phase A, the TA will drive out the major critical 208\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix G: Technology Assessment/Insertion\\ntechnological advancements required. Therefore, at \\nthe beginning of Pre-Phase A, the following should be provided:\\n• Refinement of TRL definitions.\\n• Definition of AD2.\\n• Definition of terms to be used in the assessment process.\\n• Establishment of meaningful evaluation criteria and metrics that will allow for clear identification of gaps and shortfalls in performance.\\n• Establishment of the TA team.\\n• Establishment of an independent TA review team..1: Integ MPS\\n.2: LH System.3: O2 F luid Sys.\\n.4: Pressure &\\nPneumatic Sys.\\n.5: Umbilicals & \\nDisconnect Crew Launch\\nVehicle1.3\\nLaunch\\nVehicle1.3.8\\nFirst Stage1.3.8.1\\nUpper Stage1.3.8.2\\nUpper Stage\\nEngine1.3.8.3...\\nMPS1.3.8.2.4...\\nUS R CS1.3.8.2.5\\nFS R CS1.3.8.2.6\\nTVCS1.3.8.2.7\\nAvionics1.3.8.2.8\\nSoftware1.3.8.2.9\\n...Integrated\\nTest H/W1.3.8.2.10...\\n.1: Integ RCS\\n.2: Integ Energy\\nSupport.1: Integ RCS .1: Integ TVCS\\n.2: Actuator\\n.3: Hydraulic\\nPower\\n.4: APU.1: Integ Avionic s\\n.2: C&DH System\\n.3: GN&C H/W\\n.4: Radio Frequency\\nSystem\\n.5: EPS\\n.6: Electrical Integration\\n.7: Develop Flight Instrument\\n.8: Sensor & Instrument System\\n.9: EGSE.10: Integ CLV Avionics System\\nElement Testing.11: Flight Safe ty Sys tem.1: Integ S/W\\nSystem\\n.2: Flight S/W.\\n1: MPTA\\n.2: GVT.3: STA.4: US for DTF-1.5: US for VTF-2\\n.6: US for RRF-3\\n.7: Struc. Therma l\\nComponent Test\\nFIGURE G.1-1  P BS Example\\nG.3 How to Do Technology \\nAssessment\\nThe technology assessment process makes use of basic \\nsystems engineering principles and processes. As mentioned previously, it is structured to occur within \\nthe framework of the Product Breakdown Structure \\n(PBS) to facilitate incorporation of the results. Using the PBS as a framework has a twofold benefit—it breaks the “problem” down into systems, subsys -\\ntems, and components that can be more accurately assessed; and it provides the results of the assessment \\nin a format that can be readily used in the generation 209\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix G: Technology Assessment/Insertion\\nof program costs and schedules. It can also be highly \\nbeneficial in providing milestones and metrics for progress tracking using EVM. As discussed above, it is a two-step process comprised of (1) the determina -\\ntion of the current technological maturity in terms of TRLs and (2) the determination of the difficulty associated with moving a technology from one TRL to the next through the use of the AD\\n2.\\nConceptual Level ActivitiesThe overall process is iterative, starting at the concep -\\ntual level during program Formulation, establishing the initial identification of critical technologies, and establishing the preliminary cost, schedule, and risk mitigation plans. Continuing on into Phase A, the process is used to establish the baseline maturity, the Technology Development Plan, and the associated costs and schedule. The final TA consists only of the TMA and is used to develop the TRAR, which val -\\nidates that all elements are at the requisite maturity level. (See \\nFIGURE G.3-1 .)\\nIdentify systems, sub-\\nsystems, and components\\nper hierarchical product\\nbreakdown of the WBS Assign TRL to subsystems\\nbased on lowest TRL of\\ncomponents and TRL \\nstate of integration Assign TRL to all\\ncomponents based on\\nassessment of maturity\\nAssign TRL to systems\\nbased on lowest TRL of\\nsubsystems and TRL \\nstate of integration Identify all components,\\nsubsystems, and systems\\nthat are at lower TRLs\\nthan required by programBaseline technology\\nmaturity assessment\\nPerform AD2 on all\\ncomponents, subsystems,\\nand systems that are below\\nrequisite maturity level\\nTechnology Development Plan\\nCost Plan\\nSchedule Plan\\nRisk Assessment\\nFIGURE G.3-1  Te chnology Assessment ProcessE\\nven at the conceptual level, it is important to use \\nthe formalism of a PBS to avoid allowing important technologies to slip through the cracks. Because of the preliminary nature of the concept, the systems, subsystems, and components will be defined at a level that will not permit detailed assessments to be made. The process of performing the assessment, however, is the same as that used for subsequent, more detailed steps that occur later in the program/project where systems are defined in greater detail.\\nArchitectural Studies\\nOnce the concept has been formulated and the ini -\\ntial identification of critical technologies made, it is 210\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix G: Technology Assessment/Insertion\\nnecessary to perform detailed architecture studies \\nwith the Technology Assessment Process intimately \\ninterwoven. (See FIGURE G.3-2. )\\nRequir e-\\nments\\nTRL/AD2 AssessmentArchitectural\\nStudies\\nTechnology Maturation ConceptsSystem\\nDesign\\nFIGURE G.3-2 Architectural Studies and Technology \\nDevelopment\\nThe purpose of the architecture studies is to refine \\nend-item system design to meet the overall scientific \\nrequirements of the mission. It is imperative that \\nthere be a continuous relationship between architec -\\ntural studies and maturing technology advances. The \\narchitectural studies should incorporate the results of \\nthe technology maturation, planning for alternative \\npaths and identifying new areas required for devel -\\nopment as the architecture is refined. Similarly, it \\nis incumbent upon the technology maturation pro -\\ncess to identify requirements that are not feasible \\nand development routes that are not fruitful and to \\ntransmit that information to the architecture studies \\nin a timely manner. It is also incumbent upon the \\narchitecture studies to provide feedback to the tech -\\nnology development process relative to changes in \\nrequirements. Particular attention should be given \\nto “heritage” systems in that they are often used in \\narchitectures and environments different from those \\nin which they were designed to operate.\\nG.4 Establishing TRLs\\nA Technology Readiness Level (TRL) is, at its most \\nbasic, a description of the performance history of a \\ngiven system, subsystem, or component relative to \\na set of levels first described at NASA HQ in the 1980s. The TRL essentially describes the state of a \\ngiven technology and provides a baseline from which \\nmaturity is gauged and advancement defined. (See \\nFIGURE G.4-1 .)\\nPrograms are often undertaken without fully under -\\nstanding either the maturity of key technologies \\nor what is needed to develop them to the required \\nlevel. It is impossible to understand the magnitude and \\nscope of a development program without having a clear \\nunderstanding of the baseline technological maturity of \\nall elements of the system. Establishing the TRL is a \\nvital first step on the way to a successful program. A \\nfrequent misconception is that in practice, it is too \\ndifficult to determine TRLs and that when you do, \\nit is not meaningful. On the contrary, identifying \\nTRLs can be a straightforward systems engineering \\nprocess of determining what was demonstrated and \\nunder what conditions it was demonstrated.\\nTerminology\\nAt first glance, the TRL descriptions in FIGURE G.4-1  \\nappear to be straightforward. It is in the process of \\ntrying to assign levels that problems arise. A primary \\ncause of difficulty is in terminology; e.g., everyone \\nknows what a breadboard is, but not everyone has \\nthe same definition. Also, what is a “relevant envi -\\nronment?” What is relevant to one application may \\nor may not be relevant to another. Many of these \\nterms originated in various branches of engineering \\nand had, at the time, very specific meanings to that \\nparticular field. They have since become commonly \\nused throughout the engineering field and often \\nacquire differences in meaning from discipline to \\ndiscipline, some differences subtle, some not so sub -\\ntle. “Breadboard,” for example, comes from electrical \\nengineering where the original use referred to check -\\ning out the functional design of an electrical circuit \\nby populating a “breadboard” with components to \\nverify that the design operated as anticipated. Other \\nterms come from mechanical engineering, referring 211\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix G: Technology Assessment/Insertion\\nprimarily to units that are subjected to different levels \\nof stress under testing, e.g., qualification, protoflight, and flight units. The first step in developing a uni -\\nform TRL assessment (see \\nFIGURE G.4-2 ) is to define \\nthe terms used. It is extremely important to develop and use a consistent set of definitions over the course of the program/project.TRL 1          Basic principles observed and reportedTRL 2         Technology concept and/or application form ulated\\n  __TRL 3 Analytical and experimental critical function and/or\\n __ characteristic proof-of-conceptTRL 4 Component and/or breadboard validation in laboratory   __ environment TRL 5 Component and/or breadboard validation in relevant  __ environmentTRL 6 System/subsystem model or prototype demonstration  __  in a relevant environment (ground or space)TRL 7 \\nSystem prototype demonstration in a target/space environment\\n  __TRL 8 Actual system completed and “flight qualified” through\\ntest and demonstration (ground or flight)” \\n  __ TRL 9 Actual system “flight proven” thr ough su cces sful\\n  __ mission operationsSystem test, launch, \\nand operations\\nSystem/subsystem\\ndevelopment\\nTechnology\\ndemonstration\\nTechnology\\ndevelopment\\nResearch to prove\\nfeasibility\\nBasic technology\\nresearch\\nFIGURE G.4-1  Tec hnology Readiness Levels\\nJud\\ngment Calls\\nHaving established a common set of terminology, it \\nis necessary to proceed to the next step: quantifying “judgment calls” on the basis of past experience. Even with clear definitions, judgment calls will be required wh\\nen it comes time to assess just how similar a given \\nelement is relative to what is needed (i.e., is it close enough to a prototype to be considered a proto -\\ntype, or is it more like an engineering breadboard?). Describing what has been done in terms of form, fit, and function provides a means of quantifying an element based on its design intent and subsequent performance. The current definitions for software TRLs are contained in NPR 7123.1, NASA Systems Engineering Processes and Requirements.212\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix G: Technology Assessment/Insertion\\nAssessment Team\\nA third critical element of any assessment relates to the question of who is in the best position to make judgment calls relative to the status of the technol -\\nogy in question. For this step, it is extremely import -\\nant to have a well-balanced, experienced assessment team. Team members do not necessarily have to be discipline experts. The primary expertise required for a TRL assessment is that the systems engineer/user understands the current state of the art in applica -\\ntions. User considerations are evaluated by HFE per -\\nsonnel who understand the challenges of technology insertions at various stages of the product life cycle. Having established a set of definitions, defined a pro -\\ncess for quantifying judgment calls, and assembled an expert assessment team, the process primarily consists of asking the right questions. The flowchart depicted in \\nFIGURE G.4-2  demonstrates the questions to ask to \\ndetermine TRL at any level in the assessment.Has an identical unit been successfully\\noperated/ launched in i dentical\\nconfigu ration/environment?\\nHas an identical unit in a different configuration/\\nsystem architecture been successfully operated\\nin space or the target environment or launched?\\nIf so, then this initially drops to TRL 5 until\\ndifferences are evaulated.\\nHas an identical unit been flight qualified but\\nnot yet operated in space or the target\\nenvironment or launched?\\nHas a prototype unit (or one similar enough to be\\nconsidered a prototype) been successfully operated\\nin space or the  target environment or launched?\\nHas a prototype unit (or one similar enough \\nto be considered a prototype) been \\ndemonstrated in a relevant environment?\\nHas a breadboard unit been demonstrated in \\na relevant environment?\\nHas a breadboard unit been demonstrated in \\na laboratory environment?\\nHas analytical and experimental\\nproof-of-concept been demonstrated?\\nHas concept or application\\nbeen formulated?\\nHave basic principles been observed\\nand reported?\\nRETHINK POSITION REGARDING\\nTHIS TECHNOLOGY!NONONONONONONONONONOTRL 9YES\\nTRL 5YES\\nTRL 2YES\\nTRL 1YESTRL 3YESTRL 4YESTRL 5YESTRL 6YESTRL 7YESTRL 8YES\\nFIGURE G.4-2  TM A Thought ProcessH\\neritage Systems\\nNote the second box particularly refers to heritage \\nsystems. If the architecture and the environment have changed, then the TRL drops to TRL 5—at least initially. Additional testing may need to be done for heritage systems for the new use or new environ -\\nment. If in subsequent analysis the new environment is sufficiently close to the old environment or the new architecture sufficiently close to the old architecture, then the resulting evaluation could be TRL 6 or 7, but the most important thing to realize is that it is no longer at TRL 9. Applying this process at the system level and then proceeding to lower levels of subsystem and component identifies those elements that require development and sets the stage for the subsequent phase, determining the AD\\n2.213\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix G: Technology Assessment/Insertion\\nFormal Process for Determining TRLs\\nA method for formalizing this process is shown in \\nFIGURE G.4-3 . Here, the process has been set up as a \\ntable: the rows identify the systems, subsystems, and \\ncomponents that are under assessment. The columns \\nidentify the categories that will be used to determine \\nthe TRL; i.e., what units have been built, to what \\nscale, and in what environment have they been tested. \\nAnswers to these questions determine the TRL of an \\nitem under consideration. The TRL of the system is \\ndetermined by the lowest TRL present in the system; \\ni.e., a system is at TRL 2 if any single element in the system is at TRL 2. The problem of multiple ele -\\nments being at low TRLs is dealt with in the AD2 \\nprocess. Note that the issue of integration affects the \\nTRL of every system, subsystem, and component. All \\nof the elements can be at a higher TRL, but if they \\nhave never been integrated as a unit, the TRL will \\nbe lower for the unit. How much lower depends on \\nthe complexity of the integration. The assessed com -\\nplexity depends upon the combined judgment of the \\nengineers. It is important to have a good cross-section \\nof senior people sitting in judgment.\\n Red = Below TRL 3\\n Yellow = TRL 3, 4 & 5\\n Green = TRL 6 and above\\n White = Unknown\\n X = Exists\\nConceptDemonstration Units Environment Unit Description\\nOverall TRLBreadboard\\nBrassboard\\nDevelopmental Model\\nProtogype\\nFlight Qualified\\nLaboratory Environment\\nRelevant Environment\\nSpace Environment\\nSpace Launch Operation\\nForm\\nFit\\nFunction\\nAppropriatae Scale\\n1.0 System\\n1.1 Subsystem X\\n1.1.1 Mechanical Components\\n1.1.2 Mechanical Systems\\n1.1.3 Electrical Components X X X X X\\n1.1.4 Electrical Systems\\n1.1.5 Control Systems\\n1.1.6 Thermal Systems X X X\\n1.1.7 Fluid Systems X\\n1.1.8 Optical Systems\\n1.1.9 Electro-Optical Systems\\n1.1.10 Software Systems\\n1.1.11 Mechanisms X\\n1.1.12 Integration\\n1.2 Subsystem Y\\n1.2.1 Mechanical Components\\nFIGURE G.4-3 TRL Assessment Matrix214\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nAppendix H: Integration Plan Outline\\nH.1 Purpose\\nThe integration plan defines the integration and ver -\\nification strategies for a project interface with the \\nsystem design and decomposition into the lower-level elements.\\n1 The integration plan is structured to bring \\nthe elements together to assemble each subsystem and to bring all of the subsystems together to assem -\\nble the system/product. The primary purposes of the integration plan are: (1) to describe this coordinated integration effort that supports the implementation strategy, (2) to describe for the participants what needs to be done in each integration step, and (3) to identify the required resources and when and where they will be needed.\\nH.2 Questions/Checklist\\n• Does the integration plan include and cover inte -\\ngration of all of the components and subsystems of the project, either developed or purchased?\\n• Does the integration plan account for all exter -\\nnal systems to be integrated with the system (for example, communications networks, field equip -\\nment, other complete systems owned by the gov -\\nernment or owned by other government agencies)?\\n• Does the integration plan fully support the imple -\\nmentation strategy, for example, when and where the subsystems and system are to be used?\\n1  The material in this appendix is adapted from Federal \\nHighway Administration and CalTrans, Systems Engineering \\nGuidebook for ITS, Version 2  0 .• Does the integration plan mesh with the verifica -\\ntion plan?\\n• For each integration step, does the integration plan define what components and subsystems are to be integrated?\\n• For each integration step, does the integration plan identify all the needed participants and define what their roles and responsibilities are?\\n• Does the integration plan establish the sequence and schedule for every integration step?\\n• Does the integration plan spell out how integra -\\ntion problems are to be documented and resolved?\\nH.3 Integration Plan Contents\\nTitle PageThe title page should follow the NASA procedures or style guide. At a minimum, it should contain the follow -\\ning information:\\n• INTEGRATION PLAN FOR THE [insert name \\nof project]  AND [insert name of organization]\\n• Contract number\\n• Date that the document was formally approved\\n• The organization responsible for preparing the document215\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix H: Integration Plan Outline\\n• Internal document control number, if available\\n• Revision version and date issued\\n1.0 Purpose of Document\\nThis section gives a brief statement of the purpose of this \\ndocument. It is the plan for integrating the components \\nand subsystems of the project prior to verification.\\n2.0 Scope of Project\\nThis section gives a brief description of the planned proj -\\nect and the purpose of the system to be built. Special \\nemphasis is placed on the project’s deployment complex -\\nities and challenges.\\n3.0 Integration Strategy\\nThis section tells the reader what the high-level plan for \\nintegration is and, most importantly, why the integra -\\ntion plan is structured the way it is. The integration plan \\nis subject to several, sometimes conflicting, constraints. \\nAlso, it is one part of the larger process of build, inte -\\ngrate, verify, and deploy, all of which should be syn -\\nchronized to support the same project strategy. So, for \\neven a moderately complex project, the integration strat -\\negy, which is based on a clear and concise statement of \\nthe project’s goals and objectives, is described here at a \\nhigh but all-inclusive level. It may also be necessary to \\ndescribe the analysis of alternative strategies to make it \\nclear why this particular strategy was selected.\\nThe same strategy is the basis for the build plan, the \\nverification plan, and the deployment plan. This section \\ncovers and describes each step in the integration process. \\nIt describes what components are integrated at each step \\nand gives a general idea of what threads of the opera -\\ntional capabilities (requirements) are covered. It ties the \\nplan to the previously identified goals and objectives so \\nthe stakeholders can understand the rationale for each \\nintegration step. This summary-level description also \\ndefines the schedule for all the integration efforts.4.0 Phase 1 Integration\\nThis and the following sections define and explain each \\nstep in the integration process. The intent here is to iden -\\ntify all the needed participants and to describe to them \\nwhat they have to do. In general, the description of each \\nintegration step should identify the following:\\n• The location of the activities.\\n• The project-developed equipment and software prod -\\nucts to be integrated. Initially this is just a high-level \\nlist, but eventually the list should be exact and com -\\nplete, showing part numbers and quantity.\\n• Any support equipment (special software, test hard -\\nware, software stubs, and drivers to simulate yet-to-\\nbe-integrated software components, external systems) \\nneeded for this integration step. The same support \\nequipment is most likely needed for the subsequent \\nverification step.\\n• All integration activities that need to be performed \\nafter installation, including integration with onsite \\nsystems and external systems at other sites.\\n• A description of the verification activities, as defined \\nin the applicable verification plan, that occur after \\nthis integration step.\\n• The responsible parties for each activity in the inte -\\ngration step.\\n• The schedule for each activity.\\n5.0  Multiple Phase Integration Steps  \\n(1 or N steps)\\nThis and any needed additional sections follow the \\nformat for Section 3.0. Each covers each step in a \\nmultiple-step integration effort.216\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nAppendix I: Verification and Validation Plan Outline\\nSample Outline\\nThe Verification and Validation (V&V) Plan needs \\nto be baselined after the comments from PDR are incorporated. In this annotated outline, the use of the term “system” is indicative of the entire scope for which this plan is developed. This may be an entire spacecraft, just the avionics system, or a card within the avionics system. Likewise, the terms “end item,” “subsystem,” or “element” are meant to imply the lower-level products that, when integrated together, will produce the “system.” The general term “end item” is used to encompass activities regardless of whether the end item is a hardware or software element.\\nThe various sections are intended to move from the \\nhigh-level generic descriptions to the more detailed. The sections also flow from the lower-level items in the product layer to larger and larger assemblies and to the completely integrated system. The sections also describe how that system may be integrated and fur -\\nther verified/validated with its externally interfacing elements. This progression will help build a complete understanding of the overall plans for verification and validation.\\n1.0 Introduction\\n1.1 Purpose and Scope\\nThis section states the purpose of this Verification and Validation Plan and the scope (i.e., systems) to which it applies. The purpose of the V&V Plan is to identify the activities that will establish compliance with the requirements (verification) and to establish that the sys -\\ntem will meet the customers’ expectations (validation).\\n1.2  Responsibility and Change \\nAuthority\\nThis section will identify who has responsibility for the \\nmaintenance of this plan and who or what board has the authority to approve any changes to it.\\n1.3 Definitions\\nThis section will define any key terms used in the plan. The section may include the definitions of verification, validation, analysis, test, demonstration, and test. See \\nappendix B  of this handbook for definitions of these and \\nother terms that might be used.\\n2.0 Applicable and Reference \\nDocuments\\n2.1 Applicable Documents\\nThese are the documents that may impose additional \\nrequirements or from which some of the requirements have been taken.\\n2.2 Reference Documents\\nThese are the documents that are referred to within the V&V Plan that do not impose requirements, but which may have additional useful information.\\n2.3 Order of Precedence\\nThis section identifies which documents take precedence whenever there are conflicting requirements.217\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix I: Verification and Validation Plan Outline\\n3.0 System Description\\n3.1 System Requirements Flowdown\\nThis section describes where the requirements for this \\nsystem come from and how they are flowed down to \\nsubsystems and lower-level elements. It should also \\nindicate what method will be used to perform the flow -\\ndown and bidirectional traceability of the requirements: \\nspreadsheet, model, or other means. It can point to the \\nfile, document, or spreadsheet that captures the actual \\nrequirements flowdown.\\n3.2 System Architecture\\nThis section describes the system that is within the scope \\nof this V&V Plan. The description should be enough so \\nthat the V&V activities will have the proper context and \\nbe understandable.\\n3.3 End Item Architectures\\nThis section describes each of the major end items (sub -\\nsystems, elements, units, modules, etc.) that when inte -\\ngrated together, will form the overall system that is the \\nscope of this V&V Plan.\\n3.3.1 System End Item A\\nThis section describes the first major end item/subsystem \\nin more detail so that the V&V activities have context \\nand are understandable.\\n3.3.n System End Item n\\nEach end item/subsystem is separately described in a sim -\\nilar manner as above.\\n3.4 Ground Support Equipment\\nThis section describes any major ground-support equip -\\nment that will be used during the V&V activities. This \\nmay include carts for supplying power or fuel, special test \\nfixtures, lifting aids, simulators, or other type of support.\\n3.5 Other Architecture Descriptions\\nThis section describes any other items that are import -\\nant for the V&V activities but which are not included in the sections above. This may be an existing control \\ncenter, training facility, or other support.\\n4.0 Verification and Validation \\nProcess\\nThis section describes the process that will be used to per -\\nform verification and validation.\\n4.1  Verification and Validation \\nManagement Responsibilities\\nThis section describes the responsibilities of key players \\nin the V&V activities. It may include identification and \\nduty description for test directors/conductors, managers, \\nfacility owners, boards, and other key stakeholders.\\n4.2 Verification Methods\\nThis section defines and describes the methods that will \\nbe used during the verification activities.\\n4.2.1 Analysis\\nDefines what this verification method means (See \\nAppendix B  of this handbook) and how it will be \\napplied to this system.\\n4.2.2 Inspection\\nDefines what this verification method means (See \\nAppendix B  of this handbook) and how it will be \\napplied to this system.\\n4.2.3 Demonstration\\nDefines what this verification method means (See \\nAppendix B  of this handbook) and how it will be applied \\nto this system.\\n4.2.4 Test\\nDefines what this verification method means (See \\nAppendix B  of this handbook) and how it will be applied \\nto this system. This category may need to be broken down \\ninto further categories.218\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix I: Verification and Validation Plan Outline\\n4.2.4.1 Qualification T esting\\nThis section describes the test philosophy for the envi -\\nronmental and other testing that is performed at higher \\nthan normal levels to ascertain margins and perfor -\\nmance in worst-case scenarios. Includes descriptions \\nof how the minimum and maximum extremes will be \\ndetermined for various types of tests (thermal, vibra -\\ntion, etc.), whether it will be performed at a component, \\nsubsystem, or system level, and the pedigree (flight unit, \\nqualification unit, engineering unit, etc.) of the units \\nthese tests will be performed on.\\n4.2.4.2 Other T esting\\nThis section describes any other testing that will be used \\nas part of the verification activities that are not part \\nof the qualification testing. It includes any testing of \\nrequirements within the normal operating range of the \\nend item. It may include some engineering tests that will \\nform the foundation or provide dry runs for the official \\nverification testing.\\n4.3 Validation Methods\\nThis section defines and describes the methods to be used \\nduring the validation activities.\\n4.2.1 Analysis\\nDefines what this validation method means (See \\nAppendix B  of this handbook) and how it will be applied \\nto this system.\\n4.2.2 Inspection\\nDefines what this validation method means (See \\nAppendix B  of this handbook) and how it will be applied \\nto this system.\\n4.2.3 Demonstration\\nDefines what this validation method means (See \\nAppendix B  of this handbook) and how it will be applied \\nto this system.4.2.4 Test\\nDefines what this validation method means (See \\nAppendix B  of this handbook) and how it will be applied \\nto this system. This category may need to be broken down \\ninto further categories such as end-to-end testing, testing \\nwith humans, etc.)\\n4.4 Certification Process\\nDescribes the overall process by which the results of these \\nverification and validation activities will be used to cer -\\ntify that the system meets its requirements and expec -\\ntations and is ready to be put into the field or fly. In \\naddition to the verification and validation results, the \\ncertification package may also include special forms, \\nreports, safety documentation, drawings, waivers, or \\nother supporting documentation.\\n4.5 Acceptance Testing\\nDescribes the philosophy of how/which of the verification/\\nvalidation activities will be performed on each of the \\noperational units as they are manufactured/coded and \\nare readied for flight/use. Includes how/if data packages \\nwill be developed and provided as part of the delivery.\\n5.0 Verification and Validation \\nImplementation\\n5.1  System Design and Verification \\nand Validation Flow\\nThis section describes how the system units/modules will \\nflow from manufacturing/coding through verification \\nand validation. Includes whether each unit will be veri -\\nfied/validated separately, or assembled to some level and \\nthen evaluated or other statement of flow.\\n5.2 Test Articles\\nThis section describes the pedigree of test articles that will \\nbe involved in the verification/validation activities. This 219\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix I: Verification and Validation Plan Outline\\nmay include descriptions of breadboards, prototypes, \\nengineering units, qualification units, protoflight units, \\nflight units, or other specially named units. A definition \\nof what is meant by these terms needs to be included to \\nensure clear understanding of the expected pedigree of \\neach type of test article. Descriptions of what kind of test/\\nanalysis activities will be performed on each type of test \\narticle is included.\\n5.3 Support Equipment\\nThis section describes any special support equipment that \\nwill be needed to perform the verification/validation \\nactivities. This will be a more detailed description than \\nis stated in Section 3.4 of this outline.\\n5.4 Facilities\\nThis section identifies and describes major facilities that \\nwill be needed in order to accomplish the verification \\nand validation activities. These may include environ -\\nmental test facilities, computational facilities, simula -\\ntion facilities, training facilities, test stands, and other \\nfacilities as needed.\\n6.0 End Item Verification and \\nValidation\\nThis section describes in detail the V&V activities that \\nwill be applied to the lower-level subsystems/elements/\\nend items. It can point to other stand-alone descrip -\\ntions of these tests if they will be generated as part of \\norganizational responsibilities for the products at each \\nproduct  layer.\\n6.1 End Item A\\nThis section focuses in on one of the lower-level end items \\nand describes in detail what type of verification activi -\\nties it will undergo.6.1.1 Developmental/Engineering Unit Evaluations\\nThis section describes what kind of testing, analysis, \\ndemonstrations, or inspections the prototype/engineering \\nor other types of units/modules will undergo prior to per -\\nforming official verification and validation.\\n6.1.2 Verification Activities\\nThis section describes in detail the verification activities \\nthat will be performed on this end item.\\n6.1.2.1 Verification by T esting\\nThis section describes all verification testing that will be \\nperformed on this end item.\\n6.1.2.1.1  Qualification Testing\\nThis section describes the test environmental and other \\ntesting that is performed at higher than normal levels \\nto ascertain margins and performance in worst-case \\nscenarios. It includes what minimum and maximum \\nextremes will be used on qualification tests (thermal, \\nvibration, etc.) of this unit, whether it will be performed \\nat a component, subsystem, or system level, and the ped -\\nigree (flight unit, qualification unit, engineering unit, \\netc.) of the units these tests will be performed on.\\n6.1.2.1.2  Other Testing\\nThis section describes all other verification tests that are \\nnot performed as part of the qualification testing. These \\nwill include verification of requirements in the normal \\noperating ranges.\\n6.1.2.2 Verification by Analysis\\nThis section describes the verifications that will be per -\\nformed by analysis (including verification by similarity). \\nThis may include thermal analysis, stress analysis, anal -\\nysis of fracture control, materials analysis, Electrical, \\nElectronic, and Electromechnical (EEE) parts analysis, \\nand other analyses as needed for the verification of this \\nend item.220\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix I: Verification and Validation Plan Outline\\n6.1.2.3 Verification by Inspection\\nThis section describes the verifications that will be per -\\nformed for this end item by inspection.\\n6.1.2.4 Verification Demonstration\\nThis section describes the verifications that will be per -\\nformed for this end item by demonstration.\\n6.1.3 Validation Activities\\n6.1.3.1 Validation by T esting\\nThis section describes what validation tests will be per -\\nformed on this end item.\\n6.1.3.2 Validation by Analysis\\nThis section describes the validation that will be per -\\nformed for this end item through analysis.\\n6.1.3.3 Validation by Inspection\\nThis section describes the validation that will be per -\\nformed for this end item through inspection.\\n6.1.3.4 Validation by Demonstration\\nThis section describes the validations that will be per -\\nformed for this end item by demonstration.\\n6.1.4 Acceptance Testing\\nThis section describes the set of tests, analysis, demonstra -\\ntions, or inspections that will be performed on the flight/\\nfinal version of the end item to show it has the same \\ndesign as the one that is being verified, that the work -\\nmanship on this end item is good, and that it performs \\nthe identified functions properly.\\n6.n End Item n\\nIn a similar manner as above, a description of how each \\nend item that makes up the system will be verified and \\nvalidated is made.7.0 System Verification and \\nValidation\\n7.1 End-Item Integration\\nThis section describes how the various end items will be \\nassembled/integrated together, verified and validated. \\nFor example, the avionics and power systems may be \\nintegrated and tested together to ensure their interfaces \\nand performance is as required and expected prior to \\nintegration with a larger element. This section describes \\nthe verification and validation that will be performed \\non these major assemblies. Complete system integration \\nwill be described in later sections.\\n7.1.1 Developmental/Engineering Unit Evaluations\\nThis section describes the unofficial (not the formal ver -\\nification/validation) testing/analysis that will be per -\\nformed on the various assemblies that will be tested \\ntogether and the pedigree of the units that will be used. \\nThis may include system-level testing of configurations \\nusing engineering units, breadboard, simulators, or \\nother forms or combination of forms.\\n7.1.2 Verification Activities\\nThis section describes the verification activities that will \\nbe performed on the various assemblies.\\n7.1.2.1 Verification by T esting\\nThis section describes all verification testing that will be \\nperformed on the various assemblies. The section may be \\nbroken up to describe qualification testing performed on \\nthe various assemblies and other types of testing.\\n7.1.2.2 Verification by Analysis\\nThis section describes all verification analysis that will be \\nperformed on the various assemblies.\\n7.1.2.3 Verification by Inspection\\nThis section describes all verification inspections that \\nwill be performed on the various assemblies.221\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix I: Verification and Validation Plan Outline\\n7.1.2.4 Verification by Demonstration\\nThis section describes all verification demonstrations \\nthat will be performed on the various assemblies.\\n7.1.3 Validation Activities\\n7.1.3.1 Validation by T esting\\nThis section describes all validation testing that will be \\nperformed on the various assemblies.\\n7.1.3.2 Validation by Analysis\\nThis section describes all validation analysis that will be \\nperformed on the various assemblies.\\n7.1.3.3 Validation by Inspection\\nThis section describes all validation inspections that will \\nbe performed on the various assemblies.\\n7.1.3.4 Validation by Demonstration\\nThis section describes all validation demonstrations that \\nwill be performed on the various assemblies.\\n7.2 Complete System Integration\\nThis section describes the verification and validation \\nactivities that will be performed on the systems after all \\nits assemblies are integrated together to form the complete \\nintegrated system. In some cases this will not be practical. \\nRationale for what cannot be done should be captured.\\n7.2.1 Developmental/Engineering Unit Evaluations\\nThis section describes the unofficial (not the formal \\nverification/validation) testing/analysis that will be \\nperformed on the complete integrated system and the \\npedigree of the units that will be used. This may include \\nsystem-level testing of configurations using engineering \\nunits, breadboard, simulators, or other forms or combi -\\nnation of forms.\\n7.2.2 Verification Activities\\nThis section describes the verification activities that will \\nbe performed on the completely integrated system7.2.2.1 Verification T esting\\nThis section describes all verification testing that will be \\nperformed on the integrated system. The section may be \\nbroken up to describe qualification testing performed at \\nthe integrated system level and other types of testing.\\n7.2.2.2 Verification Analysis\\nThis section describes all verification analysis that will be \\nperformed on the integrated system.\\n7.2.2.3 Verification Inspection\\nThis section describes all verification inspections that \\nwill be performed on the integrated system.\\n7.2.2.4 Verification Demonstration\\nThis section describes all verification demonstrations \\nthat will be performed on the integrated system.\\n7.2.3 Validation Activities\\nThis section describes the validation activities that will \\nbe performed on the completely integrated system.\\n7.2.3.1 Validation by T esting\\nThis section describes all validation testing that will be \\nperformed on the integrated system.\\n7.2.3.2 Validation by Analysis\\nThis section describes all validation analysis that will be \\nperformed on the integrated system.\\n7.2.3.3 Validation by Inspection\\nThis section describes the validation inspections that will \\nbe performed on the integrated system.\\n7.2.3.4 Validation by Demonstration\\nThis section describes the validation demonstrations that \\nwill be performed on the integrated system.222\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix I: Verification and Validation Plan Outline\\n8.0 Program Verification and \\nValidation\\nThis section describes any further testing that the system \\nwill be subjected to. For example, if the system is an \\ninstrument, the section may include any verification/vali -\\ndation that the system will undergo when integrated into \\nits spacecraft/platform. If the system is a spacecraft, the \\nsection may include any verification/validation the system \\nwill undergo when integrated with its launch vehicle.\\n8.1 Vehicle Integration\\nThis section describes any further verification or valida -\\ntion activities that will occur when the system is inte -\\ngrated with its external interfaces.\\n8.2 End-to-End Integration\\nThis section describes any end-to-end testing that the \\nsystem may undergo. For example, this configuration \\nwould include data being sent from a ground control \\ncenter through one or more relay satellites to the system \\nand back.\\n8.3 On-Orbit V&V Activities\\nThis section describes any remaining verification/valida -\\ntion activities that will be performed on a system after it \\nreaches orbit or is placed in the field.\\n9.0 System Certification \\nProducts\\nThis section describes the type of products that will \\nbe generated and provided as part of the certification \\nprocess. This package may include the verification and \\nvalidation matrix and results, pressure vessel certifica -\\ntions, special forms, materials certifications, test reports \\nor other products as is appropriate for the system being \\nverified and validated.Appendix A:  Acronyms and Abbreviations\\nThis is a list of all the acronyms and abbreviations used \\nin the V&V Plan and their spelled-out meaning.\\nAppendix B: Definition of Terms\\nThis section is a definition of the key terms that are used \\nin the V&V Plan.\\nAppendix C:  Requirement Verification \\nMatrix\\nThe V&V Plan needs to be baselined after the comments \\nfrom PDR are incorporated. The information in this \\nsection may take various forms. It could be a pointer to \\nanother document or model where the matrix and its \\nresults may be found. This works well for large projects \\nusing a requirements-tracking application. The infor -\\nmation in this section could also be the requirements \\nmatrix filled out with all but the results information and \\na pointer to where the results can be found. This allows \\nthe key information to be available at the time of base -\\nlining. For a smaller project, this may be the completed \\nverification matrix. In this case, the V&V Plan would \\nbe filled out as much as possible before. See Appendix D  \\nfor an example of a verification matrix.\\nAppendix D:  Validation Matrix\\nAs with the verification matrix, this product may take \\nvarious forms from a completed matrix to just a pointer \\nfor where the information can be found. Appendix E  \\nprovides an example of a validation matrix.223\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nAppendix J: SEMP Content Outline\\nJ.1 SEMP Content\\nThe Systems Engineering Management Plan (SEMP) \\nis the foundation document for the technical and engineering activities conducted during the project. The SEMP conveys information to all of the person -\\nnel on the technical integration methodologies and activities for the project within the scope of the proj -\\nect plan. SEMP content can exist as a stand-alone document or, for smaller projects, in higher-level project documentation.\\nThe SEMP provides the specifics of the technical \\neffort and describes what technical processes will be used, how the processes will be applied using appro -\\npriate activities, how the project will be organized to accomplish the activities, and the resources required for accomplishing the activities. The SEMP provides the framework for realizing the appropriate work products that meet the entry and success criteria of the applicable project life cycle phases to provide management with necessary information for assess -\\ning technical progress.\\nBecause the SEMP provides the specific technical \\nand management information to understand the technical integration and interfaces, its documenta -\\ntion and approval serve as an agreement within the project of how the technical work will be conducted. The SEMP communicates to the team itself, manag -\\ners, customers, and other stakeholders the technical effort that will be performed by the assigned techni -\\ncal team.The technical team, working under the overall pro -\\ngram/project plan, develops and updates the SEMP as necessary. The technical team works with the proj -\\nect manager to review the content and obtain con -\\ncurrence. The SEMP includes the following three general sections:\\n1. Technical program planning and control, which describe the processes for planning and control of the engineering efforts for the design, develop -\\nment, test, and evaluation of the system.\\n2. Systems engineering processes, which include spe -\\ncific tailoring of the systems engineering process as described in the NPR, implementation pro -\\ncedures, trade study methodologies, tools, and models to be used.\\n3. Engineering specialty integration describes the integration of the technical disciplines’ efforts into the systems engineering process and sum -\\nmarizes each technical discipline effort and cross references each of the specific and relevant plans.\\nThe SEMP outline in this appendix is guidance to be used in preparing a stand-alone project SEMP. The level of detail in the project SEMP should be adapted based on the size of the project. For a small project, the material in the SEMP can be placed in the project plan’s technical summary, and this annotated outline should be used as a topic guide.\\nSome additional important points on the SEMP:224\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix J: SEMP Content Outline\\n• The SEMP is a living document. The initial SEMP \\nis used to establish the technical content of the \\nengineering work early in the Formulation Phase \\nfor each project and updated as needed through -\\nout the project life cycle. Table J-1 provides some \\nhigh level guidance on the scope of SEMP con -\\ntent based on the life cycle phase.\\n• Project requirements that have been tailored or \\nsignificant customization of SE processes should \\nbe described in the SEMP.\\n• For multi-level projects, the SEMP should be consis -\\ntent with higher-level SEMPs and the project plan.• For a technical effort that is contracted, the SEMP \\nshould include details on developing requirements \\nfor source selection, monitoring performance, and \\ntransferring and integrating externally produced \\nproducts to NASA.\\nJ.2 Terms Used\\nTerms used in the SEMP should have the same mean -\\ning as the terms used in the NPR 7123.1, Systems \\nEngineering Processes and Requirements.225\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix J: SEMP Content Outline\\nJ.3 Annotated Outline\\nTitle Page\\nSystems Engineering Management Plan\\n(Provide a title for the candidate program/project and designate  \\na short title or proposed acronym in parenthesis, if appropriate.)\\n.\\n.\\n.\\nDesignated Governing Authority/Technical Authority Date\\nProgram/Project Manager Date\\nChief Engineer Date\\nDate\\nDate\\nBy signing this document, signatories are certifying that the content herein is acceptable as direction \\nfor engineering and technical management of this program/project and that they will ensure its \\nimplementation by those over whom they have authority.226\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix J: SEMP Content Outline\\n1.0 Purpose and Scope\\nThis section provides a brief description of the purpose, \\nscope, and content of the SEMP.\\n• Purpose: This section should highlight the intent of \\nthe SEMP to provide the basis for implementing and \\ncommunicating the technical effort.\\n• Scope: The scope describes the work that encompasses \\nthe SE technical effort required to generate the work \\nproducts. The plan is used by the technical team to \\nprovide personnel the information necessary to suc -\\ncessfully accomplish the required task.\\n• Content:  This section should briefly describe the orga -\\nnization of the document.\\n2.0 Applicable Documents\\nThis section of the SEMP lists the documents applicable \\nto this specific project and its SEMP implementation. \\nThis section should list major standards and procedures \\nthat this technical effort for this specific project needs \\nto follow. Examples of specific procedures to list could \\ninclude procedures for hazardous material handling, \\ncrew training plans for control room operations, special \\ninstrumentation techniques, special interface documen -\\ntation for vehicles, and maintenance procedures specific \\nto the project.\\n3.0 Technical Summary\\nThis section contains an executive summary describing \\nthe problem to be solved by this technical effort and the \\npurpose, context, and products to be developed and inte -\\ngrated with other interfacing systems identified.\\nKey Questions\\n1. What is the problem we’re trying to solve?\\n2. What are the influencing factors?\\n3. What are the critical questions?4. What are the overall project constraints \\nin terms of cost, schedule, and technical \\nperformance\\n5. How will we know when we have adequately \\ndefined the problem?\\n6. Who are the customers?\\n7. Who are the users?\\n8. What are the customer and user priorities?\\n9. What is the relationship to other projects?\\n3.1 System Description\\nThis section contains a definition of the purpose of the \\nsystem being developed and a brief description of the \\npurpose of the products of the product layer of the system \\nstructure to which this SEMP applies. Each product layer \\nincludes the system end products and their subsystems \\nand the supporting or enabling products and any other \\nwork products (plans, baselines) required for the devel -\\nopment of the system. The description should include \\nany interfacing systems and system products, including \\nhumans with which the system products will interact \\nphysically, cognitively, functionally, or electronically.\\n3.2 System Structure\\nThis section contains an explanation of how the tech -\\nnical portion of the product layer (including enabling \\nproducts, technical cost, and technical schedule) will be \\ndeveloped, how the resulting product layers will be inte -\\ngrated into the project portion of the WBS, and how the \\noverall system structure will be developed. This section \\ncontains a description of the relationship of the specifica -\\ntion tree and the drawing tree with the products of the \\nsystem structure and how the relationship and interfaces \\nof the system end products and their life cycle-enabling \\nproducts will be managed throughout the planned tech -\\nnical effort.\\n3.3 Product Integration\\nThis section contains an explanation of how the products \\nwill be integrated and describes clear organizational 227\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix J: SEMP Content Outline\\nresponsibilities and interdependencies and whether the \\norganizations are geographically dispersed or managed \\nacross Centers. This section should also address how \\nproducts created under a diverse set of contracts are to \\nbe integrated, including roles and responsibilities. This \\nincludes identifying organizations—intra-and inter-\\nNASA, other Government agencies, contractors, or \\nother partners—and delineating their roles and respon -\\nsibilities. Product integration includes the integration of \\nanalytical products.\\nWhen components or elements will be available for inte -\\ngration needs to be clearly understood and identified on \\nthe schedule to establish critical schedule issues.\\n3.4 Planning Context\\nThis section contains the programmatic constraints (e.g., \\nNPR 7120.5) that affect the planning and implemen -\\ntation of the common technical processes to be applied \\nin performing the technical effort. The constraints pro -\\nvide a linkage of the technical effort with the applicable \\nproduct life cycle phases covered by the SEMP including, \\nas applicable, milestone decision gates, major technical \\nreviews, key intermediate events leading to project com -\\npletion, life cycle phase, event entry and success criteria, \\nand major baseline and other work products to be deliv -\\nered to the sponsor or customer of the technical effort.\\n3.5 Boundary of Technical Effort\\nThis section contains a description of the boundary of \\nthe general problem to be solved by the technical effort, \\nincluding technical and project constraints that affect \\nthe planning. Specifically, it identifies what can be con -\\ntrolled by the technical team (inside the boundary) and \\nwhat influences the technical effort and is influenced by \\nthe technical effort but not controlled by the technical \\nteam (outside the boundary). Specific attention should \\nbe given to physical, cognitive, functional, and elec -\\ntronic interfaces across the boundary.A description of the boundary of the system can include \\nthe following:\\n• Definition of internal and external elements/items \\ninvolved in realizing the system purpose as well as \\nthe system boundaries in terms of space, time, physi -\\ncal, and operational.\\n• Identification of what initiates the transitions of \\nthe system to operational status and what initiates \\nits disposal is important. General and functional \\ndescriptions of the subsystems inside the boundary.\\n• Current and established subsystem performance \\ncharacteristics.\\n• Interfaces and interface characteristics.\\n• Functional interface descriptions and functional \\nflow diagrams.\\n• Key performance interface characteristics.\\n• Current integration strategies and architecture.\\n• Documented Human System Integration Plan (HSIP)\\n3.6 Cross References\\nThis section contains cross references to appropriate \\nnontechnical plans and critical reference material that \\ninterface with the technical effort. It contains a sum -\\nmary description of how the technical activities covered \\nin other plans are accomplished as fully integrated parts \\nof the technical effort.\\n4.0 Technical Effort Integration\\nThis section describes how the various inputs to the tech -\\nnical effort will be integrated into a coordinated effort \\nthat meets cost, schedule, and performance objectives.228\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix J: SEMP Content Outline\\nThe section should describe the integration and coordina -\\ntion of the specialty engineering disciplines into the systems \\nengineering process during each iteration of the processes. \\nWhere there is potential for overlap of specialty efforts, \\nthe SEMP should define the relative responsibilities and \\nauthorities of each specialty. This section should contain, \\nas needed, the project’s approach to the following:\\n• Concurrent engineering\\n• The activity phasing of specialty engineering\\n• The participation of specialty disciplines\\n• The involvement of specialty disciplines,\\n• The role and responsibility of specialty disciplines,\\n• The participation of specialty disciplines in system \\ndecomposition and definition\\n• The role of specialty disciplines in verification and \\nvalidation\\n• Reliability\\n• Maintainability\\n• Quality assurance\\n• Integrated logistics\\n• Human engineering\\n• Safety\\n• Producibility\\n• Survivability/vulnerability\\n• National Environmental Policy Act (NEPA) \\ncompliance\\n• Launch approval/flight readiness\\nThe approach for coordination of diverse technical dis -\\nciplines and integration of the development tasks should \\nbe described. For example, this can include the use of \\nmultidiscipline integrated teaming approaches—e.g., \\nan HSI team—or specialized control boards. The scope \\nand timing of the specialty engineering tasks should be \\ndescribed along with how specialty engineering disci -\\nplines are represented on all technical teams and during \\nall life cycle phases of the project.4.1 Responsibility and Authority\\nThis section describes the organizing structure for the \\ntechnical teams assigned to this technical effort and \\nincludes how the teams will be staffed and managed.\\nKey Questions\\n1. What organization/panel will serve as the des -\\nignated governing authority for this project?\\n2. How will multidisciplinary teamwork be achieved?\\n3. What are the roles, responsibilities, and \\nauthorities required to perform the activities \\nof each planned common technical process?\\n4. What should be the planned technical staff -\\ning by discipline and expertise level?\\n5. What is required for technical staff training?\\n6. How will the assignment of roles, respon -\\nsibilities, and authorities to appropriate \\nproject stakeholders or technical teams \\nbe accomplished?\\n7. How are we going to structure the project to \\nenable this problem to be solved on sched -\\nule and within cost?\\n8. What does systems engineering manage -\\nment bring to the table?\\nThe section should provide an organization chart and \\ndenote who on the team is responsible for each activity. It \\nshould indicate the lines of authority and responsibility. \\nIt should define the resolution authority to make deci -\\nsions/decision process. It should show how the engineers/\\nengineering disciplines relate.\\nThe systems engineering roles and responsibilities need \\nto be addressed for the following: project office, user, \\nContracting Officer’s Representative (COR), systems \\nengineering, design engineering, specialty engineering, \\nand contractor.229\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix J: SEMP Content Outline\\n4.2 Contractor Integration\\nThis section describes how the technical effort of in-house \\nand external contractors is to be integrated with the \\nNASA technical team efforts. The established technical \\nagreements should be described along with how contrac -\\ntor progress will be monitored against the agreement, \\nhow technical work or product requirement change \\nrequests will be handled, and how deliverables will be \\naccepted. The section specifically addresses how inter -\\nfaces between the NASA technical team and the con -\\ntractor will be implemented for each of the 17 common \\ntechnical processes. For example, it addresses how the \\nNASA technical team will be involved with reviewing \\nor controlling contractor-generated design solution defi -\\nnition documentation or how the technical team will be \\ninvolved with product verification and product valida -\\ntion activities.\\nKey deliverables for the contractor to complete their sys -\\ntems and those required of the contractor for other proj -\\nect participants need to be identified and established on \\nthe schedule.\\n4.3 Analytical Tools that Support Integration\\nThis section describes the methods (such as integrated \\ncomputer-aided tool sets, integrated work product data -\\nbases, and technical management information systems) \\nthat will be used to support technical effort integration.\\n5.0  Common Technical Processes \\nImplementation\\nEach of the 17 common technical processes will have a \\nseparate subsection that contains a plan for performing \\nthe required process activities as appropriately tailored. \\n(See NPR 7123.1 for the process activities required and \\ntailoring.) Implementation of the 17 common techni -\\ncal processes includes (1) the generation of the outcomes \\nneeded to satisfy the entry and success criteria of the \\napplicable product life cycle phase or phases identified in \\nD.4.4.4, and (2) the necessary inputs for other technical processes. These sections contain a description of the \\napproach, methods, and tools for:\\n• Identifying and obtaining adequate human and \\nnonhuman resources for performing the planned \\nprocess, developing the work products, and providing \\nthe services of the process.\\n• Assigning responsibility and authority for perform -\\ning the planned process (e.g., RACI matrix, [ http://\\nen.wikipedia.org/wiki/Responsibility_assignment_\\nmatrix ]), developing the work products, and provid -\\ning the services of the process.\\n• Training the technical staff performing or supporting \\nthe process, where training is identified as needed.\\n• Designating and placing designated work products \\nof the process under appropriate levels of configura -\\ntion management.\\n• Identifying and involving stakeholders of the process.\\n• Monitoring and controlling the systems engineering \\nprocesses.\\n• Identifying, defining, and tracking metrics and \\nsuccess.\\n• Objectively evaluating adherence of the process and \\nthe work products and services of the process to the \\napplicable requirements, objectives, and standards \\nand addressing noncompliance.\\n• Reviewing activities, status, and results of the process \\nwith appropriate levels of management and resolving \\nissues.\\nThis section should also include the project-specific \\ndescription of each of the 17 processes to be used, including 230\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix J: SEMP Content Outline\\nthe specific tailoring of the requirements to the system \\nand the project; the procedures to be used in implement -\\ning the processes; in-house documentation; trade study \\nmethodology; types of mathematical and/or simulation \\nmodels to be used; and generation of specifications.\\nKey Questions\\n1. What are the systems engineering processes \\nfor this project?\\n2. What are the methods that we will apply for \\neach systems engineering task?\\n3. What are the tools we will use to support these \\nmethods? How will the tools be integrated?\\n4. How will we control configuration development?\\n5. How and when will we conduct technical \\nreviews?\\n6. How will we establish the need for and man -\\nage trade-off studies?\\n7. Who has authorization for technical change \\ncontrol?\\n8. How will we manage requirements? inter -\\nfaces? documentation?\\n6.0 Technology Insertion\\nThis section describes the approach and methods for \\nidentifying key technologies and their associated risks \\nand criteria for assessing and inserting technologies, \\nincluding those for inserting critical technologies from \\ntechnology development projects. An approach should be \\ndeveloped for appropriate level and timing of technology \\ninsertion. This could include alternative approaches to \\ntake advantage of new technologies to meet systems needs \\nas well as alternative options if the technologies do not \\nprove appropriate in result or timing. The strategy for \\nan initial technology assessment within the scope of the \\nproject requirements should be provided to identify tech -\\nnology constraints for the system.Key Questions\\n1. How and when will we insert new of special \\ntechnology into the project?\\n2. What is the relationship to research and \\ndevelopment efforts? How will they sup -\\nport the project? How will the results be \\nincorporated?\\n3. How will we incorporate system elements \\nprovided by others? How will these items be \\ncertified for adequacy?\\n4. What facilities are required?\\n5. When and how will these items be transi -\\ntioned to be part of the configuration?\\n7.0  Additional SE Functions and \\nActivities\\nThis section describes other areas not specifically included \\nin previous sections but that are essential for proper plan -\\nning and conduct of the overall technical effort.\\n7.1 System Safety\\nThis section describes the approach and methods for con -\\nducting safety analysis and assessing the risk to operators, \\nthe system, the environment, or the public.\\n7.2 Engineering Methods and Tools\\nThis section describes the methods and tools that are \\nnot included in the technology insertion sections but \\nare needed to support the overall technical effort. It \\nidentifies those tools to be acquired and tool training \\nrequirements.\\nThis section defines the development environment for \\nthe project, including automation, simulation, and soft -\\nware tools. If required, it describes the tools and facilities \\nthat need to be developed or acquired for all disciplines \\non the project. It describes important enabling strategies \\nsuch as standardizing tools across the project, or utiliz -\\ning a common input and output format to support a 231\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix J: SEMP Content Outline\\nbroad range of tools used on the project. It defines the \\nrequirements for information management systems and \\nfor using existing elements. It defines and plans for the \\ntraining required to use the tools and technology across \\nthe project.\\n7.3 Specialty Engineering\\nThis section describes engineering discipline and spe -\\ncialty requirements that apply across projects and the \\nWBS models of the system structure. Examples of these \\nrequirement areas would include planning for safety, \\nreliability, human factors, logistics, maintainability, \\nquality, operability, and supportability. It includes esti -\\nmates of staffing levels for these disciplines and incorpo -\\nrates them with the project requirements.\\n7.4 Technical Performance Measures\\na. This section describes the TPMs that have been \\nderived from the MOEs and MOPs for the project. \\nThe TPMs are used to define and track the techni -\\ncal progress of the systems engineering effort. (The \\nunique identification numbers in red reference the \\ncorresponding requirement in NPR 7123.1.) The \\nperformance metrics need to address the minimally \\nrequired TPMs as defined in NPR 7123.1. These \\ninclude:\\n1. Mass margins for projects involving hardware  \\n[SE-62] .\\n2. Power margins for projects that are powered \\n[SE-63] .\\n3. Review Trends including closure of review action \\ndocumentation (Request for Action, Review Item \\nDiscrepancies, and/or Action Items as established \\nby the project) for all software and hardware \\nprojects [SE-64] .b. Other performance measure that should be con -\\nsidered by the project include:\\n »Requirement trends (percent growth, TBD/TBR \\nclosures, number of requirement changes);\\n »Interface trends (percent ICD approval, TBD/\\nTBR burndown, number of interface require -\\nment changes);\\n »Verification trends (closure burndown, number \\nof deviations/waivers approved/open);\\n »Software-unique trends (number of require -\\nments per build/release versus plan);\\n »Problem report/discrepancy report trends (num -\\nber open, number closed);\\n »Cost trends (plan, actual, UFE, EVM, NOA);\\n »Schedule trends (critical path slack/float, critical \\nmilestone dates); and\\n »Staffing trends (FTE, WYE).\\nKey Questions\\n1. What metrics will be used to measure tech -\\nnical progress?\\n2. What metrics will be used to identify process \\nimprovement opportunities?\\n3. How will we measure progress against the \\nplans and schedules?\\n4. How often will progress be reported? By \\nwhom? To whom?\\n7.5 Heritage\\nThis section describes the heritage or legacy products that \\nwill be used in the project. It should include a discussion \\nof which products are planned to be used, the rationale \\nfor their use, and the analysis or testing needed to assure \\nthey will perform as intended in the stated use.232\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix J: SEMP Content Outline\\n7.6 Other\\nThis section is reserved to describe any unique SE func -\\ntions or activities for the project that are not covered in \\nother sections.\\n8.0  Integration with the Project Plan \\nand Technical Resource Allocation\\nThis section describes how the technical effort will inte -\\ngrate with project management and defines roles and \\nresponsibilities. It addresses how technical requirements \\nwill be integrated with the project plan to determine \\nthe allocation of resources, including cost, schedule, and \\npersonnel, and how changes to the allocations will be \\ncoordinated.\\nKey Questions\\n1. How will we assess risk? What thresholds \\nare needed for triggering mitigation activi -\\nties? How will we integrate risk management \\ninto the technical decision process?\\n2. How will we communicate across and out -\\nside of the project?\\n3. How will we record decisions?\\n4. How do we incorporate lessons learned from \\nother projects?\\nThis section describes the interface between all of the tech -\\nnical aspects of the project and the overall project man -\\nagement process during the systems engineering planning \\nactivities and updates. All activities to coordinate tech -\\nnical efforts with the overall project are included, such \\nas technical interactions with the external stakeholders, \\nusers, and contractors.9.0 Compliance Matrices\\nAppendix H.2 in NPR 7123.1A is the basis for the com -\\npliance matrix for this section of the SEMP. The proj -\\nect will complete this matrix from the point of view of \\nthe project and the technical scope. Each requirement \\nwill be addressed as compliant, partially compliant, or \\nnoncompliant. Compliant requirements should indicate \\nwhich process or activity addresses the compliance. For \\nexample, compliance can be accomplished by using a \\nCenter process or by using a project process as described \\nin another section of the SEMP or by reference to another \\ndocumented process. Noncompliant areas should state \\nthe rationale for noncompliance.\\nAppendices\\nAppendices are included, as necessary, to provide a \\nglossary, acronyms and abbreviations, and informa -\\ntion published separately for convenience in document \\nmaintenance. Included are: (1) information that may \\nbe pertinent to multiple topic areas (e.g., description of \\nmethods or procedures); (2) charts and proprietary data \\napplicable to the technical efforts required in the SEMP; \\nand (3) a summary of technical plans associated with \\nthe project. Each appendix should be referenced in one \\nof the sections of the engineering plan where data would \\nnormally have been provided.\\nTemplates\\nAny templates for forms, plans, or reports the technical \\nteam will need to fill out, like the format for the veri -\\nfication and validation plan, should be included in the \\nappendices.\\nReferences\\nThis section contains all documents referenced in the text \\nof the SEMP.233\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix J: SEMP Content OutlineTABLE J-1  Guidance on SEMP Content per Life-Cycle Phase\\nSEMP \\nSectionSEMP \\nSubsec -\\ntionPre-Phase A\\nKDP APhase A\\nKDP BPhase B\\nKDP CPhase C\\nKDP DPhase D\\nKDP EPhase E\\nKDP FPhase F\\nMCR SRR SDR/MDR PDR CDR SIR ORR MRR/FRR DR DRR\\nPurpose and \\nScopeFinal Final Final Final Final Final Final Final Final Final\\nApplicable DocumentsInitial Initial Initial Final Final Final Final Final Final Final\\nTechnical SummaryFinal Final Final Final Final Final Final Final Final Final\\nSystem DescriptionInitial Initial Initial Final Final Final Final Final Final Final\\nSystem StructureProduct IntegrationDefine thru SDR \\ntimeframeDefine \\nthru SDR \\ntimeframeDefine \\nthru SDR \\ntimeframeDefine thru \\nSIRDefine thru SIRDefine thru SIRDefine sustaining thru \\nend of programDefine \\nsustaining \\nthru end of \\nprogramDefine sustaining \\nthru end of \\nprogramDefine sustaining \\nthru end of \\nprogram\\nPlanning \\nContextDefine thru SDR \\ntimeframeDefine \\nthru SDR \\ntimeframeDefine \\nthru SDR \\ntimeframeDefine thru \\nSIRDefine thru SIRDefine thru SIRDefine sustaining thru \\nend of programDefine \\nsustaining \\nthru end of \\nprogramDefine sustaining \\nthru end of \\nprogramDefine sustaining \\nthru end of \\nprogram\\nBoundary \\nof Technical \\nEffortInitial Initial Initial Final Final Final Final Final Final Final\\nCross ReferencesInitial Initial Initial Final Final Final Final Final Final Final\\nTechnical Effort IntegrationResponsi -\\nbility and AuthorityDefine thru SDR timeframeDefine thru SDR timeframeDefine thru SDR timeframeDefine thru SIR timeframeDefine thru SIR timeframeDefine thru SIR timeframeDefine sustain -\\ning Roles and Responsibilities \\nthrough end of \\nprogramDefine sus -\\ntaining Roles and Respon -\\nsibilities through end \\nof programDefine sus -\\ntaining Roles \\nand Respon -\\nsibilities through end \\nof program\\nContractor \\nIntegrationDefine acquisitions neededDefine insight/oversight \\nthrough SIR \\ntimeframeDefine sustaining insight/\\noversight \\nthrough end of program\\nSupport IntegrationDefine acquisitions neededDefine insight/oversight \\nthrough SIR \\ntimeframeDefine sus -\\ntaining insight/oversight \\nthrough end of \\nprogram\\n(continued)234\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix J: SEMP Content OutlineSEMP \\nSectionSEMP \\nSubsec -\\ntionPre-Phase A\\nKDP APhase A\\nKDP BPhase B\\nKDP CPhase C\\nKDP DPhase D\\nKDP EPhase E\\nKDP FPhase F\\nMCR SRR SDR/MDR PDR CDR SIR ORR MRR/FRR DR DRR\\nCommon \\nTechnical Pro -\\ncesses Imple -\\nmentationProcesses \\ndefined for \\nConcept \\nDevelop -\\nment and \\nFormulationProcesses \\ndefined for \\nthe Design \\nPhaseProcesses \\nadded for the \\nintegration \\nand \\nOperations \\nPhaseUpdate \\nOperations \\nprocesses. \\nDefine close \\nout processes \\nand sustaining \\nengineering \\nprocesses\\nTechnology \\nInsertionDefine \\ntechnologies \\nto be \\ndevelopedDefine deci -\\nsion process \\nfor on ramps \\nand off \\nramps of \\ntechnology \\neffortsDefine technol -\\nogy sustaining \\neffort through \\nend of program.\\nAdditional SE \\nFunctions and \\nActivitiesSystem \\nSafetyDefine process \\nthrough CDRDefine sustain -\\ning Roles and \\nResponsibilities \\nthrough end of \\nprogram\\nEngineering \\nMethods \\nand toolsDefine process \\nthrough CDRDefine sustain -\\ning Roles and \\nResponsibilities \\nthrough end of \\nprogram\\nSpecialty \\nEngineeringDefine process \\nthrough CDRDefine sustain -\\ning Roles and \\nResponsibilities \\nthrough end of \\nprogram\\nIntegration \\nwith the \\nProject Plan \\nand Technical \\nResource \\nAllocationDefine \\nthrough SDR \\ntimeframeDefine \\nthrough SIRDefine \\nthrough SIRDefine \\nthrough SIRDefine \\nsustaining \\nthrough end of \\nprogramDefine \\nsustaining \\nthrough end \\nof programDefine \\nsustaining \\nthrough end \\nof programDefine \\nsustaining \\nthrough end \\nof program\\nCompliance \\nMatrix \\n(Appendix H.2 \\nof SE NPR)Initial Initial Initial Final Final Final Final Final Final Final\\nAppendices As required As required As required As required As required As required As required As required As required As required\\nTemplates As required As required As required As required As required As required As required As required As required As required\\nReferences As required As required As required As required As required As required As required As required As required As required235\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nAppendix K: Technical Plans\\nThe following table represents a typical expectation of maturity of some of the key technical plans developed \\nduring the SE processes. This example is for a space flight project. Requirements for work product maturity can be found in the governing PM document (i.e., NPR 7120.5) for the associated type of project.\\nTABLE K-1  Example of Expected Maturity of Key Technical Plans\\nPlan Pre-\\nPhase APhase A Phase BPhase C Phase D Phase EPhase FRef. Page\\nMCR SRR SDR/MDRPDR CDR SIR ORR MRR/ FRRDR DRR\\nSystems Engineering \\nManagement PlanP B U U U U U U U U\\nRisk Management Plan A B U U U\\nIntegrated Logistics Support PlanA P P B U\\nTechnology Development PlanB U U U\\nReview Plan P B U U U U U U U U\\nVerification and Validation PlanA A P B U\\nIntegration Plan P B U\\nConfiguration Management PlanB U U\\nData Management Plan B U U\\nHuman Systems Integration PlanB U U U\\nSoftware Management PlanP B U\\nReliability and Maintainability PlanP B U\\nMission Operations Plan P B U\\nProject Protection Plan P B U U U U U U\\nDecommissioning Plan A B U\\nDisposal Plan A B U U\\nA = Approach  B = B aseline  P = P reliminary  U = U pdate236\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nAppendix L:  I nterface Requirements  \\nDocument Outline\\n1.0 Introduction\\n1.1 Purpose and Scope\\nState the purpose of this document and briefly iden -\\ntify the interface to be defined. (For example, “This \\nIRD defines and controls the interface(s) requirements \\nbetween ______ and ______.”)\\n1.2 Precedence\\nDefine the relationship of this document to other pro -\\ngram documents and specify which is controlling in the \\nevent of a conflict.\\n1.3  Responsibility and Change \\nAuthority\\nState the responsibilities of the interfacing organiza -\\ntions for development of this document and its contents. \\nDefine document approval authority (including change \\napproval authority).\\n2.0 Documents\\n2.1 Applicable Documents\\nList binding documents that are invoked to the extent \\nspecified in this IRD. The latest revision or most recent version should be listed. Documents and requirements \\nimposed by higher-level documents (higher order of pre -\\ncedence) should not be repeated.\\n2.2 Reference Documents\\nList any document that is referenced in the text in this \\nsubsection.3.0 Interfaces\\n3.1 General\\nIn the subsections that follow, provide the detailed descrip -\\ntion, responsibilities, coordinate systems, and numerical requirements as they relate to the interface plane.\\n3.1.1  Interface Description\\nDescribe the interface as defined in the system specifi -\\ncation. Use tables, figures, or drawings as appropriate.\\n3.1.2  Interface Responsibilities\\nDefine interface hardware and interface boundary responsibilities to depict the interface plane. Use tables, figures, or drawings as appropriate.\\n3.1.3  Coordinate Systems\\nDefine the coordinate system used for interface require -\\nments on each side of the interface. Use tables, figures, or drawings as appropriate.\\n3.1.4   Engineering Units, Tolerances, and  \\nConversion\\nDefine the measurement units along with tolerances. If required, define the conversion between measure -\\nment systems.\\n3.2 Interface Requirements\\nIn the subsections that follow, define structural limiting values at the interface, such as interface loads, forcing functions, and dynamic conditions. Define the interface \\nrequirements on each side of the interface plane.237\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix L: Interface RequirementsDocument Outline \\n3.2.1 Mass Properties\\nDefine the derived interface requirements based on the \\nallocated requirements contained in the applicable speci -\\nfication pertaining to that side of the interface. For exam -\\nple, this subsection should cover the mass of the element.\\n3.2.2 Structural/Mechanical\\nDefine the derived interface requirements based on \\nthe allocated requirements contained in the applicable \\nspecification pertaining to that side of the interface. For \\nexample, this subsection should cover attachment, stiff -\\nness, latching, and mechanisms.\\n3.2.3 Fluid\\nDefine the derived interface requirements based on \\nthe allocated requirements contained in the applicable \\nspecification pertaining to that side of the interface. For \\nexample, this subsection should cover fluid areas such as \\nthermal control, O2 and N2, potable and waste water, \\nfuel cell water, and atmospheric sampling.\\n3.2.4 Electrical (Power)\\nDefine the derived interface requirements based on \\nthe allocated requirements contained in the applicable \\nspecification pertaining to that side of the interface. For \\nexample, this subsection should cover various electric \\ncurrent, voltage, wattage, and resistance levels.\\n3.2.5 Electronic (Signal)\\nDefine the derived interface requirements based on \\nthe allocated requirements contained in the applicable \\nspecification pertaining to that side of the interface. For \\nexample, this subsection should cover various signal types \\nsuch as audio, video, command data handling, and \\nnavigation.\\n3.2.6 Software and Data\\nDefine the derived interface requirements based on \\nthe allocated requirements contained in the applicable \\nspecification pertaining to that side of the interface. For example, this subsection should cover various data stan -\\ndards, message timing, protocols, error detection/correc -\\ntion, functions, initialization, and status.\\n3.2.7 Environments\\nDefine the derived interface requirements based on \\nthe allocated requirements contained in the applicable \\nspecification pertaining to that side of the interface. For \\nexample, cover the dynamic envelope measures of the ele -\\nment in English units or the metric equivalent on this \\nside of the interface.\\n3.2.7.1 Electromagnetic Effects\\n3.2.7.1.a  Electromagnetic Compatibility\\nDefine the appropriate electromagnetic compatibility \\nrequirements. For example, the end-item-1-to-end-item-2 \\ninterface shall meet the requirements [to be determined] \\nof systems requirements for electromagnetic compatibility.\\n3.2.7.1.b  Electromagnetic Interference\\nDefine the appropriate electromagnetic interference \\nrequirements. For example, the end-item-1-to-end-\\nitem-2 interface shall meet the requirements [to be \\ndetermined] of electromagnetic emission and susceptibil -\\nity requirements for electromagnetic compatibility.\\n3.2.7.1.c  Grounding\\nDefine the appropriate grounding requirements. For exam -\\nple, the end-item-1-to-end-item-2 interface shall meet the \\nrequirements [to be determined] of grounding requirements.\\n3.2.7.1.d  Bonding\\nDefine the appropriate bonding requirements. For \\nexample, the end-item-1-to-end-item-2 structural/\\nmechanical interface shall meet the requirements [to be \\ndetermined] of electrical bonding requirements.\\n3.2.7.1.e Cable and Wire Design\\nDefine the appropriate cable and wire design require -\\nments. For example, the end-item-1-to-end-item-2 238\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix L: Interface RequirementsDocument Outline \\ncable and wire interface shall meet the requirements [to \\nbe determined] of cable/wire design and control require -\\nments for electromagnetic compatibility.\\n3.2.7.2 Acoustic\\nDefine the appropriate acoustics requirements. Define \\nthe acoustic noise levels on each side of the interface in \\naccordance with program or project requirements.\\n3.2.7.3 Structural Loads\\nDefine the appropriate structural loads requirements. Define \\nthe mated loads that each end item should accommodate.3.2.7.4 Vibroacoustics\\nDefine the appropriate vibroacoustics requirements. \\nDefine the vibroacoustic loads that each end item should \\naccommodate.\\n3.2.7.5 Human Operability\\nDefine the appropriate human interface requirements. \\nDefine the human-centered design considerations, con -\\nstraints, and capabilities that each end item should \\naccommodate.\\n3.2.8 Other Types of Interface Requirements\\nDefine other types of unique interface requirements that \\nmay be applicable.239\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nAppendix M: CM Plan Outline\\nA comprehensive Configuration Management (CM) \\nPlan that reflects efficient application of configura -\\ntion management principles and practices would nor -\\nmally include the following topics:\\n• General product definition and scope\\n• Description of CM activities and procedures for each major CM function\\n• Organization, roles, responsibilities, and resources\\n• Definitions of terms\\n• Programmatic and organizational interfaces\\n• Deliverables, milestones, and schedules\\n• Subcontract flow down requirementsThe documented CM planning should be reevalu -\\nated following any significant change affecting the context and environment, e.g., changes in suppliers or supplier responsibilities, changes in diminishing \\nmanufacturing sources/part obsolescence, changes in \\nresource availabilities, changes in customer contract, and changes in the product. CM planning should also be reviewed on a periodic basis to make sure that an organization’s application of CM functions is current.\\nFor additional information regarding a CM Plan, \\nrefer to SAE EIA-649, Rev. B.240\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nAppendix N:  Guidance on Technical Peer Reviews/\\nInspections\\nThis appendix has been removed. For additional guid -\\nance on how to perform technical peer reviews refer \\nto Appendix N in the NASA Expanded Guidance for \\nSystems Engineering at https://nen.nasa.gov/web/se/\\ndoc-repository .241\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nAppendix O: Reserved242\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nAppendix P: SOW Review Checklist\\nThis appendix has been removed. For additional \\nguidance on checklists for editorial and content review questions refer to Appendix P in the NASA \\nExpanded Guidance for Systems Engineering at \\nhttps://nen.nasa.gov/web/se/doc-repository .243\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nAppendix Q: Reserved244\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nAppendix R: HSI Plan Content Outline\\nR.1 HSI Plan Overview\\nThe Human Systems Integration (HSI) Plan docu -\\nments the strategy for and planned implementation \\nof HSI through a particular program’s/project’s life cycle. The intent of HSI is:\\n• To ensure the human elements of the total system are effectively integrated with hardware and soft -\\nware elements,\\n• To ensure all human capital required to develop and operate the system is accounted for in life cycle costing, and\\n• To ensure that the system is built to accommodate the characteristics of the user population that will operate, maintain, and support the system.\\nThe HSI Plan is specific to a program or project and applies to NASA systems engineering per NPR 7123.1, NASA Systems Engineering Processes and Requirements. The HSI Plan should address the following:\\n• Roles and responsibilities for integration across HSI domains;\\n• Roles and responsibilities for coordinating inte -\\ngrated HSI domain inputs with the program team and stakeholders;\\n• HSI goals and deliverables for each phase of the life cycle;• Entry and exit criteria with defined metrics for each phase, review, and milestone;\\n• Planned methods, tools, requirements, processes, and standards for conducting HSI;\\n• Strategies for identifying and resolving HSI risks; and\\n• Alignment strategy with the SEMP.\\nThe party or parties responsible for program/project HSI implementation—e.g., an HSI integrator (or team)—should be identified by the program/proj -\\nect manager. The HSI integrator or team develops and maintains the HSI Plan with support from and coordination with the project manager and systems engineer.\\nImplementation of HSI on a program/project utilizes \\nmany of the tools and products already required by systems engineering; e.g., development of a ConOps, clear functional allocation across the elements of a system (hardware, software, and human), and the use of key performance measurements through the life cycle to validate and verify HSI’s effectiveness. It is not the intent of the HSI Plan or its implementation to duplicate other systems engineering plans or pro -\\ncesses, but rather to define the uniquely HSI effort being made to ensure the human element is given equal consideration to hardware/software elements of a program/project.245\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix R: HSI Plan Content Outline\\nR.2 HSI Plan Content Outline\\nEach program/project-specific HSI Plan should be \\ntailored to fit the program/project’s size, scope, and \\npurpose. The following is an example outline for a \\nmajor program; e.g., space flight or aeronautics.\\n1.0 Introduction\\n1.1 Purpose\\nThis section briefly identifies the ultimate objectives for this \\nprogram/project’s HSI Plan. This section also introduces \\nthe intended implementers and users of this HSI Plan.\\n1.2 Scope\\nThis section describes the overall scope of the HSI Plan’s \\nrole in documenting the strategy for and implementa -\\ntion of HSI. Overall, this section describes that the HSI \\nPlan:\\n• Is a dynamic document that will be updated at key \\nlife cycle milestones.\\n• Is a planning and management guide that describes \\nhow HSI will be relevant to the program/project’s \\ngoals.\\n• Describes planned HSI methodology, tools, sched -\\nules, and deliverables.\\n• Identifies known program/project HSI issues and \\nconcerns and how their resolutions will be addressed.\\n• Defines program/project HSI organizational ele -\\nments, roles, and responsibilities.\\n• May serve as an audit trail that documents HSI \\ndata sources, analyses, activities, trade studies, and \\ndecisions not captured in other program/project \\ndocumentation.1.3 Definitions\\nThis section defines key HSI terms and references rele -\\nvant program/project-specific terms.\\n2.0 Applicable Documents\\nThis section lists all documents, references, and data \\nsources that are invoked by HSI’s implementation on \\nthe program/project, that have a direct impact on HSI \\noutcomes, and/or are impacted by the HSI effort.\\n3.0 HSI Objectives\\n3.1 System Description\\nThis section describes the system, missions to be per -\\nformed, expected operational environment(s), predecessor \\nand/or legacy systems (and lessons learned), capability \\ngaps, stage of development, etc. Additionally, reference \\nshould be made to the acquisition strategy for the system; \\ne.g., if it is developed in-house within NASA or if major \\nsystems are intended for external procurement. The over -\\nall strategy for program integration should be referenced.\\nNote that this information is likely captured in other \\nprogram/project documentation and can be referenced \\nin the HSI Plan rather than repeated.\\n3.2 HSI Relevance\\nAt a high level, this section describes HSI’s relevance \\nto the program/project; i.e., how the HSI strategy will \\nimprove the program/project’s outcome. Known HSI \\nchallenges should be described along with mention of \\nareas where human performance in the system’s oper -\\nations is predicted to directly impact the probability of \\noverall system performance and mission success.\\n4.0 HSI Strategy\\n4.1 HSI Strategy Summary\\nThis section summarizes the HSI approaches, planning, \\nmanagement, and strategies for the program/project. It \\nshould describe how HSI products will be integrated \\nacross all HSI domains and how HSI inputs to program/\\nproject systems engineering and management processes 246\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix R: HSI Plan Content Outline\\ncontribute to system performance and help contain life \\ncycle cost. This section (or Implementation Summary, \\nSection 6  of this outline) should include a top-level \\nschedule showing key HSI milestones.HSI RELEVANCE\\nKey Points\\n• Describe performance characteristics of the human elements known to be key drivers to a desired total \\nsystem performance outcome.\\n• Describe the total system performance goals that require HSI support.\\n• Identify HSI concerns with legacy systems; e.g., if operations and logistics, manpower, skill selection, required training, logistics support, operators’ time, maintenance, and/or risks to safety and success exceeded expectations.\\n• Identify potential cost, schedule, risk, and trade-off concerns with the integration of human elements; e.g., quantity and skills of operators, maintainers, ground controllers, etc.\\nHSI STRATEGY\\nKey Points\\n• Identify critical program/project-specific HSI key decision points that will be used to track HSI implementation and success.\\n• Identify key enabling (and particularly, emerging) technologies and methodologies that may be overlooked in hardware/software systems trade studies but that may positively contribute to HSI implementation; e.g., in the areas of human performance, workload, personnel management, training, safety, and survivability.\\n• Describe HSI products that will be integrated with program/project systems engineering products, analyses, risks, trade studies, and activities.\\n• Describe efforts to ensure HSI will contribute in critically important Phase A and Pre-Phase A cost-effective design concept studies.\\n• Describe the plan and schedule for updating the HSI Plan through the program/project life cycle.\\n4.2 HSI Domains\\nThis section identifies the HSI domains applicable to the \\nprogram/project including rationale for their relevance.5.0  HSI Requirements, Organization, \\nand Risk Management\\n5.1 HSI Requirements\\nThis section references HSI requirements and stan -\\ndards applicable to the program/project and identi -\\nfies the authority that invokes them; e.g., the NASA \\nProcedural Requirements (NPR) document(s) that \\ninvoke applicability.247\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix R: HSI Plan Content Outline\\nHSI DOMAINS\\nKey Points\\n• Identify any domain(s) associated with human performance capabilities and limitations whose integration \\ninto the program/project is likely to directly affect the probability of successful program/project outcome.\\n• An overview of processes to apply, document, validate, evaluate, and mitigate HSI domain knowledge and to integrate domain knowledge into integrated HSI inputs to program/project and systems engineering processes.\\nHSI REQUIREMENTS\\nKey Points\\n• Describe how HSI requirements that are invoked on the program/project contribute to mission success, affordability, operational effectiveness, and safety.\\n• HSI should include requirements that influence the system design to moderate manpower (operators, maintainers, system administrative, and support personnel), required skill sets (occupational specialties with high aptitude or skill requirements), and training requirements.\\n• Define the program/project-specific HSI strategy derived from NASA-STD-3001, NASA Space Flight Human-System Standard, Volume 2: Human Factors, Habitability, and Environmental Health, Standard 3.5 [V2 3005], “Human-Centered Design Process”, if applicable.\\n• Capture the development process and rationale for any program/project-specific requirements not derived from existing NASA standards. In particular, manpower, skill set, and training HSI requirements/goals may be so program/project-specific as to not have NASA parent standards or requirements.\\n• Identify functional connections between HSI measures of effectiveness used to verify requirements and key performance measures used throughout the life cycle as indicators of overall HSI effectiveness.\\n5.2 HSI Organization, Roles, and Responsibilities\\nIn this section, roles and responsibilities for program/\\nproject personnel assigned to facilitate and/or manage HSI tasks are defined; e.g., the HSI integrator (and/or team if required by NPR 8705.2). HSI integrator/team functional responsibilities to the program are described in addition to identification of organizational elements with HSI responsibilities. Describe the relationships between HSI integrator/team, stakeholders, engineering technical teams, and governing bodies (control boards).5.2.1  HSI Organization\\n• Describe the HSI management structure for the program/project and identify its leaders and membership.\\n• Reference the organizational structure of the pro -\\ngram (including industry partners) and describe the roles and responsibilities of the HSI integrator/team within that structure. Describe the HSI responsible party’s relationship to other teams, including those 248\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix R: HSI Plan Content Outline\\nfor systems engineering, logistics, risk management, \\ntest and evaluation, and requirements verification.\\n• Provide the relationship of responsible HSI person -\\nnel to NASA Technical Authorities (Engineering, \\nSafety, and Health/Medical).\\n• Identify if the program/project requires NASA- \\n(Government) and/or contractor-issued HSI Plans, \\nand identify the responsible author(s). Describe \\nhow NASA’s HSI personnel will monitor and assess \\ncontractor HSI activities. For contractor-issued \\nHSI Plans, identify requirements and processes for \\nNASA oversight and evaluation of HSI efforts by \\nsubcontractors.\\n5.2.2 HSI Roles & Responsibilities\\n• Describe the HSI responsible personnel’s functional \\nresponsibilities to the program/project, addressing (as \\nexamples) the following:\\n »developing HSI program documentation;\\n »validating human performance requirements;\\n »conducting HSI analyses;\\n »designing human machine interfaces to provide \\nthe level of human performance required for \\noperations, maintenance, and support, includ -\\ning conduct of training;\\n »describing the role of HSI experts in document -\\ning and reporting the results from tests and \\nevaluations.\\n• Define how collaboration will be performed within \\nthe HSI team, across program/project integrated \\nproduct teams and with the program/project man -\\nager and systems engineer.\\n• Define how the HSI Plan and the SEMP will be \\nkept aligned with each other.• Define responsibility for maintaining and updating \\nthe HSI Plan through the program/project’s life cycle.\\n5.3 HSI Issue and Risk Processing\\nThis section describes any HSI-unique processes for iden -\\ntifying and mitigating human system risks. HSI risks \\nshould be processed in the same manner and system as \\nother program/project risks (technical, programmatic, \\nschedule). However, human system risks may only be \\nrecognized by HSI domain and integration experts. \\nTherefore, it may be important to document any unique \\nprocedures by which the program/project HSI integra -\\ntor/team identifies, validates, prioritizes, and tracks the \\nstatus of HSI-specific risks through the program/project \\nrisk management system. Management of HSI risks may \\nbe deemed the responsibility of the program’s/project’s \\nHSI integrator/team in coordination with overall pro -\\ngram/project risk management.\\n• Ensure that potential cost, schedule, risk, and trade-\\noff concerns with the integration of human elements \\n(operators, maintainers, ground controllers, etc.) \\nwith the total system are identified and mitigated.\\n• Ensure that safety, health, or survivability concerns \\nthat arise as the system design and implementation \\nemerge are identified, tracked, and managed.\\n• Identify and describe any risks created by limitations \\non the overall program/project HSI effort (time, \\nfunding, insufficient availability of information, \\navailability of expertise, etc.).\\n• Describe any unique attributes of the process by \\nwhich the HSI integrator/team elevates HSI risks to \\nprogram/project risks.\\n• Describe any HSI-unique aspects of how human sys -\\ntem risk mitigation strategies are deemed effective.249\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix R: HSI Plan Content Outline\\n6.0 HSI Implementation\\n6.1 HSI Implementation Summary\\nThis section summarizes the HSI implementation \\napproach by program/project phase. This section shows \\nhow an HSI strategy for the particular program/project \\nis planned to be tactically enabled; i.e., establishment \\nof HSI priorities; description of specific activities, tools, \\nand products planned to ensure HSI objectives are met; \\napplication of technology in the achievement of HSI \\nobjectives; and an HSI risk processing strategy that \\nidentifies and mitigates technical and schedule concerns \\nwhen they first arise.\\n6.2 HSI Activities and Products\\nIn this section, map activities, resources, and products \\nassociated with planned HSI technical implementation \\nto each systems engineering phase of the program/project. \\nConsideration might be given to mapping the needs and \\nproducts of each HSI domain by program/project phase. \\nExamples of HSI activities include analyses, mockup/\\nprototype human-in-the-loop evaluations, simulation/\\nmodeling, participation in design and design reviews, formative evaluations, technical interchanges, and trade \\nstudies. Examples of HSI resources include acquisition \\nof unique/specific HSI skill sets and domain expertise, \\nfacilities, equipment, test articles, specific time alloca -\\ntions, etc.\\nWhen activities, products, or risks are tied to life cycle \\nreviews, they should include a description of the HSI \\nentrance and exit criteria to clearly define the boundaries \\nof each phase, as well as resource limitations that may be \\nassociated with each activity or product (time, funding, \\ndata availability, etc.). A high-level, summary example \\nlisting of HSI activities, products, and known risk mit -\\nigations by life cycle phase is provided in TABLE R.2-1 .\\n6.3 HSI Plan Update\\nThe HSI Plan should be updated throughout the pro -\\ngram/project’s life cycle management and systems \\nengineering processes at key milestones. Milestones rec -\\nommended for HSI Plan updates are listed in appendix \\nG of NPR 7123.1, NASA Systems Engineering Processes \\nand Requirements.\\nHSI IMPLEMENTATION\\nKey Points\\n• Relate HSI strategic objectives to the technical approaches planned for accomplishing these objectives.\\n• Overlay HSI milestones—e.g., requirements definition, verification, known trade studies, etc.—on the \\nprogram/project schedule and highlight any inconsistencies, conflicts, or other expected schedule \\nchallenges.\\n• Describe how critical HSI key decision points will be dealt with as the program/project progresses \\nthrough its life cycle. Indicate the plan to trace HSI key performance measures through the life cycle; \\ni.e., from requirements to human/system functional performance allocations, through design, test, and \\noperational readiness assessment.\\n• Identify HSI-unique systems engineering processes—e.g., verification using human-in-the-loop \\nevaluations—that may require special coordination with program/project processes.\\n• As the system emerges, indicate plans to identify HSI lessons learned from the application of HSI on the \\nprogram/project.\\n• Include a high-level summary of the resources required.250\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix R: HSI Plan Content Outline\\nTABLE R.2-1 HSI Activity, Product, or Risk Mitigation by Program/Project Phase\\nLife-Cycle \\nPhasePhase Description Activity, Product, or Risk Mitigation\\nPre-Phase A Concept Studies ConOps (Preliminary—to include training, maintenance, logistics, etc.)\\nPhase A Concept & Technology \\nDevelopmentHSI Plan (baseline)\\nConOps (initial)\\nHSI responsible party(ies) and/or team identified before SRR\\nDevelop mockup(s) for HSI evaluations\\nCrew Workload Evaluation Plan\\nFunctional allocation, crew task lists\\nValidation of ConOps (planning)\\nPhase B Preliminary Design & \\nTechnology CompletionHSI Plan (update)\\nConOps (baseline)\\nDevelop engineering-level mockup(s) for HSI evaluations\\nDefine crew environmental and crew health support needs (e.g., aircraft \\nflight decks, human space flight missions)\\nAssess operator interfaces through task analyses (for, e.g., aircraft \\ncockpit operations, air traffic management, spacecraft environments, \\nmission control for human space flight missions)\\nHuman-in-the-loop usability plan\\nHuman-rating report for PDR\\nPhase C Final Design & Fabrication HSI Plan (update)\\nFirst Article HSI Tests\\nHuman-rating report for CDR\\nPhase D System Assembly, Integ. & \\nTest, Launch & CheckoutHuman-rating report for ORR\\nValidation of human-centered design activities\\nValidation of ConOps\\nPhase E Operations & Sustainment Monitoring of human-centered design performance\\nPhase F Closeout Lessons learned report\\nHSI PLAN UPDATES\\nKey points to be addressed in each update\\n• Identify the current program/project phase, the publication date of the last iteration of the HSI Plan, and \\nthe HSI Plan version number. Update the HSI Plan revision history.\\n• Describe the HSI entrance criteria for the current phase and describe any unfinished work prior to the \\ncurrent phase.\\n• Describe the HSI exit criteria for the current program/project phase and the work that must be \\naccomplished to successfully complete the current program/project phase.251\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nAppendix S:  C oncept of Operations  \\nAnnotated Outline\\nThis Concept of Operations (ConOps) annotated \\noutline describes the type and sequence of informa -\\ntion that should be contained in a ConOps, although \\nthe exact content and sequence will be a function of \\nthe type, size, and complexity of the project. The text in italics describes the type of information that would be provided in the associated subsection. Additional subsections should be added as necessary to fully describe the envisioned system.\\nCover Page\\nTable of Contents\\n1.0 Introduction\\n1.1 Project Description\\nThis section will provide a brief overview of the devel -\\nopment activity and system context as delineated in the \\nfollowing two subsections.\\n1.1.1  Background\\nSummarize the conditions that created the need for the new system. Provide the high-level mission goals and objective of the system operation. Provide the rationale \\nfor the development of the system.\\n1.1.2  Assumptions and Constraints\\nState the basic assumptions and constraints in the devel -\\nopment of the concept. For example, that some technol -\\nogy will be matured enough by the time the system is ready to be fielded, or that the system has to be provided \\nby a certain date in order to accomplish the mission.\\n1.2 Overview of the Envisioned System\\nThis section provides an executive summary overview of the envisioned system. A more detailed description will be provided in Section 3.0\\n1.2.1  Overview\\nThis subsection provides a high-level overview of the sys -\\ntem and its operation. Pictorials, graphics, videos, mod -\\nels, or other means may be used to provide this basic \\nunderstanding of the concept.\\n1.2.2  System Scope\\nThis section gives an estimate of the size and complex -\\nity of the system. It defines the system’s external inter -\\nfaces and enabling systems. It describes what the project will encompass and what will lie outside of the project’s \\ndevelopment.\\n2.0 Documents\\n2.1 Applicable Documents\\nThis section lists all the documents, models, standards or other material that are applicable and some or all of which will form part of the requirements of the project.\\n2.2 Reference Documents\\nThis section provides supplemental information that might be useful in understanding the system or its scenarios.252\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix S: Concept of OperationsAnnotated Outline \\n3.0 Description of Envisioned \\nSystem\\nThis section provides a more detailed description of the \\nenvisioned system and its operation as contained in the \\nfollowing subsections.\\n3.1  Needs, Goals and Objectives of \\nEnvisioned System\\nThis section describes the needs, goals, and objectives as \\nexpectations for the system capabilities, behavior, and oper -\\nations. It may also point to a separate document or model \\nthat contains the current up-to-date agreed-to expectations.\\n3.2  Overview of System and Key \\nElements\\nThis section describes at a functional level the vari -\\nous elements that will make up the system, including \\nthe users and operators. These descriptions should be \\nimplementation free; that is, not specific to any imple -\\nmentation or design but rather a general description of \\nwhat the system and its elements will be expected to do. \\nGraphics, pictorials, videos, and models may be used to \\naid this description.\\n3.3 Interfaces\\nThis section describes the interfaces of the system with \\nany other systems that are external to the project. It may \\nalso include high-level interfaces between the major \\nenvisioned elements of the system. Interfaces may include \\nmechanical, electrical, human user/operator, fluid, \\nradio frequency, data, or other types of interactions.\\n3.4 Modes of Operations\\nThis section describes the various modes or configura -\\ntions that the system may need in order to accomplish \\nits intended purpose throughout its life cycle. This may \\ninclude modes needed in the development of the sys -\\ntem, such as for testing or training, as well as various \\nmodes that will be needed during it operational and \\ndisposal  phases.3.5 Proposed Capabilities\\nThis section describes the various capabilities that the \\nenvisioned system will provide. These capabilities cover \\nthe entire life cycle of the system’s operation, including \\nspecial capabilities needed for the verification/validation \\nof the system, its capabilities during its intended oper -\\nations, and any special capabilities needed during the \\ndecommissioning or disposal process.\\n4.0 Physical Environment\\nThis section should describe the environment that the \\nsystem will be expected to perform in throughout its \\nlife cycle, including integration, tests, and transpor -\\ntation. This may include expected and off-nominal \\ntemperatures, pressures, radiation, winds, and other \\natmospheric, space, or aquatic conditions. A description \\nof whether the system needs to operate, tolerate with \\ndegraded performance, or just survive in these condi -\\ntions should be noted.\\n5.0 Support Environment\\nThis section describes how the envisioned system will be \\nsupported after being fielded. This includes how opera -\\ntional planning will be performed and how command -\\ning or other uploads will be determined and provided, \\nas required. Discussions may include how the envi -\\nsioned system would be maintained, repaired, replaced, \\nit’s sparing philosophy, and how future upgrades may be \\nperformed. It may also include assumptions on the level \\nof continued support from the design teams.\\n6.0 Operational Scenarios, \\nUse Cases and/or Design \\nReference Missions\\nThis section takes key scenarios, use cases, or DRM and \\ndiscusses what the envisioned system provides or how \\nit functions throughout that single-thread timeline. 253\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix S: Concept of OperationsAnnotated Outline \\nThe number of scenarios, use cases, or DRMs discussed \\nshould cover both nominal and off-nominal conditions \\nand cover all expected functions and capabilities. A good \\npractice is to label each of these scenarios to facilitate \\nrequirements traceability; e.g., [DRM-0100], [DRM-\\n0200], etc.\\n6.1 Nominal Conditions\\nThese scenarios, use cases, or DRMs cover how the envi -\\nsioned system will operate under normal circumstances \\nwhere there are no problems or anomalies taking place.\\n6.2 Off-Nominal Conditions\\nThese scenarios cover cases where some condition has \\noccurred that will need the system to perform in a way \\nthat is different from normal. This would cover failures, \\nlow performance, unexpected environmental conditions, \\nor operator errors. These scenarios should reveal any \\nadditional capabilities or safeguards that are needed in \\nthe system.\\n7.0 Impact Considerations\\nThis section describes the potential impacts, both positive \\nand negative, on the environment and other areas.\\n7.1 Environmental Impacts\\nDescribes how the envisioned system could impact the \\nenvironment of the local area, state, country, world -\\nwide, space, and other planetary bodies as appropriate \\nfor the systems intended purpose. This includes the pos -\\nsibility of the generation of any orbital debris, potential \\ncontamination of other planetary bodies or atmosphere, \\nand generation of hazardous wastes that will need dis -\\nposal on earth and other factors. Impacts should cover \\nthe entire life cycle of the system from development \\nthrough disposal.7.2 Organizational Impacts\\nDescribes how the envisioned system could impact existing or \\nfuture organizational aspects. This would include the need \\nfor hiring specialists or operators, specialized or widespread \\ntraining or retraining, and use of multiple organizations.\\n7.3 Scientific/Technical Impacts\\nThis subsection describes the anticipated scientific or \\ntechnical impact of a successful mission or deploy -\\nment, what scientific questions will be answered, what \\nknowledge gaps will be filled, and what services will be \\nprovided. If the purpose of this system is to improve oper -\\nations or logistics instead of science, describe the antici -\\npated impact of the system in those terms.\\n8.0 Risks and Potential Issues\\nThis section describes any risks and potential issues asso -\\nciated with the development, operations or disposal of \\nthe envisioned system. Also includes concerns/risks with \\nthe project schedule, staffing support, or implementation \\napproach. Allocate subsections as needed for each risk \\nor issue consideration. Pay special attention to closeout \\nissues at the end of the project.\\nAppendix A: Acronyms\\nThis part lists each acronym used in the ConOps and \\nspells it out.\\nAppendix B: Glossary of Terms\\nThe part lists key terms used in the ConOps and provides \\na description of their meaning.254\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nAppendix T: Systems Engineering in Phase E\\nT.1 Overview\\nIn general, normal Phase E activities reflect a reduced \\nemphasis on system design processes but a continued focus on product realization and technical manage -\\nment. Product realization process execution in Phase E takes the form of continued mission plan generation (and update), response to changing flight conditions (and occurrence of in-flight anomalies), and update of mission operations techniques, procedures, and guidelines based on operational experience gained. \\nTechnical management processes ensure that appro -\\npriate rigor and risk management practices are applied \\nin the execution of the product realization processes.\\nSuccessful Phase E execution requires the prior estab -\\nlishment of mission operations capabilities in four \\n(4) distinct categories: tools, processes, products, and trained personnel. These capabilities may be developed as separate entities, but need to be fused together in Phase E to form an end-to-end opera -\\ntional capability.\\nAlthough systems engineering activities and pro -\\ncesses are constrained throughout the entire project \\nlife cycle, additional pressures exist in Phase E:\\n• Increased resource constraints:  Even when \\nadditional funding or staffing can be secured, building new capabilities or training new person -\\nnel may require more time or effort than is avail -\\nable. Project budget and staffing profiles generally decrease at or before entry into Phase E, and the remaining personnel are typically focused on mis-sion execution.• Unforgiving schedule:  Unlike pre-flight test \\nactivities, it may be difficult or even impossible to pause mission execution to deal with technical \\nissues of a spacecraft in operation. It is typically \\ndifficult or impossible to truly pause mission exe -\\ncution after launch.\\nThese factors must be addressed when consider -\\ning activities that introduce change and risk during \\nPhase E.\\nNOTE:\\n When significant hardware or software \\nchanges are required in Phase E, the logical decom -\\nposition process may more closely resemble that \\nexercised in earlier project phases. In such cases, it may be more appropriate to identify the modifi -\\ncation as a new project executing in parallel—and coordinated with—the operating project.\\nT.2 Transition from Development \\nto Operations\\nAn effective transition from development to opera -\\ntions phases requires prior planning and coordina -\\ntion among stakeholders. This planning should focus \\nnot only on the effective transition of hardware and \\nsoftware systems into service but also on the effec -\\ntive transfer of knowledge, skills, experience, and processes into roles that support the needs of flight operations.\\nDevelopment phase activities need to clearly and \\nconcisely document system knowledge in the form 255\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix T: Systems Engineering in Phase E\\nof operational techniques, characteristics, limits, \\nand constraints—these are key inputs used by flight \\noperations personnel in building operations tools \\nand techniques. Phase D Integration and Test (I&T) \\nactivities share many common needs with Phase E \\noperations activities. Without prior planning and \\nagreement, however, similar products used in these \\ntwo phases may be formatted so differently that one \\nset cannot be used for both purposes. The associated \\nproduct duplication is often unexpected and results \\nin increased cost and schedule risk. Instead, system \\nengineers should identify opportunities for product \\nreuse early in the development process and establish \\ncommon standards, formats, and content expecta -\\ntions to enable transition and reuse.\\nSimilarly, the transfer of skills and experience should \\nbe managed through careful planning and placement \\nof key personnel. In some cases, key design, integra -\\ntion, and test personnel may be transitioned into the \\nmission operations team roles. In other cases, dedi -\\ncated mission operations personnel may be assigned \\nto shadow or assist other teams during Phase A–D \\nactivities. In both cases, assignees bring knowledge, \\nskills, and experience into the flight operations envi -\\nronment. Management of this transition process can, \\nhowever, be complex as these personnel may be con -\\nsidered key to both ongoing I&T and preparation for \\nupcoming operations. Careful and early planning of \\npersonnel assignments and transitions is key to suc -\\ncess in transferring skills and experience.\\nT.3 System Engineering \\nProcesses in Phase E\\nT.3.1  System Design Processes\\nIn general, system design processes are complete well \\nbefore the start of Phase E. However, events during \\noperations may require that these processes be revis -\\nited in Phase E.T.3.1.1 Stakeholder Expectations Definition\\nStakeholder expectations should have been identi -\\nfied during development phase activities, including \\nthe definition of operations concepts and design ref -\\nerence missions. Central to this definition is a con -\\nsensus on mission success criteria and the priority of \\nall intended operations. The mission operations plan \\nshould state and address these stakeholder expec -\\ntations with regard to risk management practices, \\nplanning flexibility and frequency of opportunities \\nto update the plan, time to respond and time/scope \\nof status communication, and other key parameters \\nof mission execution. Additional detail in the form \\nof operational guidelines and constraints should be \\nincorporated in mission operations procedures and \\nflight rules.\\nThe Operations Readiness Review (ORR) should \\nconfirm that stakeholders accept the mission opera -\\ntions plan and operations implementation products.\\nHowever, it is possible for events in Phase E to require \\na reassessment of stakeholder expectations. Significant \\nin-flight anomalies or scientific discoveries during \\nflight operations may change the nature and goals of \\na mission. Mission systems engineers, mission oper -\\nations managers, and program management need to \\nremain engaged with stakeholders throughout Phase \\nE to identify potential changes in expectations and to \\nmanage the acceptance or rejection of such changes \\nduring operations.\\nT.3.1.2 Technical Requirements Definition\\nNew technical requirements and changes to existing \\nrequirements may be identified during operations as \\na result of:\\n• New understanding of system characteristics \\nthrough flight experience;\\n• The occurrence of in-flight anomalies; or256\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix T: Systems Engineering in Phase E\\n• Changing mission goals or parameters (such as \\nmission extension).\\nThese changes or additions are generally handled as \\nchange requests to an operations baseline already under \\nconfiguration management and possibly in use as part \\nof ongoing flight operations. Such changes are more \\ncommonly directed to the ground segment or opera -\\ntions products (operational constraints, procedures, \\netc.). Flight software changes may also be considered, \\nbut flight hardware changes for anything other than \\nhuman-tended spacecraft are rarely possible.\\nTechnical requirement change review can be more \\nchallenging in Phase E as fewer resources are avail -\\nable to perform comprehensive review. Early and \\nclose involvement of Safety and Mission Assurance \\n(SMA) representatives can be key in ensuring that \\nproposed changes are appropriate and within the \\nproject’s allowable risk tolerance.\\nT.3.1.3 Logical Decomposition\\nIn general, logical decomposition of mission oper -\\nations functions is performed during development \\nphases. Additional logical decomposition during \\noperations is more often applied to the operations \\nproducts: procedures, user interfaces, and operational \\nconstraints. The authors and users of these products \\nare often the most qualified people to judge the \\nappropriate decomposition of new or changed func -\\ntionality as a series of procedures or similar products.\\nT.3.1.4 Design Solution Definition\\nSimilar to logical decomposition, design solution \\ndefinition tasks may be better addressed by those who \\ndevelop and use the products. Minor modifications \\nmay be handled entirely within an operations team \\n(with internal reviews), while larger changes or addi -\\ntions may warrant the involvement of program-level \\nsystem engineers and Safety and Mission Assurance \\n(SMA) personnel.Scarcity of time and resources during Phase E can \\nmake implementation of these design solutions chal -\\nlenging. The design solution needs to take into account \\nthe availability of and constraints to resources.\\nT.3.1.5 Product Implementation\\nPersonnel who implement mission operations prod -\\nucts such as procedures and spacecraft command \\nscripts should be trained and certified to the appro -\\npriate level of skill as defined by the project. Processes \\ngoverning the update and creation of operations \\nproducts should be in place and exercised prior to \\nPhase E.\\nT.3.2  Product Realization Processes\\nProduct realization processes in Phase E are typically \\nexecuted by Configuration Management (CM) and \\ntest personnel. It is common for these people to be \\n“shared resources;” i.e., personnel who fulfil other \\nroles in addition to CM and test roles.\\nT.3.2.1 Product Integration\\nProduct integration in Phase E generally involves bring -\\ning together multiple operations products—some pre -\\nexisting and others new or modified—into a proposed \\nupdate to the baseline mission operations capability.\\nThe degree to which a set of products is integrated may \\nvary based on the size and complexity of the project. \\nSmall projects may define a baseline—and update to \\nthat baseline—that spans the entire set of all opera -\\ntions products. Larger or more complex projects may \\nchoose to create logical baseline subsets divided along \\npractical boundaries. In a geographically disperse set \\nof separate mission operations Centers, for example, \\neach Center may be initially integrated as a separate \\nproduct. Similarly, the different functions within a \\nsingle large control Center—planning, flight dynam -\\nics, command and control, etc.—may be established \\nas separately baselined products. Ultimately, however, 257\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix T: Systems Engineering in Phase E\\nsome method needs to be established to ensure that \\nthe product realization processes identify and assess \\nall potential impacts of system changes.\\nT.3.2.2 Product Verification\\nProduct verification in Phase E generally takes the \\nform of unit tests of tools, data sets, procedures, and \\nother items under simulated conditions. Such “thread \\ntests” may exercise single specific tasks or functions. \\nThe fidelity of simulation required for verification \\nvaries with the nature and criticality of the product. \\nKey characteristics to consider include:\\n• Runtime:  Verification of products during flight \\noperations may be significantly time constrained. \\nGreater simulation fidelity can result in slower \\nsimulation performance. This slower performance \\nmay be acceptable for some verification activities \\nbut may be too constraining for others.\\n• Level of detail:  Testing of simple plans and pro -\\ncedures may not require high-fidelity simulation \\nof a system’s dynamics. For example, simple state \\nchange processes may be tested on relatively \\nlow-fidelity simulations. However, operational \\nactivities that involve dynamic system attributes – \\nsuch as changes in pressure, temperature, or other \\nphysical properties may require testing with much \\nhigher-fidelity simulations.\\n• Level of integration:  Some operations may impact \\nonly a single subsystem, while others can affect \\nmultiple systems or even the entire spacecraft.\\n• Environmental effects:  Some operations products \\nand procedures may be highly sensitive to envi -\\nronmental conditions, while others may not. For \\nexample, event sequences for atmospheric entry \\nand deceleration may require accurate weather \\ndata. In contrast, simple system reconfiguration procedures may not be impacted by environmen -\\ntal conditions at all.\\nT.3.2.3 Product Validation\\nProduct validation is generally executed through the \\nuse of products in integrated operational scenarios \\nsuch as mission simulations, operational readiness \\ntests, and/or spacecraft end-to-end tests. In these \\nenvironments, a collection of products is used by a \\nteam of operators to simulate an operational activ -\\nity or set of activities such as launch, activation, ren -\\ndezvous, science operations, or Entry, Descent, and \\nLanding (EDL). The integration of multiple team \\nmembers and operations products provides the con -\\ntext necessary to determine if the product is appropri -\\nate and meets the true operations need.\\nT.3.2.4 Product Transition\\nTransition of new operational capabilities in Phase \\nE is generally overseen by the mission operations \\nmanager or a Configuration Control Board (CCB) \\nchaired by the mission operations manager or the \\nproject manager.\\nProper transition management includes the inspec -\\ntion of product test (verification and validation) \\nresults as well as the readiness of the currently oper -\\nating operations system to accept changes. Transition \\nduring Phase E can be particularly challenging as the \\npersonnel using these capabilities also need to change \\ntechniques, daily practices, or other behaviors as a \\nresult. Careful attention should be paid to planned \\noperations, such as spacecraft maneuvers or other \\nmission critical events and risks associated with per -\\nforming product transition at times near such events.\\nT.3.3   Technical Management \\nProcesses\\nTechnical management processes are generally a \\nshared responsibility of the project manager and 258\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix T: Systems Engineering in Phase E\\nthe mission operations manager. Clear agreement \\nbetween these two parties is essential in ensuring that \\nPhase E efforts are managed effectively.\\nT.3.3.1 Technical Planning\\nTechnical planning in Phase E generally focuses \\non the management of scarce product develop -\\nment resources during mission execution. Key \\ndecision-makers, including the mission operations \\nmanager and lower operations team leads, need to \\nreview the benefits of a change against the resource \\ncost to implement changes. Many resources are \\nshared in Phase E – for example, product developers \\nmay also serve other real-time operations roles– and \\nthe additional workload placed on these resources \\nshould be viewed as a risk to be mitigated during \\noperations.\\nT.3.3.2 Requirements Management\\nRequirements management during Phase E is simi -\\nlar in nature to pre-Phase E efforts. Although some \\nstreamlining may be implemented to reduce process \\noverhead in Phase E, the core need to review and val -\\nidate requirements remains. As most Phase E changes \\nare derived from a clearly demonstrated need, pro -\\ngram management may reduce or waive the need \\nfor complete requirements traceability analysis and \\ndocumentation.\\nT.3.3.3 Interface Management\\nIt is relatively uncommon for interfaces to change in \\nPhase E, but this can occur when a software tool is \\nmodified or a new need is uncovered. Interface defi -\\nnitions should be managed in a manner similar to \\nthat used in other project phases.\\nT.3.3.4 Technical Risk Management\\nManaging technical risks during operations can be \\nmore challenging during Phase E than during other \\nphases. New risks discovered during operations may be the result of system failures or changes in the sur -\\nrounding environment. Where additional time may \\nbe available to assess and mitigate risk in other proj -\\nect phases, the nature of flight operations may limit \\nthe time over which risk management can be exe -\\ncuted. For this reason, every project should develop a \\nformal process for handling anomalies and managing \\nrisk during operations. This process should be exer -\\ncised before flight, and decision-makers should be \\nwell versed in the process details.\\nT.3.3.5 Configuration Management\\nEffective and efficient Configuration Management \\n(CM) is essential during operations. Critical oper -\\nations materials, including procedures, plans, flight \\ndatasets, and technical reference material need to \\nbe secure, up to date, and easily accessed by those \\nwho make and enact mission critical decisions. CM \\nsystems—in their intended flight configuration—\\nshould be exercised as part of operational readiness \\ntests to ensure that the systems, processes, and par -\\nticipants are flight-ready.\\nAccess to such operations products is generally \\ntime-critical, and CM systems supporting that access \\nshould be managed accordingly. Scheduled mainte -\\nnance or other “downtime” periods should be coor -\\ndinated with flight operations plans to minimize \\nthe risk of data being inaccessible during critical \\nactivities.\\nT.3.3.6 Technical Data Management\\nTools, procedures, and other infrastructure for \\nTechnical Data Management must be baselined, \\nimplemented, and verified prior to flight operations. \\nChanges to these capabilities are rarely made during \\nPhase E due to the high risk of data loss or reduc -\\ntion in operations efficiency when changing during \\noperations.259\\nNASA SYSTEMS ENGINEERING HANDBOOKAppendix T: Systems Engineering in Phase E\\nMandatory Technical Data Management infrastruc -\\nture changes, when they occur, should be carefully \\nreviewed by those who interact with the data on a \\nregular basis. This includes not only operations per -\\nsonnel, but also engineering and science customers of \\nthat data.\\nT.3.3.7 Technical Assessment\\nFormal technical assessments during Phase E are typ -\\nically focused on the upcoming execution of a spe -\\ncific operational activity such as launch, orbit entry, \\nor decommissioning. Reviews executed while flight \\noperations are in progress should be scoped to answer \\ncritical questions while not overburdening the project \\nor operations team.\\nTechnical Performance Measures (TPMs) in Phase E \\nmay differ significantly from those in other project phases. Phase E TPMs may focus on the accomplish -\\nment of mission events, the performance of the sys -\\ntem in operation, and the ability of the operations \\nteam to support upcoming events.\\nT.3.3.8 Decision Analysis\\nThe Phase E Decision Analysis Process is similar to \\nthat in other project phases but may emphasize dif -\\nferent criteria. For example, the ability to change a \\nschedule may be limited by the absolute timing of \\nevents such as an orbit entry or landing on a plan -\\netary surface. Cost trades may be more constrained \\nby the inability to add trained personnel to support \\nan activity. Technical trades may be limited by the \\ninability to modify hardware in operation.260\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nReferences Cited\\nThis appendix contains references that were cited in \\nthe sections of the handbook.\\nPreface\\nNPR 7123.1, Systems Engineering Processes and \\nRequirements\\nNASA Chief Engineer and the NASA Integrated \\nAction Team (NIAT) report, Enhancing Mission \\nSuccess – A Framework for the Future , December \\n21, 2000. Authors: McBrayer, Robert O and \\nThomas, Dale, NASA Marshall Space Flight Center, Huntsville, AL United States.\\nNASA. Columbia Accident Investigation Board \\n(CAIB) Report , 6 volumes: Aug. 26, Oct. 2003. \\nhttp://www.nasa.gov/columbia/caib/html/report.\\nhtml\\nNASA. Diaz Report, A Renewed Commitment to \\nExcellence : An Assessment of the NASA Agency-\\nwide Applicability of the Columbia Accident \\nInvestigation Board Report , January 30, 2004. \\nMr. Al Diaz, Director, Goddard Space Flight \\nCenter, and team.\\nInternational Organization for Standardization \\n(ISO) 9000:2015, Quality management sys -\\ntems – Fundamentals and vocabulary . Geneva: \\nInternational Organization for Standardization, 2015.Section 1.1  P urpose\\nNPR 7123.1. Systems Engineering Processes and \\nRequirements\\nSection 1.2  S cope and Depth\\nNASA Office of Chief Information Officer (OCIO), \\nInformation Technology Systems Engineering Handbook Version 2.0\\nNASA-HDBK-2203, NASA Software Engineering \\nHandbook  (February 28, 2013)\\nSection 2.0  F undamentals of Systems \\nEngineeringNPR 7120.5, NASA Space Flight Program and \\nProject Management Requirements\\nNPR 7120.7, NASA Information Technology and \\nInstitutional Infrastructure Program and Project Management Requirements\\nNPR 7120.8, NASA Research and Technology \\nProgram and Project Management Requirements\\nNPR 7123.1, NASA Systems Engineering Processes \\nand Requirements\\nNASA Engineering Network (NEN) Systems \\nEngineering Community of Practice (SECoP), located at \\nhttps://nen.nasa.gov/web/se\\nGriffin, Michael D., NASA Administrator. \\n“System Engineering and the Two Cultures 261\\nNASA SYSTEMS ENGINEERING HANDBOOKReferences Cited\\nof Engineering.” Boeing Lecture, Purdue \\nUniversity, March 28, 2007.\\nRechtin, Eberhardt. Systems Architecting of \\nOrganizations: Why Eagles Can’t Swim . Boca \\nRaton: CRC Press, 2000.\\nSection 2.1 The Common Technical \\nProcesses and the SE Engine\\nNPR 7123.1, NASA Systems Engineering Processes \\nand Requirements\\nSociety of Automotive Engineers (SAE) and the \\nEuropean Association of Aerospace Industries \\n(EAAI). AS9100C Quality Management Systems \\n(QMS) – Requirements for Aviation, Space, and \\nDefense Organizations  Revision C: January 15, \\n2009.\\nSection 2.3 Example of Using the SE \\nEngine\\nNPD 1001.0, 2006 NASA Strategic Plan\\nNPR 7120.5, NASA Space Flight Program and \\nProject Management Requirements\\nSection 2.5  Cost Effectiveness \\nConsiderations\\nDepartment of Defense (DOD) Defense Acquisition \\nUniversity (DAU). Systems Engineering \\nFundamentals Guide . Fort Belvoir, VA, 2001.\\nINCOSE-TP-2003-002-04, Systems Engineering \\nHandbook: A Guide for System Life Cycle Processes \\nand Activities , Version 4, edited by Walden, \\nDavid D., et al., 2015\\nSection 2.6 Human Systems Integration \\n(HSI) in the SE Process\\nNPR 7120.5, NASA Space Flight Program and \\nProject Management RequirementsNPR 7123.1, NASA Systems Engineering Processes \\nand Requirements\\nSection 3.0 NASA Program/Project Life \\nCycle\\nNPR 7120.5, NASA Space Flight Program and \\nProject Management Requirements\\nNPR 7120.8, NASA Research and Technology \\nProgram and Project Management \\nRequirements\\nNASA Office of the Chief Information Officer \\n(OCIO), Information Technology Systems \\nEngineering Handbook  Version 2.0\\nNASA/SP-2014-3705, NASA Space Flight Program \\nand Project Management Handbook\\nSection 3.1  Program Formulation\\nNPR 7120.5, NASA Space Flight Program and \\nProject Management Requirements\\nNPR 7120.7, NASA Information Technology and \\nInstitutional Infrastructure Program and Project \\nManagement Requirements\\nNPR 7120.8, NASA Research and Technology \\nProgram and Project Management \\nRequirements\\nNPR 7123.1, NASA Systems Engineering Processes \\nand Requirements\\nSection 3.2  Program Implementation\\nNPR 7120.5, NASA Space Flight Program and \\nProject Management Requirements\\nNPR 7123.1, NASA Systems Engineering Processes \\nand Requirements262\\nNASA SYSTEMS ENGINEERING HANDBOOKReferences Cited\\nSection 3.3 Project Pre-Phase A: Concept \\nStudies\\nNPR 7120.5, NASA Space Flight Program and \\nProject Management Requirements\\nNPR 7123.1, NASA Systems Engineering Processes \\nand Requirements\\nSection 3.4 Project Phase A: Concept and \\nTechnology Development\\nNPD 1001.0, 2014 NASA Strategic Plan\\nNPR 2810.1, Security of Information Technology\\nNPR 7120.5, NASA Space Flight Program and \\nProject Management Requirements\\nNPR 7123.1, NASA Systems Engineering Processes \\nand Requirements\\nNPR 7150.2, NASA Software Engineering \\nRequirements\\nNASA-STD-8719.14, Handbook for Limiting Orbital \\nDebris . Rev A with Change 1. December 8, \\n2011.\\nNational Institute of Standards and Technology \\n(NIST), Federal Information Processing \\nStandard Publication (FIPS PUB) 199, \\nStandards for Security Categorization of Federal \\nInformation and Information Systems , February \\n2004.\\nSection 3.5 Project Phase B: Preliminary \\nDesign and Technology Completion\\nNPR 7120.5, NASA Space Flight Program and \\nProject Management Requirements\\nNPR 7123.1, NASA Systems Engineering Processes \\nand RequirementsSection 3.6 Project Phase C: Final Design \\nand Fabrication\\nNPR 7120.5, NASA Space Flight Program and \\nProject Management Requirements\\nNPR 7123.1, NASA Systems Engineering Processes \\nand Requirements\\nSection 3.7 Project Phase D: System \\nAssembly, Integration and Test, Launch\\nNPR 7120.5, NASA Space Flight Program and \\nProject Management Requirements\\nNPR 7123.1, NASA Systems Engineering Processes \\nand Requirements\\nNASA Office of the Chief Information Officer \\n(OCIO),  Information Technology Systems \\nEngineering Handbook Version 2.0\\nSection 3.8 Project Phase E: Operations \\nand Sustainment\\nNPR 7120.5, NASA Space Flight Program and \\nProject Management Requirements\\nNPR 7123.1, NASA Systems Engineering Processes \\nand Requirements\\nSection 3.9 Project Phase F: Closeout\\nNPR 7120.5, NASA Space Flight Program and \\nProject Management Requirements\\nNPR 7123.1, NASA Systems Engineering Processes \\nand Requirements\\nNPD 8010.3, Notification of Intent to \\nDecommission or Terminate Operating Space \\nSystems and Terminate Missions\\nNPR 8715.6, NASA Procedural Requirements for \\nLimiting Orbital Debris263\\nNASA SYSTEMS ENGINEERING HANDBOOKReferences Cited\\nSection 3.10 Funding: The Budget Cycle\\nNASA’s Financial Management Requirements (FMR)  \\nVolume 4\\nSection 3.11  Tailoring and Customization \\nof NPR 7123.1 Requirements\\nNPD 1001.0, 2014 NASA Strategic Plan\\nNPR 7120.5, NASA Space Flight Program and \\nProject Management Requirements\\nNPR 7120.7, NASA Information Technology and \\nInstitutional Infrastructure Program and Project \\nManagement Requirements\\nNPR 7120.8, NASA Research and Technology \\nProgram and Project Management \\nRequirements\\nNPR 7123.1, NASA Systems Engineering Processes \\nand Requirements\\nNPR 7150.2, NASA Software Engineering \\nRequirements\\nNPR 8705.4, Risk Classification for NASA Payloads\\nNASA-HDBK-2203, NASA Software Engineering \\nHandbook  (February 28, 2013)\\nNASA Engineering Network (NEN) Systems \\nEngineering Community of Practice (SECoP), \\nlocated at https://nen.nasa.gov/web/se\\nSection 4.1  Stakeholder Expectations \\nDefinition\\nNPR 7120.5, NASA Space Flight Program and \\nProject Management Requirements\\nNASA Science Mission Directorate strategic plansPresidential Policy Directive PPD-4 (2010), \\nNational Space Policy\\nPresidential Policy Directive PPD-21 (2013), Critical \\nInfrastructure Security and Resilience\\nBall, Robert E. (Naval Postgraduate School), The \\nFundamentals of Aircraft Combat Survivability \\nAnalysis and Design , 2nd Edition, AIAA \\nEducation Series, 2003\\nLarson (Wiley J.), Kirkpatrick, Sellers, Thomas, \\nand Verma. Applied Space Systems Engineering: \\nA Practical Approach to Achieving Technical \\nBaselines.  2nd Edition, Boston, MA: McGraw-\\nHill Learning Solutions, CEI Publications, \\n2009.\\nSection 4.2  Technical Requirements \\nDefinition\\nNPR 7120.10, Technical Standards for NASA \\nPrograms and Projects\\nNPR 8705.2, Human-Rating Requirements for \\nSpace Systems\\nNPR 8715.3, NASA General Safety Program \\nRequirements\\nNASA-STD-3001, NASA Space Flight Human \\nSystem Standard  – 2 volumes\\nNASA-STD-8719.13, Software Safety Standard , Rev \\nC. Washington, DC, May 7, 2013.\\nNA SA /SP-2010 -3407, Human Integration Design \\nHandbook (HIDH)264\\nNASA SYSTEMS ENGINEERING HANDBOOKReferences Cited\\nSection 4.3  Logical Decomposition\\nDepartment of Defense (DOD) Architecture \\nFramework (DODAF)  Version 2.02 Change 1, \\nJanuar y 2015\\nInstitute of Electrical and Electronics Engineers \\n(IEEE) STD 610.12-1990, IEEE Standard \\nGlossary of Software Engineering Terminology . \\nReaffirmed 2002. Superseded by ISO/IEC/\\nIEEE 24765:2010, Systems and Software \\nEngineering – Vocabulary\\nSection 4.4 Design Solution Definition\\nNPD 8730.5, NASA Quality Assurance Program \\nPolicy\\nNPR 8735.2, Management of Government Quality \\nAssurance Functions for NASA Contracts\\nNASA-HDBK-1002, Fault Management (FM) \\nHandbook , Draft 2, April 2012.\\nNASA-STD-3001, NASA Space Flight Human \\nSystem Standard  – 2 volumes\\nNASA-STD-8729.1, Planning, Developing, and \\nMaintaining an Effective Reliability and \\nMaintainability (R&M) Program . Washington, \\nDC, December 1, 1998.\\nCode of Federal Regulations (CFR), Title 48 – \\nFederal Acquisition Regulation (FAR) System, \\nPart 46.4 Government Contract Quality \\nAssurance (48 CFR 46.4)\\nInternational Organization for Standardization, ISO \\n9001:2015 Quality Management Systems (QMS)\\nSociety of Automotive Engineers and the \\nEuropean Association of Aerospace Industries. \\nAS9100C Quality Management Systems (QMS)—Requirements for Aviation, Space, and \\nDefense Organizations  Revision C: 2009-01-15\\nBlanchard, Benjamin S., System Engineering \\nManagement . 4th Edition, Hoboken, NJ: John \\nWiley & Sons, Inc., 2008\\nSection 5.1  Product Implementation\\nNPR 7150.2, NASA Software Engineering \\nRequirements\\nNASA Engineering Network (NEN) Systems \\nEngineering Community of Practice (SECoP), \\nlocated at https://nen.nasa.gov/web/se\\nNASA Engineering Network (NEN) V&V \\nCommunity of Practice, located at https://nen.\\nnasa.gov/web/se\\nAmerican Institute of Aeronautics and Astronautics \\n(AIAA) G-118-2006e. AIAA Guide for \\nManaging the Use of Commercial Off the Shelf \\n(COTS) Software Components for Mission-\\nCritical Systems . Reston, VA, 2006\\nSection 5.2  Product Integration\\nNASA Lyndon B. Johnson Space Center (JSC-\\n60576), National Space Transportation System \\n(NSTS), Space Shuttle Program, Transition \\nManagement Plan, May 9, 2007\\nSection 5.3  Product Verification\\nNPR 7120.5, NASA Space Flight Program and \\nProject Management Requirements\\nNPR 7120.8, NASA Research and Technology \\nProgram and Project Management \\nRequirements\\nNPR 7123.1, NASA Systems Engineering Processes \\nand Requirements265\\nNASA SYSTEMS ENGINEERING HANDBOOKReferences Cited\\nNPR 8705.4, Risk Classification for NASA Payloads\\nNASA-STD-7009, Standard for Models and \\nSimulations . Washington, DC, October 18, \\n2013\\nNASA GSFC-STD-7000, Goddard Technical \\nStandard: General Environmental Verification \\nStandard (GEVS) for GSFC Flight Programs and \\nProjects . Goddard Space Flight Center. April \\n2005\\nDepartment of Defense (DOD). MIL-STD-1540D, \\nProduct Verification Requirements for Launch, \\nUpper Stage, and Space Vehicles . January 15, \\n1999\\nSection 5.4  Product Validation\\nNPD 7120.4, NASA Engineering and Program/\\nProject Management Policy\\nNPR 7150.2, NASA Software Engineering \\nRequirements\\nSection 5.5  Product Transition\\n(The) National Environmental Policy Act of 1969 \\n(NEPA). See 42 U.S.C. 4321-4347. https://ceq.\\ndoe.gov/welcome.html\\nSection 6.1 Technical Planning\\nNPR 7120.5, NASA Space Flight Program and \\nProject Management Requirements\\nNPD 7120.6, Knowledge Policy on Programs and \\nProjects\\nNPR 7123.1, NASA Systems Engineering Processes \\nand Requirements\\nNASA-SP-2010-3403, NASA Schedule Management \\nHandbookNASA-SP-2010-3404, NASA Work Breakdown \\nStructure Handbook\\nNASA Cost Estimating Handbook (CEH) , Version 4, \\nFebruar y 2015.\\nDOD. MIL-STD-881C, Work Breakdown Structure \\n(WBS) for Defense Materiel Items . Washington, \\nDC, October 3, 2011.\\nInstitute of Electrical and Electronics Engineers \\n(IEEE) STD 1220-2005. IEEE Standard for \\nApplication and Management of the Systems \\nEngineering Process , Washington, DC, 2005.\\nOffice of Management and Budget (OMB) Circular \\nA-94, “Guidelines and Discount Rates for \\nBenefit-Cost Analysis of Federal Programs” \\n(10/29/1992)\\nJoint (cost and schedule) Confidence Level (JCL). \\nFrequently asked questions (FAQs) can be found \\nat: http://www.nasa.gov/pdf/394931main_JCL_\\nFAQ_10_12_09.pdf\\nThe U. S. Chemical Safety Board (CSB) case study \\nreports on mishaps found at: http://www.csb.gov/\\nSection 6.3 Interface Management\\nNPR 7120.5, NASA Space Flight Program and \\nProject Management Requirements\\nSection 6.4 Technical Risk Management\\nNPR 8000.4, Agency Risk Management Procedural \\nRequirements\\nNASA/SP-2010-576, NASA Risk-Informed Decision \\nMaking Handbook266\\nNASA SYSTEMS ENGINEERING HANDBOOKReferences Cited\\nNASA/SP-2011-3421, Probabilistic Risk Assessment \\nProcedures Guide for NASA Managers and \\nPractitioners\\nNASA/SP-2011-3422, NASA Risk Management \\nHandbook\\nCode of Federal Regulations (CFR) Title 22 – \\nForeign Relations, Parts 120-130 Department of \\nState: International Traffic in Arms Regulations \\n(ITAR) (22 CFR 120-130). Implements 22 \\nU.S.C. 2778 of the Arms Export Control Act \\n(AECA) of 1976 and Executive Order 13637, \\n“Administration of Reformed Export Controls,” \\nMarch 8, 2013\\nSection 6.5  Configuration Management\\nNPR 7120.5, NASA Space Flight Program and \\nProject Management Requirements\\nNASA. Columbia Accident Investigation Board \\n(CAIB) Report , 6 volumes: Aug. 26, Oct. 2003. \\nhttp://www.nasa.gov/columbia/caib/html/report.html\\nNASA. NOAA N-Prime Mishap Investigation Final \\nReport , Sept. 13, 2004 http://www.nasa.gov/pdf/\\n65776main_noaa_np_mishap.pdf\\nSAE International (SAE)/Electronic Industries \\nAlliance (EIA) 649B-2011, Configuration \\nManagement Standard (Aerospace Sector)  April 1, \\n2011\\nAmerican National Standards Institute (ANSI)/\\nElectronic Industries Alliance (EIA). ANSI/\\nEIA-649, National Consensus Standard for \\nConfiguration Management , 1998–1999\\nSection 6.6 Technical Data Management\\nNPR 1441.1, NASA Records Retention SchedulesNPR 1600.1, NASA Security Program Procedural \\nRequirements\\nNID 1600.55, Sensitive But Unclassified (SBU) \\nControlled Information\\nNPR 7120.5, NASA Space Flight Program and \\nProject Management Requirements\\nNPR 7123.1, NASA Systems Engineering Processes \\nand Requirements\\nNASA Form (NF) 1686, NASA Scientific and \\nTechnical Document Availability Authorization \\n(DAA) for Administratively Controlled \\nInformation.\\nCode of Federal Regulations (CFR) Title 22 – \\nForeign Relations, Parts 120-130 Department of \\nState: International Traffic in Arms Regulations \\n(ITAR) (22 CFR 120-130). Implements 22 \\nU.S.C. 2778 of the Arms Export Control Act \\n(AECA) of 1976 and Executive Order 13637, \\n“Administration of Reformed Export Controls,” \\nMarch 8, 2013\\nThe Invention Secrecy Act of 1951, 35 U.S.C. \\n§181–§188. Secrecy of Certain Inventions and \\nFiling Applications in Foreign Country; §181 – \\nSecrecy of Certain Inventions and Withholding \\nof Patent.\\nCode of Federal Regulations (CFR) Title 37 – \\nPatents, Trademarks, and Copyrights; Part 5 \\nSecrecy of Certain Inventions and Licenses \\nto Export and File Applications in Foreign \\nCountries; Part 5.2 Secrecy Order. (37 CFR 5.2)\\nSection 6.7  Technical Assessment\\nNPR 1080.1, Requirements for the Conduct of \\nNASA Research and Technology (R&T)267\\nNASA SYSTEMS ENGINEERING HANDBOOKReferences Cited\\nNPR 7120.5, NASA Space Flight Program and \\nProject Management Requirements\\nNPR 7120.7, NASA Information Technology and \\nInstitutional Infrastructure Program and Project \\nManagement Requirements\\nNPR 7120.8, NASA Research and Technology \\nProgram and Project Management \\nRequirements\\nNPR 7123.1, NASA Systems Engineering Processes \\nand Requirements\\nNPR 8705.4, Risk Classification for NASA Payloads\\nNPR 8705.6, Safety and Mission Assurance (SMA) \\nAudits, Reviews, and Assessments\\nNPR 8715.3, NASA General Safety Program \\nRequirements\\nNASA-HDBK-2203, NASA Software Engineering \\nHandbook . February 28, 2013\\nNASA/SP-2012-599, NASA’s Earned Value \\nManagement (EVM) Implementation Handbook\\nNASA Federal Acquisition Regulation (FAR) \\nSupplement (NFS) 1834.201, Earned Value \\nManagement System Policy.\\nNASA EVM website http://evm.nasa.gov/index.html\\nNASA Engineering Network (NEN) EVM \\nCommunity of Practice located at https://nen.\\nnasa.gov/web/pm/evm\\nNASA Engineering Network (NEN) Systems \\nEngineering Community of Practice (SECoP) \\nunder Tools and Methods at https://nen.nasa.gov/web/se/tools/  and then NASA Tools & \\nMethods\\nAmerican National Standards Institute/Electronic \\nIndustries Alliance (ANSI-EIA), Standard \\n748-C Earned Value Management Systems . \\nMarch, 2013.\\nInternational Council on Systems Engineering \\n(INCOSE). INCOSE-TP-2003-020-01, \\nTechnical Measurement , Version 1.0, 27 \\nDecember 2005. Prepared by Garry J. Roedler \\n(Lockheed Martin) and Cheryl Jones (U.S. \\nArmy).\\nSection 6.8  Decision Analysis\\nNPR 7120.5, NASA Space Flight Program and \\nProject Management Requirements\\nNPR 7123.1, NASA Systems Engineering Processes \\nand Requirements\\nBrughelli, Kevin (Lockheed Martin), Deborah \\nCarstens (Florida Institute of Technology), \\nand Tim Barth (Kennedy Space Center), \\n“Simulation Model Analysis Techniques,” \\nLockheed Martin presentation to KSC, \\nNovember 2003\\nSaaty, Thomas L. The Analytic Hierarchy Process . \\nNew York: McGraw-Hill, 1980\\nAppendix B: Glossary\\nNPR 2210.1, Release of NASA Software\\nNPD 7120.4, NASA Engineering and Program/\\nProject Management Policy\\nNPR 7120.5, NASA Space Flight Program and \\nProject Management Requirements268\\nNASA SYSTEMS ENGINEERING HANDBOOKReferences Cited\\nNPR 7123.1, NASA Systems Engineering Processes \\nand Requirements\\nNPR 7150.2, NASA Software Engineering \\nRequirements\\nNPR 8000.4, Agency Risk Management Procedural \\nRequirements\\nNPR 8705.2, Human-Rating Requirements for \\nSpace Systems\\nNPR 8715.3, NASA General Safety Program \\nRequirements\\nInternational Organization for Standardization \\n(ISO). ISO/IEC/IEEE 42010:2011. Systems and \\nSoftware Engineering – Architecture Description . \\nGeneva: International Organization for \\nStandardization, 2011. ( http://www.iso-architec -\\nture.org/ieee-1471/index.html )\\nAvizienis, A., J.C. Laprie, B. Randell, C. Landwehr, \\n“Basic concepts and taxonomy of dependable \\nand secure computing,” IEEE Transactions on \\nDependable and Secure Computing  1 (1), 11–33, \\n2004\\nAppendix F: Functional, Timing, and State \\nAnalysis\\nNASA Reference Publication 1370, Training \\nManual for Elements of Interface Definition and \\nControl . 1997\\nDefense Acquisition University. Systems Engineering \\nFundamentals Guide . Fort Belvoir, VA, 2001\\nBuede, Dennis. The Engineering Design of Systems: \\nModels and Methods . New York: Wiley & Sons, \\n2000Long, James E.  Relationships Between Common \\nGraphical Representations in Systems Engineering.  \\nVienna, VA: Vitech Corporation, 2002\\nSage, Andrew, and William Rouse. The Handbook \\nof Systems Engineering and Management . New \\nYork: Wiley & Sons, 1999\\nAppendix G:  Technology Assessment/\\nInsertion\\nNPR 7120.5, NASA Space Flight Program and \\nProject Management Requirements\\nNPR 7123.1, NASA Systems Engineering Processes \\nand Requirements\\nAppendix H: Integration Plan Outline\\nFederal Highway Administration and CalTrans, \\nSystems Engineering Guidebook for ITS , Version \\n2.0. Washington, DC: U.S. Department of \\nTransportation, 2007\\nAppendix J: SEMP Content Outline\\nNPR 7120.5, NASA Space Flight Program and \\nProject Management Requirements\\nNPR 7123.1, Systems Engineering Processes and \\nRequirements\\nAppendix K:  Technical Plans\\nNPR 7120.5, NASA Space Flight Program and \\nProject Management Requirements\\nAppendix M: CM Plan Outline\\nSAE International (SAE)/Electronic Industries \\nAlliance (EIA) 649B-2011, Configuration \\nManagement Standard (Aerospace Sector)  April 1, \\n2011269\\nNASA SYSTEMS ENGINEERING HANDBOOKReferences Cited\\nAppendix N: Guidance on Technical Peer \\nReviews/Inspections\\nNPR 7123.1, Systems Engineering Processes and \\nRequirements\\nNPR 7150.2, NASA Software Engineering \\nRequirements\\nNASA Langley Research Center (LARC), \\nInstructional Handbook for Formal Inspections . \\nhttp://sw-eng.larc.nasa.gov/files/2013/05/\\nInstructional-Handbook-for-Formal-Inspections.\\npdf\\nAppendix P: SOW Review Checklist\\nNASA Langley Research Center (LaRC) Procedural \\nRequirements (LPR) 5000.2 Procurement \\nInitiator’s GuideNASA Langley Research Center (LaRC) Guidance \\non System and Software Metrics for Performance-\\nBased Contracting  sites-e.larc.nasa.gov/sweng/\\nfiles/2013/05/Guidance_on_Metrics_for_PBC_\\nR1V01.doc\\nAppendix R: HSI Plan Content Outline\\nNPR 7123.1, NASA Systems Engineering Processes \\nand Requirements\\nNPR 8705.2, Human-Rating Requirements for \\nSpace Systems\\nNASA-STD-3001, Space Flight Human-System \\nStandard , Volume 2: Human Factors, \\nHabitability, and Environmental Health, \\nSection 3.5 [V2 3005], “Human-Centered \\nDesign Process.” February 10, 2015270\\nNASA SYSTEMS ENGINEERING HANDBOOK\\nBibliography\\nThe bibliography contains sources cited in sections of \\nthe document and additional sources for developing the material in the document.\\nAIAA American Institute of Aeronautics and \\nAstronautics\\nANSI American National Standards Institute\\nASME American Society of Mechanical Engineers\\nASQ American Society for Quality\\nCCSDS Consultative Committee for Space Data Systems\\nCFR (U.S.) Code of Federal Regulations\\nCOSPAR The Committee on Space Research\\nDOD (U.S.) Department of Defense\\nEIA Electronic Industries Alliance\\nGEIA Government Electronics Information Technology Association\\nIEEE Institute of Electrical and Electronics Engineers\\nINCOSE International Council on Systems Engineering\\nISO International Organization for Standardization\\nNIST National Institute of Standards and Technology\\nSAE Society of Automotive Engineers\\nTOR Technical Operating Report\\nU.S.C. United States CodeA\\nAdams, R. J., et al. Software Development Standard \\nfor Space Systems, Aerospace Corporation Report  \\nNo. TOR-2004(3909)3537, Revision B. March \\n11, 2005. Prepared for the U.S. Air Force.\\nAIAA G-118-2006e, AIAA Guide for Managing the \\nUse of Commercial Off the Shelf (COTS) Software Components for Mission-Critical Systems , Reston, \\nVA, 2006\\nAIAA S-120-2006, Mass Properties Control for Space \\nSystems . Reston, VA, 2006\\nAIAA S-122-2007, Electrical Power Systems for \\nUnmanned Spacecraft , Reston, VA, 2007\\nANSI/AIAA G-043-1992, Guide for the Preparation \\nof Operational Concept Documents , Washington, \\nDC, 1992\\nANSI/EIA-632, Processes for Engineering a System , \\nArlington, VA, 1999\\nANSI/EIA-649, National Consensus Standard for \\nConfiguration Management , 1998-1999\\nANSI/GEIA-649, National Consensus Standard for \\nConfiguration Management , National Defense \\nIndustrial Association (NDIA), Arlington, VA 1998\\nANSI/EIA-748-C Standard: Earned Value \\nManagement Systems , March, 2013271\\nNASA SYSTEMS ENGINEERING HANDBOOKBibliography\\nANSI/GEIA GEIA-859, Data Management , \\nNational Defense Industrial Association \\n(NDIA), Arlington, VA 2004\\nANSI/IEEE STD 1042. IEEE Guide to Software \\nConfiguration Management . Washington, DC, \\n1987\\nArchitecture Analysis & Design Language (AADL): \\nhttps://wiki.sei.cmu.edu/aadl/index.php/Main_Page\\n(The) Arms Export Control Act (AECA) of 1976, \\nsee 22 U.S.C. 2778\\nASME Y14.24, Types and Applications of Engineering \\nDrawings , New York, 1999\\nASME Y14.100, Engineering Drawing Practices , New \\nYork, 2004\\nASQ, Statistics Division, Statistical Engineering, \\nhttp://asq.org/statistics/quality-information/\\nstatistical-engineering\\nAvizienis, A., J.C. Laprie, B. Randell, C. Landwehr, \\n“Basic concepts and taxonomy of dependable \\nand secure computing,” IEEE Transactions on \\nDependable and Secure Computing  1 (1), 11–33, \\n2004\\nB\\nBall, Robert E. The Fundamentals of Aircraft Combat \\nSurvivability Analysis and Design . 2nd Edition, \\nAIAA Education Series, 2003\\nBayer, T.J., M. Bennett, C. L. Delp, D. Dvorak, \\nJ. S. Jenkins, and S. Mandutianu. “Update: \\nConcept of Operations for Integrated Model-\\nCentric Engineering at JPL,” paper #1122, IEEE \\nAerospace Conference 2011Blanchard, Benjamin S., System Engineering \\nManagement . 4th Edition, Hoboken, NJ: John \\nWiley & Sons, Inc., 2008\\nBlanchard, Benjamin S., and Wolter J. Fabrycky. \\nSystems Engineering and Analysis , 5th Edition \\nPrentice Hall International Series in Industrial \\n& Systems Engineering; February 6, 2010\\nBrown, Barclay. “Model-based systems engineer -\\ning: Revolution or Evolution,” IBM Software, \\nThought Leadership White Paper, IBM \\nRational , December 2011\\nBrughelli, Kevin (Lockheed Martin), Deborah \\nCarstens (Florida Institute of Technology), \\nand Tim Barth (Kennedy Space Center), \\n“Simulation Model Analysis Techniques,” \\nLockheed Martin presentation to KSC, \\nNovember 2003\\nBuede, Dennis. The Engineering Design of Systems: \\nModels and Methods . New York: Wiley & Sons, \\n2000.\\nBusiness Process Modeling Notation (BPMN)  http://\\nwww.bpmn.org/\\nC\\nCCSDS 311.0-M-1, Reference Architecture for \\nSpace Data Systems, Recommended Practice \\n(Magenta), Sept 2008. http://public.ccsds.org/\\npublications/MagentaBooks.aspx\\nCCSDS 901-0-G-1, Space Communications Cross \\nSupport Architecture Description Document , \\nInformational Report (Green) Sept 2013. http://\\npublic.ccsds.org/publications/GreenBooks.aspx\\nChapanis, A. “The Error-Provocative Situation: \\nA Central Measurement Problem in Human 272\\nNASA SYSTEMS ENGINEERING HANDBOOKBibliography\\nFactors Engineering.” In The Measurement of \\nSafety Performance . Edited by W. E. Tarrants. \\nNew York: Garland STPM Press, 1980\\nChattopadhyay, Debarati, Adam M. Ross, and \\nDonna H. Rhodes, “A Method for Tradespace \\nExploration of Systems of Systems,” presen -\\ntation in Track 34-SSEE-3: Space Economic \\nCost Modeling, AIAA Space 2009 , September \\n15, 2009. © 2009 Massachusetts Institute \\nof Technology (MIT), SEARI: Systems \\nEngineering Advancement Research Initiative, \\nseari.mit.edu\\nChung, Seung H., Todd J. Bayer, Bjorn Cole, Brian \\nCooke, Frank Dekens, Christopher Delp, \\nDoris Lam. “Model-Based Systems Engineering \\nApproach to Managing Mass Margin,” in \\nProceedings of the 5th International Workshop \\non Systems & Concurrent Engineering for Space \\nApplications (SECESA),  Lisbon, Portugal, \\nOctober, 2012\\nClark, J.O. “System of Systems Engineering \\nand Family of Systems Engineering From \\na Standards, V-Model, and Dual-V Model \\nPerspective,” 3rd Annual IEEE International \\nSystems Conference , Vancouver, Canada, March \\n23–26, 2009\\nClemen, R., and T. Reilly. Making Hard Decisions \\nwith DecisionTools Suite . Pacific Grove, CA: \\nDuxbury Resource Center, 2002\\nCFR, Title 14 – Aeronautics and Space, Part 1214 \\nNASA Space Flight (14 CFR 1214)\\nCFR, Title 14 – Aeronautics and Space, Part 1216.3 \\nNASA Environmental Quality: Procedures for \\nImplementing the National Environmental \\nPolicy Act (NEPA) (14 CFR 1216.3)CFR Title 22 – Foreign Relations, Parts 120-130 \\nDepartment of State: International Traffic in \\nArms Regulations (ITAR) (22 CFR 120-130). \\nImplements 22 U.S.C. 2778 of the Arms Export \\nControl Act (AECA) of 1976 and Executive \\nOrder 13637, “Administration of Reformed \\nExport Controls,” March 8, 2013\\nCFR Title 37 – Patents, Trademarks, and \\nCopyrights; Part 5 Secrecy of Certain \\nInventions and Licenses to Export and File \\nApplications in Foreign Countries; Part 5.2 \\nSecrecy Order. (37 CFR 5.2)\\nCFR Title 40 – Protection of Environment, Part \\n1508.27 Council on Environmental Quality: \\nTerminology “significantly.” (40 CFR 1508.27)\\nCFR Title 48 – Federal Acquisition Regulation \\n(FAR) System, Part 1214 NASA Acquisition \\nPlanning: Acquisition of Commercial Items: \\nSpace Flight. (48 CFR 1214)\\nCFR Title 48 – Federal Acquisition Regulation \\n(FAR) System, Part 46.103 Government \\nContract Quality Assurance: Contracting office \\nresponsibilities. (48 CFR 46.103)\\nCFR Title 48 – Federal Acquisition Regulation \\n(FAR) System, Part 46.4 Government Contract \\nQuality Assurance (48 CFR 46.4)\\nCFR Title 48 – Federal Acquisition Regulation \\n(FAR) System, Part 46.407 Government \\nContract Quality Assurance: Nonconforming \\nSupplies or Services (48 CFR 46.407)\\nCOSPAR, Planetary Protection Policy . March \\n24, 2005. http://w.astro.berkeley.edu/~kalas/\\nethics/documents/environment/COSPAR%20\\nPlanetary%20Protection%20Policy.pdf273\\nNASA SYSTEMS ENGINEERING HANDBOOKBibliography\\nD\\nDeming, W. Edwards, see https://www.deming.org/\\nDezfuli, H. “Role of System Safety in Risk-informed \\nDecisionmaking.” In Proceedings, the NASA \\nRisk Management Conference 2005 . Orlando, \\nDecember 7, 2005\\nDOD Architecture Framework (DODAF)  Version \\n2.02 Change 1, January 2015 http://dodcio.\\ndefense.gov/Library/DoDArchitectureFramework.\\naspx\\nDOD. Defense Acquisition Guidebook (DAG).  2014\\nDOD Defense Acquisition University (DAU). \\nSystems Engineering Fundamentals Guide . Fort \\nBelvoir, VA, 2001\\nDOD Defense Logistics Agency (DLA). Cataloging \\nHandbook, H4/H8 Series . Washington, DC, \\nFebruary 2003\\nDOD Defense Technical Information Center \\n(DTIC). Directory of Design Support Methods \\n(DDSM) . 2007. http://www.dtic.mil/dtic/tr/fulltext/\\nu2/a437106.pdf\\nDOD MIL-HDBK-727 (Validation Notice 1). \\nMilitary Handbook:  Design Guidance for \\nProducibility , U.S. Army Research Laboratory, \\nWeapons and Materials Research Directorate: \\nAdelphi,MD, 1990\\nDOD. MIL-HDBK-965. Acquisition Practices \\nfor Parts Management . Washington, DC, \\nSeptember 26, 1996. Notice 1: October 2000\\nDOD. MIL-STD-881C. Work Breakdown Structure \\n(WBS) for Defense Materiel Items . Washington, \\nDC, October 3, 2011DOD. MIL-STD-1472G, DOD Design Criteria \\nStandard: Human Engineering . Washington, \\nDC, January 11, 2012\\nDOD. MIL-STD-1540D, Product Verification \\nRequirements for Launch, Upper Stage, and Space \\nVehicles . January 15, 1999\\nDOD. MIL-STD-46855A, Human Engineering \\nRequirements for Military Systems, Equipment, \\nand Facilities . May 24, 2011. Replacement for \\nDOD HDBK 763 and DOD MIL-HDBK-\\n46855A, which have been cancelled.\\nDOD Office of the Under Secretary of Defense, \\nAcquisition, Technology, & Logistics. SD-10. \\nDefense Standardization Program: Guide \\nfor Identification and Development of Metric \\nStandards . Washington, DC, April, 2010\\nDOD Systems Management College. Systems \\nEngineering Fundamentals . Defense Acquisition \\nUniversity Press: Fort Belvoir, VA 22060-\\n5565, 2001 http://ocw.mit.edu/courses/\\naeronautics-and-astronautics/16-885j-air -\\ncraft-systems-engineering-fall-2005/readings/sef -\\nguide_01_01.pdf\\nDuren, R. et al., “Systems Engineering for the \\nKepler Mission: A Search for Terrestrial \\nPlanets,” IEEE Aerospace Conference , 2006\\nE\\nEggemeier, F. T., and G. F. Wilson. “Performance \\nand Subjective Measures of Workload in \\nMultitask Environments.” In Multiple-Task \\nPerformance . Edited by D. Damos. London: \\nTaylor and Francis, 1991\\nEndsley, M. R., and M. D. Rogers. “Situation \\nAwareness Information Requirements 274\\nNASA SYSTEMS ENGINEERING HANDBOOKBibliography\\nAnalysis for En Route Air Traffic Control.” In \\nProceedings of the Human Factors and Ergonomics \\nSociety 38th Annual Meeting . Santa Monica: \\nHuman Factors and Ergonomics Society, 1994\\nEslinger, Suellen. Software Acquisition Best Practices \\nfor the Early Acquisition Phases . El Segundo, CA: \\nThe Aerospace Corporation, 2004\\nEstefan, Jeff, Survey of Model-Based Systems \\nEngineering (MBSE) Methodologies , Rev B, \\nSection 3.2. NASA Jet Propulsion Laboratory \\n(JPL), June 10, 2008. The document was orig -\\ninally authored as an internal JPL report, and \\nthen modified for public release and submitted \\nto INCOSE to support the INCOSE MBSE \\nInitiative.\\nExecutive Order (EO) 12114, Environmental Effects \\nAbroad of Major Federal Actions . January 4, \\n1979.\\nExecutive Order (EO) 12770, Metric Usage in \\nFederal Government Programs , July 25, 1991.\\nExecutive Order (EO) 13637, Administration of \\nReformed Export Controls , March 8, 2013.\\nExtensible Markup Language (XML)  http://www.\\nw3.org/TR/REC-xml/\\nExtensible Markup Language (XML) Metadata \\nInterchange (XMI)  http://www.omg.org/spec/XMI/\\nF\\nFederal Acquisition Regulation (FAR). See: Code of \\nFederal Regulations (CFR), Title 48.\\nFederal Aviation Administration (FAA), \\nHF-STD-001, Human Factors Design Standard (HFDS).  Washington, DC, May 2003. \\nUpdated: May 03, 2012. hf.tc.faa.gov/hfds\\nFederal Highway Administration, and CalTrans. \\nSystems Engineering Guidebook for ITS , Version \\n2.0. Washington, DC: U.S. Department of \\nTransportation, 2007\\nFriedenthal, Sanford, Alan Moore, and Rick Steiner. \\nA Practical Guide to SysML: Systems Modeling \\nLanguage , Morgan Kaufmann Publishers, Inc., \\nJuly 2008\\nFuld, R. B. “The Fiction of Function Allocation.” \\nErgonomics in Design  (January 1993): 20–24\\nG\\nGarlan, D., W. Reinholtz, B. Schmerl, N. \\nSherman, T. Tseng. “Bridging the Gap \\nbetween Systems Design and Space Systems \\nSoftware,” Proceedings of the 29th IEEE/NASA \\nSoftware Engineering Workshop , 6-7 April 2005, \\nGreenbelt, MD, USA\\nGlass, J. T., V. Zaloom, and D. Gates. “A Micro-\\nComputer-Aided Link Analysis Tool.” \\nComputers in Industry  16, (1991): 179–87\\nGopher, D., and E. Donchin. “Workload: An \\nExamination of the Concept.” In Handbook \\nof Perception and Human Performance: Vol. II. \\nCognitive Processes and Performance . Edited by \\nK. R. Boff, L. Kaufman, and J. P. Thomas. New \\nYork: John Wiley & Sons, 1986\\nGriffin, Michael D., NASA Administrator. \\n“System Engineering and the Two Cultures \\nof Engineering.” Boeing Lecture, Purdue \\nUniversity, March 28, 2007275\\nNASA SYSTEMS ENGINEERING HANDBOOKBibliography\\nH\\nHart, S. G., and C. D. Wickens. “Workload \\nAssessment and Prediction.” In MANPRINT: \\nAn Approach to Systems Integration . Edited by H. \\nR. Booher. New York: Van Nostrand Reinhold, \\n1990\\nHoerl, R.W. and R.S. Snee, Statistical Thinking – \\nImproving Business Performance , John Wiley & \\nSons. 2012\\nHoffmann, Hans-Peter, “Harmony-SE/SysML \\nDeskbook: Model-Based Systems Engineering \\nwith Rhapsody,” Rev. 1.51, Telelogic/I-Logix \\nwhite paper, Telelogic AB, May 24, 2006\\nHofmann, Hubert F., Kathryn M. Dodson, Gowri \\nS. Ramani, and Deborah K. Yedlin. Adapting \\nCMMI® for Acquisition Organizations: A \\nPreliminary Report , CMU/SEI-2006-SR-005. \\nPittsburgh: Software Engineering Institute, \\nCarnegie Mellon University, 2006, pp. 338–40\\nHuey, B. M., and C. D. Wickens, eds. Workload \\nTransition . Washington, DC: National Academy \\nPress, 1993\\nI\\nIEEE STD 610.12-1990. IEEE Standard Glossary \\nof Software Engineering Terminology . 1999, \\nsuperceded by ISO/IEC/IEEE 24765:2010 , \\nSystems and Software Engineering – Vocabulary . \\nWashington, DC, 2010\\nIEEE STD 828. IEEE Standard for Software \\nConfiguration Management Plans . Washington, \\nDC, 1998\\nIEEE STD 1076-2008 IEEE Standard VHDL \\nLanguage Reference Manual , 03 February 2009IEEE STD 1220-2005. IEEE Standard for \\nApplication and Management of the Systems \\nEngineering Process , Washington, DC, 2005\\nIEEE Standard12207.1, EIA Guide for Information \\nTechnology Software Life Cycle Processes—Life \\nCycle Data , Washington, DC, 1997\\nINCOSE. Systems Engineering Handbook , Version \\n3.2.2. Seattle, 2011\\nINCOSE-TP-2003-002-04, Systems Engineering \\nHandbook: A Guide for System Life Cycle Processes \\nand Activities , Version 4, Edited by Walden, \\nDavid D., et al., 2015\\nINCOSE-TP-2003-020-01, Technical Measurement , \\nVersion 1.0, 27 December 2005. Prepared by \\nGarry J. Roedler (Lockheed Martin) and Cheryl \\nJones (U.S. Army).\\nINCOSE-TP-2004-004-02, Systems Engineering \\nVision 2020 , Version 2.03, September 2007, \\nhttp://www.incose.org/ProductsPubs/pdf/\\nSEVision2020_20071003_v2_03.pdf\\nINCOSE-TP-2005-001-03, Systems Engineering \\nLeading Indicators Guide , Version 2.0, January \\n29, 2010; available at http://seari.mit.edu/docu -\\nments/SELI-Guide-Rev2.pdf . Edited by Garry J. \\nRoedler and Howard Schimmoller (Lockheed \\nMartin), Cheryl Jones (U.S. Army), and \\nDonna H. Rhodes (Massachusetts Institute of \\nTechnology)\\nISO 9000:2015, Quality management systems – \\nFundamentals and vocabulary . Geneva: \\nInternational Organization for Standardization, \\n2015276\\nNASA SYSTEMS ENGINEERING HANDBOOKBibliography\\nISO 9001:2015 Quality Management Systems \\n(QMS).  Geneva: International Organization for \\nStandardization, September 2015\\nISO 9100/AS9100, Quality Systems Aerospace—\\nModel for Quality Assurance in Design, \\nDevelopment, Production, Installation, and \\nServicing . Geneva: International Organization \\nfor Standardization, 1999\\nISO 10007: 1995(E). Quality Management—\\nGuidelines for Configuration Management , \\nGeneva: International Organization for \\nStandardization, 1995\\nISO 10303-AP233, Application Protocol (AP) for \\nSystems Engineering Data Exchange (AP-233)  \\nWorking Draft 2 published July 2006\\nISO/TS 10303-433:2011 Industrial automation \\nsystems and integration – Product data representa -\\ntion and exchange – Part 433: Application mod -\\nule: AP233 systems engineering . ISO: Geneva, \\n2011\\nISO/IEC 10746-1 to 10746-4, ITU-T Specifications \\nX.901 to x.904, Reference Model of Open \\ndistributed Processing (RM-ODP),  Geneva: \\nInternational Organization for Standardization, \\n1998. http:// www.rm-odp.net\\nISO 13374-1, Condition monitoring and diagnostics of \\nmachines—Data processing, communication and \\npresentation – Part 1: General guidelines . Geneva: \\nInternational Organization for Standardization, \\n2002\\nISO/IEC 15288:2002. Systems Engineering—System \\nLife Cycle Processes . Geneva: International \\nOrganization for Standardization, 2002ISO/TR 15846. Information Technology—Software \\nLife Cycle Processes Configuration Management , \\nGeneva: International Organization for \\nStandardization, 1998\\nISO/IEC TR 19760:2003. Systems Engineering—A \\nGuide for the Application of ISO/IEC 15288. \\nGeneva: International Organization for \\nStandardization, 2003\\nISO/IEC/IEEE 24765:2010, Systems and \\nSoftware Engineering – Vocabulary. Geneva: \\nInternational Organization for Standardization, \\n2010\\nISO/IEC/IEEE 42010:2011. Systems and Software \\nEngineering—Architecture Description. \\nGeneva: International Organization for \\nStandardization, 2011 http://www.iso-architec -\\nture.org/ieee-1471/index.html\\n(The) Invention Secrecy Act of 1951, see 35 U.S.C. \\n§181–§188. Secrecy of Certain Inventions and \\nFiling Applications in Foreign Country; §181 – \\nSecrecy of Certain Inventions and Withholding \\nof Patent\\nJ\\nJoint (cost and schedule) Confidence Level (JCL). \\nFrequently asked questions (FAQs) can be found \\nat: http://www.nasa.gov/pdf/394931main_JCL_\\nFAQ_10_12_09.pdf\\nJennions, Ian K. editor. Integrated Vehicle Health \\nManagement (IVHM): Perspectives on an \\nEmerging Field . SAE International, Warrendale \\nPA (IVHM Book) September 27, 2011\\nJennions, Ian K. editor. Integrated Vehicle Health \\nManagement (IVHM): Business Case Theory and 277\\nNASA SYSTEMS ENGINEERING HANDBOOKBibliography\\nPractice . SAE International, Warrendale PA \\n(IVHM Book) November 12, 2012\\nJennions, Ian K. editor. Integrated Vehicle Health \\nManagement (IVHM): The Technology . SAE \\nInternational, Warrendale PA (IVHM Book) \\nSeptember 5, 2013\\nJohnson, Stephen B. et al., editors. System Health \\nManagement with Aerospace Applications . John \\nWiley & Sons, Ltd, West Sussex, UK, 2011\\nJones, E. R., R. T. Hennessy, and S. Deutsch, \\neds. Human Factors Aspects of Simulation . \\nWashington, DC: National Academy Press, \\n1985\\nK\\nKaplan, S., and B. John Garrick. “On the \\nQuantitative Definition of Risk.” Risk Analysis  \\n1(1). 1981\\nKarpati, G., Martin, J., Steiner, M., Reinhardt, \\nK., “The Integrated Mission Design Center \\n(IMDC) at NASA Goddard Space Flight \\nCenter,” IEEE Aerospace Conference 2003 \\nProceedings , Volume 8, Page(s): 8_3657–8_3667, \\n2003\\nKeeney, Ralph L. Value-Focused Thinking: A Path \\nto Creative Decisionmaking . Cambridge, MA: \\nHarvard University Press, 1992\\nKeeney, Ralph L., and Timothy L. McDaniels. “A \\nFramework to Guide Thinking and Analysis \\nRegarding Climate Change Policies.” Risk \\nAnalysis  21(6): 989–1000. 2001\\nKeeney, Ralph L., and Howard Raiffa. Decisions \\nwith Multiple Objectives: Preferences and Value Tradeoffs . Cambridge, UK: Cambridge \\nUniversity Press, 1993\\nKirwin, B., and L. K. Ainsworth. A Guide to Task \\nAnalysis . London: Taylor and Francis, 1992\\nKluger, Jeffrey with Dan Cray, “Management \\nTips from the Real Rocket Scientists,” Time \\nMagazine , November 2005\\nKnowledge Based Systems, Inc. (KBSI), Integration \\nDefinition for functional modeling (IDEF0) ISF0 \\nFunction Modeling Method , found at http://www.\\nidef.com/idef0.htm\\nKruchten, Philippe B. The Rational Unified Process: \\nAn Introduction , Third Edition, Addison-Wesley \\nProfessional: Reading, MA, 2003\\nKruchten, Philippe B. “A 4+1 view model of soft -\\nware architecture,” IEEE Software Magazine  \\n12(6) (November 1995), 42–50\\nKurke, M. I. “Operational Sequence Diagrams in \\nSystem Design.” Human Factors  3: 66–73. 1961\\nL\\nLarson, Wiley J.et al.. Applied Space Systems \\nEngineering: A Practical Approach to Achieving \\nTechnical Baselines . 2nd Edition, Boston, \\nMA: McGraw-Hill Learning Solutions, CEI \\nPublications, 2009\\nLong, James E., Relationships Between Common \\nGraphical Representations in Systems Engineering . \\nVienna, VA: Vitech Corporation, 2002\\nLong, James E., “Systems Engineering (SE) 101,” \\nCORE®: Product & Process Engineering Solutions , \\nVitech training materials. Vienna, VA: Vitech \\nCorporation, 2000278\\nNASA SYSTEMS ENGINEERING HANDBOOKBibliography\\nM\\nMaier, M.W. “Architecting Principles for Systems-of-\\nSystems,” Systems Engineering  1(1998), 267-284, \\nJohn Wiley & Sons, Inc.\\nMaier, M.W., D. Emery, and R. Hillard, “ANSI/\\nIEEE 1471 and Systems Engineering,” \\nSystems Engineering  7 (2004), 257–270, Wiley \\nInterScience, http:// www.interscience.wiley.com\\nMaier, M.W. “System and Software Architecture \\nReconciliation,” Systems Engineering  9 (2006), \\n146–159, Wiley InterScience, http:// www.inter -\\nscience.wiley.com\\nMaier, M.W., and E. Rechtin, The Art of Systems \\nArchitecting , 3rd Edition, CRC Press, Boca \\nRaton, FL, 2009\\nMartin, James N., Processes for Engineering a System: \\nAn Overview of the ANSI/GEIA EIA-632 \\nStandard and Its Heritage . New York: Wiley & \\nSons, 2000\\nMartin, James N., Systems Engineering Guidebook: A \\nProcess for Developing Systems and Products . Boca \\nRaton: CRC Press, 1996.\\nMathworks: Matlab  http://www.mathworks.com/\\nMcGuire, M., Oleson, S., Babula, M., and Sarver-\\nVerhey, T., “Concurrent Mission and Systems \\nDesign at NASA Glenn Research Center: The \\norigins of the COMPASS Team,” AIAA Space \\n2011 Proceedings , September 27-29, 2011, Long \\nBeach, CA\\nMeister, David, Behavioral Analysis and Measurement \\nMethods . New York: John Wiley & Sons, 1985Meister, David, Human Factors: Theory and Practice . \\nNew York: John Wiley & Sons, 1971\\n(The) Metric Conversion Act of 1975 (Public Law \\n94-168) amended by the Omnibus Trade and \\nCompetitiveness Act of 1988 (Public Law 100-\\n418), the Savings in Construction Act of 1996 \\n(Public Law 104-289), and the Department of \\nEnergy High-End Computing Revitalization \\nAct of 2004 (Public Law 108-423). See 15 \\nU.S.C. §205a et seq.\\nMiao, Y., and J. M. Haake. “Supporting Concurrent \\nDesign by Integrating Information Sharing and \\nActivity Synchronization.” In Proceedings of the \\n5th ISPE International Conference on Concurrent \\nEngineering Research and Applications (CE98) . \\nTokyo, 1998, pp. 165–74\\nThe Mitre Corporation, Common Risks and Risk \\nMitigation Actions for a COTS-based System . \\nMcLean, VA. http://www2.mitre.org/…/files/\\nCommonRisksCOTS.doc  (no date)\\nMODAF http://www.modaf.com/\\nMoeller, Robert C., Chester Borden, Thomas \\nSpilker, William Smythe, Robert Lock , \\n“Space Missions Trade Space Generation \\nand Assessment using the JPL Rapid Mission \\nArchitecture (RMA) Team Approach,” IEEE \\nAerospace Conference , Big Sky, Montana, March \\n2011\\nMorgan, M. Granger, and M. Henrion, Uncertainty: \\nA Guide to Dealing with Uncertainty in \\nQuantitative Risk and Policy Analysis . \\nCambridge, UK: Cambridge University Press, \\n1990279\\nNASA SYSTEMS ENGINEERING HANDBOOKBibliography\\nM. Moshir, et al., “Systems engineering and appli -\\ncation of system performance modeling in SIM \\nLite mission,” Proceedings. SPIE  7734, 2010\\nMulqueen, J.; R. Hopkins; D. Jones, “The MSFC \\nCollaborative Engineering Process for \\nPreliminary Design and Concept Definition \\nStudies.” 2012 http://ntrs.nasa.gov/archive/nasa/\\ncasi.ntrs.nasa.gov/20120001572.pdf\\nNASA Publications\\nNASA Federal Acquisition Regulation (FAR) \\nSupplement (NFS) 1834.201, Earned Value \\nManagement System Policy\\nNASA Form (NF) 1686, NASA Scientific and \\nTechnical Document Availability Authorization \\n(DAA) for Administratively Controlled \\nInformation\\nReports\\nNASA Chief Engineer and the NASA Integrated \\nAction Team (NIAT) report, “Enhancing \\nMission Success—A Framework for the Future,” \\nDecember 21, 2000. Authors: McBrayer, Robert \\nO and Thomas, Dale, NASA Marshall Space \\nFlight Center, Huntsville, AL United States\\nNASA. Columbia Accident Investigation Board \\n(CAIB) Report , 6 volumes: Aug. 26, Oct. 2003. \\nhttp://www.nasa.gov/columbia/caib/html/report.\\nhtml\\nNASA. NOAA N-Prime Mishap Investigation Final \\nReport , Sept. 13, 2004. http://www.nasa.gov/pdf/\\n65776main_noaa_np_mishap.pdf\\nNASA. Diaz Report, A Renewed Commitment to \\nExcellence: An Assessment of the NASA Agency-\\nwide Applicability of the Columbia Accident Investigation Board Report , January 30, 2004. \\nMr. Al Diaz, Director, Goddard Space Flight \\nCenter, and team\\nNASA JPL D-71990, Europa Study 2012 Full Report . \\nMay 1 2012, publicly available here: http://\\nsolarsystem.nasa.gov/europa/2012study.cfm\\nNASA Office of Inspector General. Final \\nMemorandum on NASA’s Acquisition Approach \\nRegarding Requirements for Certain Software \\nEngineering Tools to Support NASA Programs , \\nAssignment No. S06012. Washington, DC, \\n2006\\nNASA Office of Inspector General. Performance-\\nBased Contracting https://oig.nasa.gov/august/\\nreport/FY06/s06012\\nSpecialty Web Sites\\nNASA Engineering Network (NEN) Systems \\nEngineering Community of Practice (SECoP) \\nlocated at https://nen.nasa.gov/web/se\\nNASA Engineering Network (NEN) Systems \\nEngineering Community of Practice (SECoP) \\nunder Tools and Methods at https://nen.nasa.\\ngov/web/se/tools/  and then NASA Tools & \\nMethods\\nNASA Engineering Network (NEN) V&V \\nCommunity of Practice, located at https://nen.\\nnasa.gov/web/se\\nNASA Engineering Network (NEN) EVM \\nCommunity of Practice, https://nen.nasa.gov/\\nweb/pm/evm\\nNASA EVM website http://evm.nasa.gov/index.html280\\nNASA SYSTEMS ENGINEERING HANDBOOKBibliography\\nNASA Procurement Library found at http://www.\\nhq.nasa.gov/office/procurement/\\nConference Publications\\nNASA 2011 Statistical Engineering Symposium, \\nProceedings . http://engineering.larc.nasa.gov/2011_\\nNASA_Statistical_Engineering_Symposium.html\\nAerospace Conference, 2007 IEEE  Big Sky, MT 3–10 \\nMarch 2007. NASA/Aerospace Corp. paper: \\n“Using Historical NASA Cost and Schedule \\nGrowth to Set Future Program and Project \\nReserve Guidelines,” by Emmons, D. L., R.E. \\nBitten, and C.W. Freaner. IEEE Conference \\nPublication pages: 1–16, 2008. Also presented \\nat the NASA Cost Symposium, Denver CO, \\nJuly 17–19, 2007\\nNASA Cost Symposium 2014 , NASA “Mass Growth \\nAnalysis: Spacecraft & Subsystems.” LaRC, \\nAugust 14th, 2014. Presenter: Vincent Larouche \\n– Tecolote Research, also James K. Johnson, \\nNASA HQ Study Point of Contact\\nPlanetary Science Subcommittee, NASA Advisory \\nCouncil, 23 June, 2008, NASA GSFC. NASA/\\nAerospace Corp. presentation; “An Assessment \\nof the Inherent Optimism in Early Conceptual \\nDesigns and its Effect on Cost and Schedule \\nGrowth,” by Freaner, Claude, Bob Bitten, Dave \\nBearden, and Debra Emmons\\nTechnical Documents\\nNASA Office of Chief Information Officer (OCIO). \\nInformation Technology Systems Engineering \\nHandbook  Version 2.0\\nNASA Science Mission Directorate, Risk \\nCommunication Plan for Planetary and Deep \\nSpace Missions , 1999NASA PD-EC-1243, Preferred Reliability Practices \\nfor Fault Protection , October 1995\\nNASA-CR-192656, Contractor Report:  Research \\nand technology goals and objectives for Integrated \\nVehicle Health Management (IVHM).  October \\n10, 1992\\nNASA Jet Propulsion Laboratory (JPL), JPL-\\nD-17868 (REV.1), JPL Guideline: Design, \\nVerification/Validation and Operations Principles \\nfor Flight Systems . February 16, 2001\\nNASA Lyndon B. Johnson Space Center (JSC-\\n65995), Commercial Human Systems Integration \\nProcesses (CHSIP) , May 2011\\nNASA/TP-2014-218556, Technical Publication: \\nHuman Integration Design Processes (HIDP).  \\nNASA ISS Program, Lyndon B. Johnson Space \\nCenter, Houston TX, September 2014. http://\\nston.jsc.nasa.gov/collections/TRS/_techrep/\\nTP-2014-218556.pdf\\nNASA Lyndon B. Johnson Space Center (JSC-\\n60576), National Space Transportation System \\n(NSTS), Space Shuttle Program, Transition \\nManagement Plan , May 9, 2007\\nNASA Langley Research Center (LARC) Guidance \\non System and Software Metrics for Performance-\\nBased Contracting . 2013 sites-e.larc.nasa.gov/\\nsweng/files/2013/05/Guidance_on_Metrics_for_\\nPBC_R1V01.doc\\nNASA Langley Research Center (LARC), \\nInstructional Handbook for Formal Inspections . \\n2013 http://sw-eng.larc.nasa.gov/files/2013/05/\\nInstructional-Handbook-for-Formal-Inspections.\\npdf281\\nNASA SYSTEMS ENGINEERING HANDBOOKBibliography\\nNASA/TM-2008-215126/Volume II (NESC-\\nRP-06-108/05-173-E/Part 2), Technical \\nMemorandum: Design Development Test and \\nEvaluation (DDT&E) Considerations for Safe and \\nReliable Human-Rated Spacecraft Systems . April \\n2008.Volume II: Technical Consultation Report . \\nJames Miller, Jay Leggett, and Julie Kramer-\\nWhite, NASA Langley Research Center, \\nHampton VA, June 14, 2007\\nNASA Reference Publication 1370. Training \\nManual for Elements of Interface Definition and \\nControl.  Vincent R. Lalli, Robert E. Kastner, \\nand Henry N. Hartt. NASA Lewis Research \\nCenter, Cleveland OH, January 1997\\nNASA. Systems Engineering Leading Indicators \\nGuide , http://seari.mit.edu/\\nNASA Cost Estimating Handbook (CEH) , Version 4, \\nFebruar y 2015\\nNASA  Financial Management Requirements (FMR)  \\nVolume 4\\nSpecial Publications\\nNASA/SP-2010-576 NASA Risk-Informed Decision \\nMaking Handbook\\nNASA/SP-2012-599, NASA’s Earned Value \\nManagement (EVM) Implementation Handbook\\nNASA/SP-2010-3403, NASA Schedule Management \\nHandbook\\nNASA/SP-2010-3404, NASA Work Breakdown \\nStructure Handbook\\nNASA/SP-2010-3406, Integrated Baseline Review \\n(IBR) HandbookNA SA /SP-2010 -3407, Human Integration Design \\nHandbook (HIDH)\\nNASA/SP-2011-3421, Probabilistic Risk Assessment \\nProcedures Guide for NASA Managers and \\nPractitioners\\nNASA/SP-2011-3422, NASA Risk Management \\nHandbook\\nNASA/SP-2013-3704, Earned Value Management \\n(EVM) System Description\\nNASA/SP-2014-3705, NASA Space Flight Program \\nand Project Management Handbook\\nNASA /SP-2015-3709, Human Systems Integration \\nPractitioners Guide\\nHandbooks and Standards\\nNASA-HDBK-1002, Fault Management (FM) \\nHandbook , Draft 2, April 2012\\nNASA-HDBK-2203, NASA Software Engineering \\nHandbook , February 28, 2013\\nNASA Safety Standard (NSS) 1740.14, Guidelines \\nand Assessment Procedures for Limiting Orbital \\nDebris . Washington, DC, 1995 http://www.\\nhq.nasa.gov/office/codeq/doctree/174014.htm  \\nNASA-STD 8719.14 should be used in place \\nof NSS 1740.14 to implement NPR 8715.6. See \\nNPR 8715.6  for restrictions on the use of NSS \\n1740.14.\\nNASA GSFC-STD-1000, Rules for the Design, \\nDevelopment, Verification, and Operation of \\nFlight Systems . NASA Goddard Space Flight \\nCenter, February 8, 2013282\\nNASA SYSTEMS ENGINEERING HANDBOOKBibliography\\nNASA-STD-3001, Space Flight Human System \\nStandard . Volume 1: Crew Health . Rev. A, July \\n30, 2014\\nNASA-STD-3001, Space Flight Human System \\nStandard . Volume 2: Human Factors, \\nHabitability, and Environmental Health . Rev. A, \\nFebruary 10, 2015\\nNASA GSFC-STD-7000, Goddard Technical \\nStandard: General Environmental Verification \\nStandard (GEVS) for GSFC Flight Programs and \\nProjects . Goddard Space Flight Center, April \\n2005\\nNASA KSC-NE-9439 Kennedy Space Center Design \\nEngineering Handbook, Best Practices for Design \\nand Development of Ground Systems . Kennedy \\nSpace Center, November 20 2009\\nNASA-STD-7009, Standard for Models and \\nSimulations . Washington, DC, October 18, \\n2013\\nNASA-STD-8719.13, Software Safety Standard , Rev \\nC. Washington, DC, May 7, 2013\\nNASA-STD-8719.14, Handbook for Limiting Orbital \\nDebris . Rev A with Change 1. December 8, 2011\\nNASA-STD-8729.1, Planning, Developing, and \\nMaintaining an Effective Reliability and \\nMaintainability (R&M) Program . Washington, \\nDC, December 1, 1998\\nPolicy Directives\\nNPD 1001.0, 2014 NASA Strategic Plan\\nNID 1600.55, Sensitive But Unclassified (SBU) \\nControlled InformationNPD 2820.1, NASA Software Policy\\nNPD 7120.4, NASA Engineering and Program/\\nProject Management Policy\\nNPD 7120.6, Knowledge Policy on Programs and \\nProjects\\nNPD 8010.2, Use of the SI (Metric) System of \\nMeasurement in NASA Programs\\nNPD 8010.3, Notification of Intent to \\nDecommission or Terminate Operating Space \\nSystems and Terminate Missions\\nNPD 8020.7, Biological Contamination Control for \\nOutbound and Inbound Planetary Spacecraft\\nNPD 8730.5, NASA Quality Assurance Program \\nPolicy\\nProcedural Requirements\\nNPR 1080.1, Requirements for the Conduct of \\nNASA Research and Technology (R&T)\\nNPR 1441.1, NASA Records Retention Schedules\\nNPR 1600.1, NASA Security Program Procedural \\nRequirements\\nNPR 2210.1, Release of NASA Software\\nNPR 2810.1, Security of Information Technology\\nLPR 5000.2, Procurement Initiator’s Guide. NASA \\nLangley Research Center (LARC)\\nJPR 7120.3, Project Management: Systems \\nEngineering & Project Control Processes and \\nRequirements. NASA Lyndon B. Johnson Space \\nCenter (JSC)283\\nNASA SYSTEMS ENGINEERING HANDBOOKBibliography\\nNPR 7120.5, NASA Space Flight Program \\nand Project Management Processes and \\nRequirements\\nNPR 7120.7, NASA Information Technology and \\nInstitutional Infrastructure Program and Project \\nManagement Requirements\\nNPR 7120.8, NASA Research and Technology \\nProgram and Project Management \\nRequirements\\nNPR 7120.10, Technical Standards for NASA \\nPrograms and Projects\\nNPR 7120.11, NASA Health and Medical Technical \\nAuthority (HMTA) Implementation\\nNPR 7123.1, Systems Engineering Processes and \\nRequirements\\nNPR 7150.2, NASA Software Engineering \\nRequirements\\nNPR 8000.4, Risk Management Procedural \\nRequirements\\nNPI 8020.7, NASA Policy on Planetary Protection \\nRequirements for Human Extraterrestrial \\nMissions\\nNPR 8020.12, Planetary Protection Provisions for \\nRobotic Extraterrestrial Missions\\nAPR 8070.2, EMI/EMC Class D Design and \\nEnvironmental Test Requirements. NASA Ames \\nResearch Center (ARC)\\nNPR 8580.1, Implementing the National \\nEnvironmental Policy Act and Executive Order \\n12114NPR 8705.2, Human-Rating Requirements for \\nSpace Systems\\nNPR 8705.3, Probabilistic Risk Assessment \\nProcedures Guide for NASA Managers and \\nPractitioners\\nNPR 8705.4, Risk Classification for NASA Payloads\\nNPR 8705.5, Probabilistic Risk Assessment (PRA) \\nProcedures for NASA Programs and Projects\\nNPR 8705.6, Safety and Mission Assurance (SMA) \\nAudits, Reviews, and Assessments\\nNPR 8710.1, Emergency Preparedness Program\\nNPR 8715.2, NASA Emergency Preparedness Plan \\nProcedural Requirements\\nNPR 8715.3, NASA General Safety Program \\nRequirements\\nNPR 8715.6, NASA Procedural Requirements for \\nLimiting Orbital Debris\\nNPR 8735.2, Management of Government Quality \\nAssurance Functions for NASA Contracts\\nNPR 8900.1, NASA Health and Medical \\nRequirements for Human Space Exploration\\nWork Instructions\\nMSFC NASA MWI 8060.1, Off-the-Shelf Hard -\\nware Utilization in Flight Hardware Develop -\\nment. NASA Marshall Space Flight Center.\\nJSC Work Instruction EA-WI-016, Off-the-Shelf \\nHardware Utilization in Flight Hardware \\nDevelopment. NASA Lyndon B. Johnson Space \\nCenter.284\\nNASA SYSTEMS ENGINEERING HANDBOOKBibliography\\nAcquisition Documents\\nNASA. The SEB Source Evaluation Process . \\nWashington, DC, 2001\\nNASA. Solicitation to Contract Award . Washington, \\nDC, NASA Procurement Library, 2007\\nNASA. Statement of Work Checklist . Washington, \\nDC. See: Appendix P  in this handbook.\\nN\\n(The) National Environmental Policy Act of 1969 \\n(NEPA). See 42 U.S.C. 4321-4347. https://ceq.\\ndoe.gov/welcome.html\\nNational Research Council (NRC) of the National \\nAcademy of Sciences (NAS), The Planetary \\nDecadal Survey 2013–2022, Vision and Voyagers \\nfor Planetary Science in the Decade 2013 –2022 , \\nThe National Academies Press: Washington, \\nD.C., 2011. http://www.nap.edu\\nNIST Special Publication 330: The International \\nSystem of Units (SI)  Barry N. Taylor and Ambler \\nThompson, Editors, March 2008. The United \\nStates version of the English text of the eighth \\nedition (2006) of the International Bureau of \\nWeights and Measures publication Le Système \\nInternational d’ Unités (SI)\\nNIST Special Publication 811: NIST Guide for the \\nUse of the International System of Units (SI)  A. \\nThompson and B. N. Taylor, Editors. Created \\nJuly 2, 2009; Last updated January 28, 2016\\nNIST, Federal Information Processing Standard \\nPublication (FIPS PUB) 199, Standards for \\nSecurity Categorization of Federal Information \\nand Information Systems,  February 2004O\\nOberto, R.E., Nilsen, E., Cohen, R., Wheeler, R., \\nDeFlorio, P., and Borden, C., “The NASA \\nExploration Design Team; Blueprint for a \\nNew Design Paradigm”, 2005 IEEE Aerospace \\nConference , Big Sky, Montana, March 2005\\nObject Constraint Language (OCL)  http://www.omg.\\norg/spec/OCL/\\nOffice of Management and Budget (OMB) Circular \\nA-94, Guidelines and Discount Rates for Benefit-\\nCost Analysis of Federal Programs , October 29, \\n1992\\nOliver, D., T. Kelliher, and J. Keegan, Engineering \\nComplex Systems with Models and Objects , New \\nYork, NY, USA: McGraw-Hill 1997\\nOOSEM Working Group, Object-Oriented Systems \\nEngineering Method (OOSEM) Tutorial , Version \\n03.00, Lockheed Martin Corporation and \\nINCOSE, October 2008\\nOWL, Web Ontology Language (OWL)  http://www.\\nw3.org/2001/sw/wiki/OWL\\nP\\nParedis, C., Y. Bernard, R. Burkhart, H.P. Koning, \\nS. Friedenthal, P. Fritzon, N.F. Rouquette, \\nW. Schamai. “Systems Modeling Language \\n(SysML)-Modelica Transformation.” INCOSE \\n2010\\nPennell, J. and Winner, R., “Concurrent \\nEngineering: Practices and Prospects,” Global \\nTelecommunications Conference, GLOBECOM \\n‘89, 1989\\nPresidential Directive/National Security Council \\nMemorandum No. 25 (PD/NSC-25), “Scientific 285\\nNASA SYSTEMS ENGINEERING HANDBOOKBibliography\\nor Technological Experiments with Possible \\nLarge-Scale Adverse Environmental Effects \\nand Launch of Nuclear Systems into Space,” as \\namended May 8, 1996\\nPresidential Policy Directive PPD-4 (2010), National \\nSpace Policy\\nPresidential Policy Directive PPD-21 (2013), Critical \\nInfrastructure Security and Resilience\\nPrice, H. E. “The Allocation of Functions in \\nSystems.” Human Factors  27: 33–45. 1985\\nThe Project Management Institute® (PMI). Practice \\nStandards for Work Breakdown Structures . \\nNewtown Square, PA, 2001\\nQ\\nQuery View Transformation (QVT)  http://www.omg.\\norg/spec/QVT/1.0/\\nR\\nRasmussen, Robert. “Session 1: Overview of \\nState Analysis,” (internal document), State \\nAnalysis Lite Course , Jet Propulsion Laboratory, \\nCalifornia Institute of Technology, Pasadena, \\nCA, 2005\\nR. Rasmussen, B. Muirhead, Abridged Edition:  \\nA Case for Model-Based Architecting in NASA , \\nCalifornia Institute of Technology, August 2012\\nRechtin, Eberhardt. Systems Architecting of \\nOrganizations: Why Eagles Can’t Swim . Boca \\nRaton: CRC Press, 2000\\nS\\nSaaty, Thomas L. The Analytic Hierarchy Process . \\nNew York: McGraw-Hill, 1980SAE Standard AS5506B, Architecture Analysis & \\nDesign Language (AADL) , SAE International, \\nSeptember 10, 2012\\nSAE International and the European Association of \\nAerospace Industries (EAAI) AS9100C, Quality \\nManagement Systems (QMS): Requirements for \\nAviation, Space, and Defense Organizations \\nRevision C, January 15, 2009\\nSAE International/Electronic Industries Alliance \\n(EIA) 649B-2011, Configuration Management \\nStandard (Aerospace Sector) , April 1, 2011\\nSage, Andrew, and William Rouse. The Handbook \\nof Systems Engineering and Management , New \\nYork: Wiley & Sons, 1999\\nShafer, J. B. “Practical Workload Assessment in the \\nDevelopment Process.” In Proceedings of the \\nHuman Factors Society 31st Annual Meeting , \\nSanta Monica: Human Factors Society, 1987\\nShames, P., and J. Skipper. “Toward a Framework \\nfor Modeling Space Systems Architectures,” \\nSpaceOps 2006 Conference , AIAA 2006-5581, \\n2006\\nShaprio, J., “George H. Heilmeier,” IEEE Spectrum , \\n31(6), 1994, pg. 56–59 http://ieeexplore.ieee.org/\\niel3/6/7047/00284787.pdf?arnumber=284787\\nSoftware Engineering Institute (SEI). A Framework \\nfor Software Product Line Practice , Version 5.0. \\nCarnegie Mellon University, http:// www.sei.cmu.\\nedu/productlines/frame_report/arch_def.htm\\nStamelatos, M., H. Dezfuli, and G. Apostolakis. \\n“A Proposed Risk-Informed Decision making \\nFramework for NASA.” In Proceedings of the 8th \\nInternational Conference on Probabilistic Safety 286\\nNASA SYSTEMS ENGINEERING HANDBOOKBibliography\\nAssessment and Management . New Orleans, LA, \\nMay 14–18, 2006\\nStern, Paul C., and Harvey V. Fineberg, eds. \\nUnderstanding Risk: Informing Decisions in a \\nDemocratic Society . Washington, DC: National \\nAcademies Press, 1996\\nSystems Modeling Language (SysML)  http://www.\\nomgsysml.org/\\nT\\nTaylor, Barry. Guide for the Use of the International \\nSystem of Units (SI),  Special Publication 811. \\nGaithersburg, MD: NIST, Physics Laboratory, \\n2007\\nU\\nUnified Modeling Language (UML)  http://www.uml.\\norg/\\nUPDM: Unified Profile for the (US) Department \\nof Defense Architecture Framework (DoDAF) \\nand the (UK) Ministry Of Defense Architecture \\nFramework (MODAF)  http://www.omg.org/spec/\\nUPDM/\\nU.S. Air Force. SMC Systems Engineering Primer \\nand Handbook , 3rd ed. Los Angeles: Space and \\nMissile Systems Center, 2005\\nU. S. Chemical Safety Board (CSB) case study \\nreports on mishaps found at: http://www.csb.gov/\\nU.S. Navy. Naval Air Systems Command, Systems \\nEngineering Guide : 2003 (based on require -\\nments of ANSI/EIA 632:1998). Patuxent River, \\nMD, 2003\\nU.S. Nuclear Regulatory Commission. SECY-\\n98-144, White Paper on Risk-Informed and Performance-Based Regulation , Washington, DC, \\n1998\\nU.S. Nuclear Regulatory Commission. NUREG-\\n0700, Human-System Interface Design Review \\nGuidelines , Rev.2. Washington, DC, Office of \\nNuclear Regulatory Research, 2002\\nUnited Nations, Office for Outer Space Affairs. \\nTreaty of Principles Governing the Activities of \\nStates in the Exploration and Use of Outer Space, \\nIncluding the Moon and Other Celestial Bodies . \\nKnown as the “Outer Space Treaty of 1967”\\nW\\nWall, S., “Use of Concurrent Engineering in Space \\nMission Design,” Proceedings of EuSEC 2000 , \\nMunich, Germany, September 2000\\nWarfield, K., “Addressing Concept Maturity in the \\nEarly Formulation of Unmanned Spacecraft,” \\nProceedings of the 4th International Workshop \\non System and Concurrent Engineering for Space \\nApplications , October 13–15, 2010, Lausanne, \\nSwitzerland\\nWeb Ontology Language (OWL) http://www.\\nw3.org/2001/sw/wiki/OWL\\nWessen, Randii R., Chester Borden, John Ziemer, \\nand Johnny Kwok. “Space Mission Concept \\nDevelopment Using Concept Maturity Levels,” \\nConference paper presented at the American \\nInstitute of Aeronautics and Astronautics \\n(AIAA) Space 2013 Conference and Exposition; \\nSeptember 10–12, 2013; San Diego, CA. \\nPublished in the AIAA Space 2013 Proceedings\\nWinner, R., Pennell, J., Bertrand, H., and \\nSlusarczuk, M., The Role Of Concurrent \\nEngineering In Weapons System Acquisition , 287\\nNASA SYSTEMS ENGINEERING HANDBOOKBibliography\\nInstitute of Defense Analyses (IDA) Report \\nR-338, Dec 1988\\nWolfram, Mathematica http://www.wolfram.com/\\nmathematica/\\nX\\nXMI: Extensible Markup Language (XML) \\nMetadata Interchange (XMI) http://www.omg.\\norg/spec/XMI/XML: Extensible Markup Language (XML) http://\\nwww.w3.org/TR/REC-xml/\\nZ\\nZiemer, J., Ervin, J., Lang, J., “Exploring Mission \\nConcepts with the JPL Innovation Foundry \\nA-Team,” AIAA Space 2013 Proceedings , \\nSeptember 10–12, 2013, San Diego, CA'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  extracts the first 100 characters from the variable 'raw_text'.\n",
        "raw_text[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YNSpLLUUAWmw",
        "outputId": "24a39a77-f688-4a44-be51-b7986ceacc8d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'National Aeronautics and  \\n Space Administration\\nNASA  \\nSYSTEMS ENGINEERING  \\nHANDBOOK\\ndesign\\ntest\\ni'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 06: Split text into smaller Chunks"
      ],
      "metadata": {
        "id": "SWgsIXWgAlWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specifying the length_function to len, which is a built-in Python function that returns the length of an object.\n",
        "# This function will be used to calculate the length of the text being processed, which is likely important for chunking.\n",
        "textsplitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\",\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len\n",
        ")\n"
      ],
      "metadata": {
        "id": "qDAqppAAAfd4"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the raw text into smaller text chunks\n",
        "texts = textsplitter.split_text(raw_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpzTTASwBZd1",
        "outputId": "ceb76d0f-d8ef-4690-8565-091097777e64"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 1113, which is longer than the specified 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the length of the texts list to see how many text chunks were generated\n",
        "len_texts = len(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onOrCOZwBjhR",
        "outputId": "a7d0a857-8a83-438c-f4d0-2104e046e2d2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1041"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "zVav9aa7BlM9",
        "outputId": "2f907114-fa33-489f-c05c-96edaa059246"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'National Aeronautics and  \\n Space Administration\\nNASA  \\nSYSTEMS ENGINEERING  \\nHANDBOOK\\ndesign\\ntest\\nintegrate\\nfly\\nwww.nasa.govNASA SP-2016-6105 Rev2 supersedes SP-2007-6105 Rev 1 dated December, 2007.\\nCover photos:  Top left:  In this photo, engineers led by researcher Greg Gatlin have sprayed fluorescent oil on a 5.8 percent scale \\nmodel of a futuristic hybrid wing body during tests in the 14- by 22-Foot Subsonic Wind Tunnel at NASA’s Langley Research Center \\nin Hampton, VA. The oil helps researchers “see” the flow patterns when air passes over and around the model. (NASA Langley/\\nPreston Martin) Top right:  Water impact test of a test version of the Orion spacecraft took place on August 24, 2016, at NASA \\nLangley Research Center (NASA) Bottom left:  two test mirror segments are placed onto the support structure that will hold them. \\n(NASA/Chris Gunn) Bottom right: This self-portrait of NASA’s Curiosity Mars rover shows the vehicle at the “Mojave” site, where its'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "b7a0vxYLBo0c",
        "outputId": "ceedafee-501d-40fb-ca7f-c0a8f8865155"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'(NASA/Chris Gunn) Bottom right: This self-portrait of NASA’s Curiosity Mars rover shows the vehicle at the “Mojave” site, where its \\ndrill collected the mission’s second taste of Mount Sharp. (NASA/JPL-Caltech/MSSS)\\nComments, questions, and suggestions  regarding this document can be sent to:\\nSteven R. Hirshorn\\nChief Engineer, Aeronautics Research Mission Directorate (ARMD)\\nOffice of the Chief Engineer\\nNASA Headquarters, Room 6D37\\n300 E St SW\\nWashington, DC 20546-0001\\n202-358-0775\\nsteven.r.hirshorn@nasa.goviii\\nNASA SYSTEMS ENGINEERING HANDBOOKTable of Contents\\nPreface                                        viii\\nAcknowledgments                              ix\\n1.0 Introduction  1\\n1.1 Purpose    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 1\\n1.2 Scope and Depth    .  .  .  .  .  .  .  .  .  .  . 1\\n2.0 Fundamentals of Systems Engineering  3\\n2.1 The Common Technical Processes  \\nand the SE Engine   .  .  .  .  .  .  .  .  .  .  . 5\\n2.2 An Overview of the SE Engine by'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "PL07N3E2B9f3",
        "outputId": "f5e4c923-fbe4-4843-b667-28d9f6131a9f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0 Fundamentals of Systems Engineering  3\\n2.1 The Common Technical Processes  \\nand the SE Engine   .  .  .  .  .  .  .  .  .  .  . 5\\n2.2 An Overview of the SE Engine by  \\nProject Phase    .  .  .  .  .  .  .  .  .  .  .  .  . 8\\n2.3 Example of Using the SE Engine   .  .  .  .  10\\n2.4 Distinctions between Product  \\nVerification and Product Validation   .  .  .  11\\n2.5 Cost Effectiveness Considerations   .  .  .  11\\n2.6 Human Systems Integration (HSI)  \\nin the SE Process    .  .  .  .  .  .  .  .  .  .  . 12\\n2.7 Competency Model for  \\nSystems Engineers   .  .  .  .  .  .  .  .  .  .  .  13\\n3.0 NASA Program/Project Life Cycle  17\\n3.1 Program Formulation   .  .  .  .  .  .  .  .  .  . 20\\n3.2 Program Implementation   .  .  .  .  .  .  .  . 20\\n3.3 Project Pre-Phase A: Concept Studies   . 21\\n3.4 Project Phase A: Concept and  \\nTechnology Development   .  .  .  .  .  .  .  . 23\\n3.5 Project Phase B: Preliminary Design  \\nand Technology Completion   .  .  .  .  .  .  25'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 07: Download embeddings from OpenAi"
      ],
      "metadata": {
        "id": "s2clKKS_CBAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an instance of the OpenAIEmbeddings class to access OpenAI's embeddings\n",
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "-KrYOi-gB-k-"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a FAISS index for efficient text search\n",
        "# The 'from_texts' method builds the index from the provided list of text chunks using the embeddings\n",
        "docsearch = FAISS.from_texts(texts, embeddings)"
      ],
      "metadata": {
        "id": "6kP0CJr-CTu7"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading a question answering (QA) chain using OpenAI API\n",
        "# The 'load_qa_chain' function initializes a QA chain with the specified provider (in this case, OpenAI) and type of chain (\"stuff\")\n",
        "chain = load_qa_chain(OpenAI(), chain_type=\"stuff\")"
      ],
      "metadata": {
        "id": "7YFcoc5fDJKf"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 08: Buinding querys to OpenAi response"
      ],
      "metadata": {
        "id": "xbLmOIJEDlij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'Who is the authors of this handbook'\n",
        "# Querying for documents similar to the input query using the FAISS index\n",
        "docs = docsearch.similarity_search(query)\n",
        "\n",
        "# Running the question answering chain on the retrieved documents with the input query\n",
        "# The chain will attempt to answer the question based on the provided documents\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_ZtL7rBADXNc",
        "outputId": "46ef7d5d-acad-4cc3-b764-3471cbbb9f6e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The handbook has multiple authors, but it was edited by David D. Walden and others in 2015.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'What this handbook talks about?'\n",
        "docs = docsearch.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "a7owLrgLEDWj",
        "outputId": "5ecdf114-0a86-48a2-e53c-cf7e8439b92a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' This handbook talks about systems engineering best practices that should be incorporated in the development and implementation of large and small NASA programs/projects. It provides guidance and information on systems engineering that will be useful to the NASA community and is meant to increase awareness and consistency across the Agency and advance the practice of SE.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'What are some of the best practices cited in the handbook?'\n",
        "docs = docsearch.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "Qy-H0w6METYi",
        "outputId": "b4394075-1ab4-424d-87e6-d0df2b45794d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Some of the best practices cited in the handbook include using boxes and figures to define and illustrate concepts, incorporating guidance from NASA practitioners in the field, and following a systematic and disciplined set of processes throughout the life cycle of programs and projects.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZmZhEJrfEj8H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}